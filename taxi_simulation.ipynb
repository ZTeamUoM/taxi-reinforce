{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 30 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catagorical dqn agent\n",
    "gamma = 0.99\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fd2382ffa60>\n"
     ]
    }
   ],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=n_step_update+1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npolicy.action(reset)\\n#tf_env.time_step_spec()\\nprint(reset)\\n#print(env.reset())\\n#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\\nprint(\" \")\\nprint(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\\n\\n#print(tensor_spec.to_array_spec(reset))\\n#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\\n#encoder_func(env.reset())\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation(acceptPol)\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5\n",
      "41.0\n",
      "13.5\n",
      "-5.5\n",
      "52.5\n",
      "21.5\n",
      "62.5\n",
      "17.0\n",
      "-47.0\n",
      "59.0\n",
      "33.5\n",
      "-3.0\n",
      "131.0\n",
      "52.5\n",
      "-32.5\n",
      "156.5\n",
      "67.0\n",
      "61.0\n",
      "-51.5\n",
      "27.0\n",
      "-1.5\n",
      "13.0\n",
      "44.0\n",
      "48.5\n",
      "-63.0\n",
      "31.5\n",
      "93.5\n",
      "-20.0\n",
      "137.5\n",
      "139.0\n",
      "151.5\n",
      "3.5\n",
      "-35.5\n",
      "22.5\n",
      "62.5\n",
      "-116.0\n",
      "-43.0\n",
      "151.5\n",
      "123.0\n",
      "511.5\n",
      "driver reward  1957.5\n",
      "14.0\n",
      "-65.5\n",
      "-89.5\n",
      "93.0\n",
      "66.5\n",
      "67.0\n",
      "95.5\n",
      "100.0\n",
      "60.0\n",
      "3.0\n",
      "19.0\n",
      "-11.5\n",
      "53.0\n",
      "20.0\n",
      "117.5\n",
      "105.5\n",
      "-83.5\n",
      "61.0\n",
      "78.5\n",
      "-23.5\n",
      "11.0\n",
      "70.5\n",
      "12.0\n",
      "-57.5\n",
      "-7.0\n",
      "75.5\n",
      "-29.5\n",
      "-41.0\n",
      "4.5\n",
      "60.5\n",
      "104.0\n",
      "31.0\n",
      "37.0\n",
      "-45.5\n",
      "-15.0\n",
      "125.0\n",
      "6.5\n",
      "44.0\n",
      "driver reward  1066.0\n",
      "-19.0\n",
      "0.0\n",
      "100.0\n",
      "-46.0\n",
      "94.0\n",
      "-44.5\n",
      "60.0\n",
      "38.0\n",
      "8.0\n",
      "29.5\n",
      "-6.5\n",
      "-26.0\n",
      "6.0\n",
      "45.5\n",
      "72.5\n",
      "12.5\n",
      "177.5\n",
      "-27.5\n",
      "52.0\n",
      "-52.0\n",
      "98.5\n",
      "64.5\n",
      "47.5\n",
      "-7.0\n",
      "3.0\n",
      "83.5\n",
      "70.5\n",
      "2.0\n",
      "25.5\n",
      "-3.5\n",
      "-14.5\n",
      "10.0\n",
      "-34.0\n",
      "-1.0\n",
      "38.0\n",
      "5.0\n",
      "-36.5\n",
      "driver reward  825.5\n",
      "46.5\n",
      "77.5\n",
      "60.5\n",
      "84.0\n",
      "94.5\n",
      "75.0\n",
      "1.5\n",
      "-33.0\n",
      "74.0\n",
      "141.0\n",
      "54.0\n",
      "59.5\n",
      "-101.0\n",
      "64.0\n",
      "76.0\n",
      "20.5\n",
      "-28.0\n",
      "49.0\n",
      "103.5\n",
      "-39.5\n",
      "-119.0\n",
      "37.0\n",
      "25.5\n",
      "-39.0\n",
      "-9.5\n",
      "-57.5\n",
      "14.0\n",
      "-176.0\n",
      "-21.5\n",
      "21.0\n",
      "83.0\n",
      "50.0\n",
      "78.0\n",
      "54.0\n",
      "-125.5\n",
      "27.0\n",
      "-18.5\n",
      "35.0\n",
      "40.5\n",
      "580.5\n",
      "511.5\n",
      "577.5\n",
      "571.5\n",
      "driver reward  3019.0\n",
      "55.5\n",
      "10.0\n",
      "-14.5\n",
      "8.0\n",
      "8.0\n",
      "60.5\n",
      "18.0\n",
      "125.5\n",
      "42.0\n",
      "134.5\n",
      "21.5\n",
      "-20.0\n",
      "-65.5\n",
      "117.0\n",
      "-151.5\n",
      "29.5\n",
      "9.5\n",
      "2.0\n",
      "-107.0\n",
      "-79.0\n",
      "-23.0\n",
      "72.0\n",
      "7.5\n",
      "-7.0\n",
      "85.5\n",
      "72.0\n",
      "104.0\n",
      "-41.0\n",
      "110.0\n",
      "-5.0\n",
      "-44.0\n",
      "driver reward  535.0\n",
      "58.5\n",
      "149.0\n",
      "46.0\n",
      "60.0\n",
      "-24.0\n",
      "-13.0\n",
      "-49.0\n",
      "-27.5\n",
      "120.5\n",
      "49.0\n",
      "-24.5\n",
      "-102.0\n",
      "49.0\n",
      "132.5\n",
      "157.0\n",
      "-15.5\n",
      "160.5\n",
      "-5.0\n",
      "94.0\n",
      "46.0\n",
      "53.0\n",
      "58.0\n",
      "54.0\n",
      "157.5\n",
      "120.5\n",
      "-27.0\n",
      "45.5\n",
      "31.0\n",
      "108.5\n",
      "driver reward  1462.5\n",
      "170.0\n",
      "55.5\n",
      "-53.0\n",
      "38.5\n",
      "75.5\n",
      "109.5\n",
      "-11.0\n",
      "112.0\n",
      "42.0\n",
      "42.5\n",
      "-75.0\n",
      "87.0\n",
      "-18.5\n",
      "-3.0\n",
      "18.0\n",
      "-46.5\n",
      "13.5\n",
      "54.0\n",
      "-16.0\n",
      "-1.5\n",
      "46.5\n",
      "180.5\n",
      "149.0\n",
      "84.5\n",
      "44.5\n",
      "23.0\n",
      "-129.0\n",
      "-9.5\n",
      "87.0\n",
      "-41.0\n",
      "-18.0\n",
      "76.5\n",
      "-8.0\n",
      "52.5\n",
      "100.5\n",
      "66.5\n",
      "-44.5\n",
      "driver reward  1254.5\n",
      "33.0\n",
      "99.5\n",
      "27.0\n",
      "-48.0\n",
      "-41.5\n",
      "130.0\n",
      "7.5\n",
      "-59.0\n",
      "-82.5\n",
      "86.0\n",
      "101.5\n",
      "150.0\n",
      "36.0\n",
      "42.5\n",
      "8.0\n",
      "-1.0\n",
      "33.5\n",
      "46.5\n",
      "3.5\n",
      "48.5\n",
      "-94.5\n",
      "-2.0\n",
      "-70.0\n",
      "-34.0\n",
      "-28.0\n",
      "41.5\n",
      "60.5\n",
      "-2.0\n",
      "-92.5\n",
      "-71.0\n",
      "-9.0\n",
      "-10.0\n",
      "57.5\n",
      "-11.0\n",
      "-81.5\n",
      "-15.5\n",
      "-27.0\n",
      "16.5\n",
      "-23.0\n",
      "driver reward  226.0\n",
      "124.0\n",
      "-4.0\n",
      "33.0\n",
      "81.0\n",
      "131.5\n",
      "83.5\n",
      "-93.0\n",
      "-16.5\n",
      "-14.0\n",
      "-42.0\n",
      "-57.5\n",
      "11.0\n",
      "94.5\n",
      "130.0\n",
      "7.5\n",
      "60.0\n",
      "-48.0\n",
      "-10.5\n",
      "-34.5\n",
      "-38.5\n",
      "-4.5\n",
      "67.5\n",
      "-5.5\n",
      "-102.5\n",
      "-106.0\n",
      "82.0\n",
      "22.5\n",
      "-6.0\n",
      "47.5\n",
      "58.0\n",
      "15.0\n",
      "140.5\n",
      "2.0\n",
      "57.0\n",
      "18.5\n",
      "93.0\n",
      "188.0\n",
      "109.5\n",
      "34.0\n",
      "575.5\n",
      "560.5\n",
      "500.5\n",
      "557.0\n",
      "458.0\n",
      "539.5\n",
      "536.5\n",
      "600.5\n",
      "486.5\n",
      "520.5\n",
      "driver reward  6443.0\n",
      "-23.0\n",
      "-12.0\n",
      "172.0\n",
      "-9.0\n",
      "-3.0\n",
      "19.5\n",
      "-60.0\n",
      "88.5\n",
      "-111.5\n",
      "-28.5\n",
      "39.0\n",
      "35.0\n",
      "-60.0\n",
      "62.5\n",
      "77.5\n",
      "-74.0\n",
      "25.0\n",
      "40.0\n",
      "34.0\n",
      "39.0\n",
      "6.5\n",
      "-98.5\n",
      "72.5\n",
      "71.5\n",
      "175.5\n",
      "7.5\n",
      "31.0\n",
      "26.5\n",
      "9.0\n",
      "-23.5\n",
      "3.0\n",
      "-61.0\n",
      "83.0\n",
      "-32.5\n",
      "8.5\n",
      "-85.0\n",
      "37.0\n",
      "-77.0\n",
      "71.0\n",
      "536.0\n",
      "484.0\n",
      "611.0\n",
      "555.5\n",
      "552.0\n",
      "driver reward  3214.5\n",
      "93.0\n",
      "63.5\n",
      "7.5\n",
      "-9.0\n",
      "-45.5\n",
      "-68.0\n",
      "-22.0\n",
      "48.5\n",
      "-72.5\n",
      "41.0\n",
      "-57.5\n",
      "19.5\n",
      "77.5\n",
      "-27.0\n",
      "40.5\n",
      "88.5\n",
      "54.0\n",
      "-20.5\n",
      "36.0\n",
      "18.0\n",
      "64.0\n",
      "54.0\n",
      "-86.5\n",
      "-17.5\n",
      "43.0\n",
      "-15.0\n",
      "14.5\n",
      "89.5\n",
      "-39.5\n",
      "11.0\n",
      "94.0\n",
      "83.0\n",
      "-20.0\n",
      "24.5\n",
      "41.0\n",
      "-65.5\n",
      "40.5\n",
      "11.0\n",
      "-26.5\n",
      "driver reward  565.0\n",
      "127.5\n",
      "138.5\n",
      "138.5\n",
      "-35.0\n",
      "6.0\n",
      "15.0\n",
      "14.0\n",
      "-47.0\n",
      "123.0\n",
      "112.5\n",
      "16.0\n",
      "55.5\n",
      "90.5\n",
      "-86.5\n",
      "18.5\n",
      "-22.0\n",
      "23.0\n",
      "9.5\n",
      "4.5\n",
      "72.5\n",
      "-41.0\n",
      "-5.5\n",
      "118.0\n",
      "62.5\n",
      "-54.5\n",
      "-58.5\n",
      "-12.0\n",
      "-97.0\n",
      "18.0\n",
      "-28.5\n",
      "-15.0\n",
      "-76.5\n",
      "-39.5\n",
      "-55.0\n",
      "42.5\n",
      "4.5\n",
      "91.0\n",
      "-76.0\n",
      "driver reward  552.0\n",
      "32.5\n",
      "-28.0\n",
      "-47.0\n",
      "-36.0\n",
      "45.5\n",
      "-35.5\n",
      "30.5\n",
      "41.5\n",
      "-67.0\n",
      "85.0\n",
      "-5.5\n",
      "-63.5\n",
      "-67.0\n",
      "21.5\n",
      "78.0\n",
      "98.5\n",
      "102.0\n",
      "3.0\n",
      "-101.5\n",
      "86.0\n",
      "-57.0\n",
      "99.5\n",
      "55.0\n",
      "1.0\n",
      "-93.0\n",
      "73.5\n",
      "-27.0\n",
      "-29.5\n",
      "-53.0\n",
      "12.5\n",
      "94.0\n",
      "149.0\n",
      "23.0\n",
      "-89.0\n",
      "-42.0\n",
      "-56.0\n",
      "-128.0\n",
      "57.5\n",
      "driver reward  163.5\n",
      "119.0\n",
      "59.0\n",
      "104.0\n",
      "0.0\n",
      "-67.5\n",
      "5.5\n",
      "43.5\n",
      "135.0\n",
      "68.5\n",
      "-7.0\n",
      "-27.0\n",
      "13.5\n",
      "50.0\n",
      "120.0\n",
      "119.0\n",
      "-73.0\n",
      "-25.0\n",
      "34.5\n",
      "-106.5\n",
      "139.5\n",
      "48.0\n",
      "86.0\n",
      "141.5\n",
      "96.5\n",
      "179.0\n",
      "34.5\n",
      "53.5\n",
      "-8.0\n",
      "113.0\n",
      "116.5\n",
      "81.0\n",
      "-23.0\n",
      "63.5\n",
      "-39.0\n",
      "6.0\n",
      "62.5\n",
      "70.5\n",
      "-44.0\n",
      "126.5\n",
      "480.5\n",
      "572.5\n",
      "596.5\n",
      "driver reward  3519.0\n",
      "-44.0\n",
      "-60.5\n",
      "85.5\n",
      "60.5\n",
      "-11.5\n",
      "48.5\n",
      "122.5\n",
      "-14.0\n",
      "67.0\n",
      "123.5\n",
      "123.5\n",
      "73.5\n",
      "-20.5\n",
      "44.0\n",
      "65.0\n",
      "-52.5\n",
      "148.0\n",
      "45.0\n",
      "-56.0\n",
      "20.5\n",
      "-5.5\n",
      "-21.5\n",
      "36.0\n",
      "46.0\n",
      "24.0\n",
      "-48.0\n",
      "-37.5\n",
      "-96.0\n",
      "49.0\n",
      "9.0\n",
      "7.0\n",
      "18.5\n",
      "-155.5\n",
      "-18.0\n",
      "-72.5\n",
      "5.0\n",
      "33.5\n",
      "26.0\n",
      "95.0\n",
      "497.0\n",
      "545.5\n",
      "541.5\n",
      "482.5\n",
      "416.0\n",
      "601.0\n",
      "527.0\n",
      "490.5\n",
      "482.5\n",
      "352.5\n",
      "538.0\n",
      "629.0\n",
      "602.5\n",
      "536.5\n",
      "397.0\n",
      "driver reward  8301.5\n",
      "-20.5\n",
      "61.0\n",
      "69.0\n",
      "33.5\n",
      "34.0\n",
      "-114.5\n",
      "37.0\n",
      "114.0\n",
      "-19.0\n",
      "85.5\n",
      "95.0\n",
      "-54.5\n",
      "-88.5\n",
      "146.0\n",
      "23.0\n",
      "-75.5\n",
      "30.5\n",
      "-83.0\n",
      "-4.5\n",
      "20.0\n",
      "-52.5\n",
      "60.5\n",
      "35.5\n",
      "74.5\n",
      "32.5\n",
      "96.0\n",
      "-172.5\n",
      "32.0\n",
      "4.0\n",
      "84.0\n",
      "-10.5\n",
      "5.5\n",
      "5.0\n",
      "97.5\n",
      "-22.5\n",
      "85.0\n",
      "driver reward  642.5\n",
      "45.5\n",
      "57.0\n",
      "-20.0\n",
      "-52.5\n",
      "55.5\n",
      "-66.0\n",
      "41.0\n",
      "147.0\n",
      "-29.0\n",
      "-20.0\n",
      "-48.5\n",
      "13.0\n",
      "64.0\n",
      "83.5\n",
      "61.5\n",
      "159.5\n",
      "135.5\n",
      "45.0\n",
      "82.5\n",
      "-61.5\n",
      "12.5\n",
      "-53.5\n",
      "-60.5\n",
      "22.0\n",
      "6.5\n",
      "93.0\n",
      "-121.0\n",
      "101.0\n",
      "-26.5\n",
      "-12.0\n",
      "92.0\n",
      "-1.5\n",
      "55.5\n",
      "25.5\n",
      "-53.0\n",
      "64.0\n",
      "41.0\n",
      "47.0\n",
      "26.0\n",
      "462.0\n",
      "548.0\n",
      "574.5\n",
      "525.5\n",
      "637.5\n",
      "driver reward  3698.5\n",
      "46.0\n",
      "40.5\n",
      "-18.0\n",
      "46.0\n",
      "146.0\n",
      "-14.5\n",
      "-74.5\n",
      "103.5\n",
      "28.5\n",
      "-21.5\n",
      "74.5\n",
      "-80.0\n",
      "-19.0\n",
      "-12.5\n",
      "41.5\n",
      "2.0\n",
      "-4.0\n",
      "78.0\n",
      "-36.5\n",
      "-23.5\n",
      "-47.5\n",
      "54.5\n",
      "31.5\n",
      "38.5\n",
      "-68.5\n",
      "-17.0\n",
      "-4.5\n",
      "51.5\n",
      "55.5\n",
      "43.0\n",
      "93.0\n",
      "0.0\n",
      "87.0\n",
      "100.5\n",
      "-40.5\n",
      "17.0\n",
      "88.5\n",
      "56.0\n",
      "34.5\n",
      "380.5\n",
      "550.5\n",
      "599.0\n",
      "484.0\n",
      "449.5\n",
      "driver reward  3339.0\n",
      "46.5\n",
      "62.0\n",
      "-36.0\n",
      "39.5\n",
      "-95.0\n",
      "-6.0\n",
      "148.0\n",
      "-12.5\n",
      "17.5\n",
      "13.0\n",
      "-27.0\n",
      "95.5\n",
      "80.5\n",
      "-42.0\n",
      "73.5\n",
      "-43.0\n",
      "113.0\n",
      "124.5\n",
      "-189.5\n",
      "21.5\n",
      "-57.0\n",
      "-1.5\n",
      "-39.5\n",
      "-8.0\n",
      "8.0\n",
      "73.0\n",
      "-10.5\n",
      "43.0\n",
      "78.0\n",
      "-39.5\n",
      "27.5\n",
      "43.5\n",
      "11.5\n",
      "-5.5\n",
      "70.0\n",
      "-4.0\n",
      "-68.5\n",
      "111.0\n",
      "driver reward  615.5\n",
      "49.5\n",
      "-16.0\n",
      "31.0\n",
      "-13.5\n",
      "151.0\n",
      "87.5\n",
      "133.5\n",
      "73.0\n",
      "-12.0\n",
      "24.0\n",
      "11.5\n",
      "-25.5\n",
      "-8.0\n",
      "-36.5\n",
      "9.0\n",
      "165.0\n",
      "144.0\n",
      "45.5\n",
      "-42.5\n",
      "67.5\n",
      "46.0\n",
      "15.5\n",
      "-42.5\n",
      "154.5\n",
      "72.0\n",
      "47.5\n",
      "53.0\n",
      "-16.0\n",
      "16.5\n",
      "-102.5\n",
      "-13.0\n",
      "15.0\n",
      "8.0\n",
      "-6.0\n",
      "23.5\n",
      "-3.5\n",
      "116.5\n",
      "-113.5\n",
      "100.5\n",
      "310.5\n",
      "driver reward  1520.0\n",
      "total reward  42920.0\n"
     ]
    }
   ],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "\n",
    "evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average returnstep\n",
    "def compute_avg_return(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        episode_reward = 0\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            driver_reward = 0\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                driver_reward += state_tf.reward\n",
    "            episode_reward += driver_reward\n",
    "        \n",
    "        #take average reward for all drivers in the episode\n",
    "        episode_reward = episode_reward / len(state_lists)\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_return = total_reward / num_episodes\n",
    "    print(avg_return)\n",
    "    return avg_return.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories\n",
    "\n",
    "def collect_data(num_iterations, policy, replay_buffer):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        print(\"driver count : \", len(state_lists))\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            actions = []\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                #create time step\n",
    "                if i == 0:\n",
    "                    #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                    state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                    #print (state_tf)\n",
    "                elif i < (len(state_list) - 1):\n",
    "                    #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                else:\n",
    "                    state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                #create action\n",
    "                \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                    action = tf.constant([1], dtype=tf.int32)\n",
    "                else:\n",
    "                    action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                action = state_list[i][\"action\"]\n",
    "\n",
    "                #print (action)\n",
    "                states.append(state_tf)\n",
    "                actions.append(action)\n",
    "\n",
    "            for j in range(len(states)-1):\n",
    "                present_state = states[j]\n",
    "                next_state = states[j+1]\n",
    "                action = actions[j]\n",
    "                traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                #print(action)\n",
    "                # Add trajectory to the replay buffer\n",
    "                replay_buffer.add_batch(traj)\n",
    "                #print(traj)\n",
    "\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-2837.6375, shape=(), dtype=float32)\n",
      " Average Return = -2837.637451171875\n",
      "driver count :  20\n",
      "WARNING:tensorflow:From /home/haritha/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/utils/value_ops.py:85: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/haritha/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/utils/value_ops.py:85: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "step = 5: Average Return = 0.0\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 10: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=3.3665907>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(1545.85, shape=(), dtype=float32)\n",
      "step = 10: Average Return = 1545.8499755859375\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1699.3625, shape=(), dtype=float32)\n",
      "step = 15: Average Return = 1699.362548828125\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 20: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=2.9633873>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(589.875, shape=(), dtype=float32)\n",
      "step = 20: Average Return = 589.875\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1611.0625, shape=(), dtype=float32)\n",
      "step = 25: Average Return = 1611.0625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 30: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=3.3098555>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(1578.725, shape=(), dtype=float32)\n",
      "step = 30: Average Return = 1578.7249755859375\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1200.3501, shape=(), dtype=float32)\n",
      "step = 35: Average Return = 1200.35009765625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 40: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=5.530862>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(1526.9126, shape=(), dtype=float32)\n",
      "step = 40: Average Return = 1526.91259765625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1763.5125, shape=(), dtype=float32)\n",
      "step = 45: Average Return = 1763.512451171875\n",
      "evaluation\n",
      "driver count :  20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b8a68fd86324>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_steps_per_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-f1a94cb27ca8>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(num_iterations, policy, replay_buffer)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m#print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Add trajectory to the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;31m#print(traj)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/replay_buffers/replay_buffer.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mAdds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \"\"\"\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_stacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py\u001b[0m in \u001b[0;36m_add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0mwrite_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_for_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0mwrite_id_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m       \u001b[0mwrite_data_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_id_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_data_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/replay_buffers/table.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, rows, values, slots)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mflattened_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     write_ops = [\n\u001b[0m\u001b[1;32m    129\u001b[0m         tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n\u001b[1;32m    130\u001b[0m                                     value).op\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/replay_buffers/table.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mflattened_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     write_ops = [\n\u001b[0;32m--> 129\u001b[0;31m         tf.compat.v1.scatter_update(self._slot2storage_map[slot], rows,\n\u001b[0m\u001b[1;32m    130\u001b[0m                                     value).op\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_slots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36mscatter_update\u001b[0;34m(ref, indices, updates, use_locking, name)\u001b[0m\n\u001b[1;32m    302\u001b[0m     return gen_state_ops.scatter_update(ref, indices, updates,\n\u001b[1;32m    303\u001b[0m                                         use_locking=use_locking, name=name)\n\u001b[0;32m--> 304\u001b[0;31m   return ref._lazy_read(gen_resource_variable_ops.resource_scatter_update(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    305\u001b[0m       \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m       name=name))\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_scatter_update\u001b[0;34m(resource, indices, updates, name)\u001b[0m\n\u001b[1;32m   1113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1115\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   1116\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ResourceScatterUpdate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         tld.op_callbacks, resource, indices, updates)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print(' Average Return = {0}'.format( avg_return))\n",
    "returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize progress\n",
    "iterations = range(0, num_iterations +1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5\n",
      "41.0\n",
      "13.5\n",
      "-5.5\n",
      "52.5\n",
      "21.5\n",
      "62.5\n",
      "17.0\n",
      "0\n",
      "59.0\n",
      "33.5\n",
      "-3.0\n",
      "131.0\n",
      "52.5\n",
      "0\n",
      "156.5\n",
      "67.0\n",
      "61.0\n",
      "0\n",
      "27.0\n",
      "-1.5\n",
      "13.0\n",
      "44.0\n",
      "48.5\n",
      "0\n",
      "31.5\n",
      "93.5\n",
      "0\n",
      "137.5\n",
      "139.0\n",
      "151.5\n",
      "3.5\n",
      "0\n",
      "22.5\n",
      "62.5\n",
      "0\n",
      "0\n",
      "151.5\n",
      "123.0\n",
      "511.5\n",
      "driver reward  2366.0\n",
      "14.0\n",
      "0\n",
      "0\n",
      "93.0\n",
      "66.5\n",
      "67.0\n",
      "95.5\n",
      "100.0\n",
      "60.0\n",
      "3.0\n",
      "19.0\n",
      "0\n",
      "53.0\n",
      "20.0\n",
      "117.5\n",
      "105.5\n",
      "0\n",
      "61.0\n",
      "78.5\n",
      "0\n",
      "11.0\n",
      "70.5\n",
      "12.0\n",
      "0\n",
      "-7.0\n",
      "75.5\n",
      "0\n",
      "0\n",
      "4.5\n",
      "60.5\n",
      "104.0\n",
      "31.0\n",
      "37.0\n",
      "0\n",
      "0\n",
      "125.0\n",
      "6.5\n",
      "44.0\n",
      "driver reward  1528.0\n",
      "-19.0\n",
      "0.0\n",
      "100.0\n",
      "0\n",
      "94.0\n",
      "0\n",
      "60.0\n",
      "38.0\n",
      "8.0\n",
      "29.5\n",
      "0\n",
      "0\n",
      "6.0\n",
      "45.5\n",
      "72.5\n",
      "12.5\n",
      "177.5\n",
      "0\n",
      "52.0\n",
      "0\n",
      "98.5\n",
      "64.5\n",
      "47.5\n",
      "0\n",
      "3.0\n",
      "83.5\n",
      "70.5\n",
      "2.0\n",
      "25.5\n",
      "-3.5\n",
      "0\n",
      "10.0\n",
      "0\n",
      "-1.0\n",
      "38.0\n",
      "5.0\n",
      "0\n",
      "driver reward  1120.0\n",
      "46.5\n",
      "77.5\n",
      "60.5\n",
      "84.0\n",
      "94.5\n",
      "75.0\n",
      "1.5\n",
      "0\n",
      "74.0\n",
      "141.0\n",
      "54.0\n",
      "59.5\n",
      "0\n",
      "64.0\n",
      "76.0\n",
      "20.5\n",
      "-28.0\n",
      "49.0\n",
      "103.5\n",
      "0\n",
      "0\n",
      "37.0\n",
      "25.5\n",
      "0\n",
      "-9.5\n",
      "0\n",
      "14.0\n",
      "0\n",
      "-21.5\n",
      "21.0\n",
      "83.0\n",
      "50.0\n",
      "78.0\n",
      "54.0\n",
      "0\n",
      "27.0\n",
      "0\n",
      "35.0\n",
      "40.5\n",
      "580.5\n",
      "511.5\n",
      "577.5\n",
      "571.5\n",
      "driver reward  3728.0\n",
      "55.5\n",
      "10.0\n",
      "0\n",
      "8.0\n",
      "8.0\n",
      "60.5\n",
      "18.0\n",
      "125.5\n",
      "42.0\n",
      "134.5\n",
      "21.5\n",
      "-20.0\n",
      "0\n",
      "117.0\n",
      "0\n",
      "29.5\n",
      "9.5\n",
      "2.0\n",
      "0\n",
      "0\n",
      "-23.0\n",
      "72.0\n",
      "7.5\n",
      "0\n",
      "85.5\n",
      "72.0\n",
      "104.0\n",
      "0\n",
      "110.0\n",
      "-5.0\n",
      "0\n",
      "driver reward  1044.5\n",
      "58.5\n",
      "149.0\n",
      "46.0\n",
      "60.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "120.5\n",
      "49.0\n",
      "0\n",
      "0\n",
      "49.0\n",
      "132.5\n",
      "157.0\n",
      "-15.5\n",
      "160.5\n",
      "-5.0\n",
      "94.0\n",
      "46.0\n",
      "53.0\n",
      "58.0\n",
      "54.0\n",
      "157.5\n",
      "120.5\n",
      "0\n",
      "45.5\n",
      "31.0\n",
      "108.5\n",
      "driver reward  1729.5\n",
      "170.0\n",
      "55.5\n",
      "0\n",
      "38.5\n",
      "75.5\n",
      "109.5\n",
      "0\n",
      "112.0\n",
      "42.0\n",
      "42.5\n",
      "0\n",
      "87.0\n",
      "0\n",
      "-3.0\n",
      "18.0\n",
      "0\n",
      "13.5\n",
      "54.0\n",
      "-16.0\n",
      "-1.5\n",
      "46.5\n",
      "180.5\n",
      "149.0\n",
      "84.5\n",
      "44.5\n",
      "23.0\n",
      "0\n",
      "0\n",
      "87.0\n",
      "0\n",
      "0\n",
      "76.5\n",
      "0\n",
      "52.5\n",
      "100.5\n",
      "66.5\n",
      "0\n",
      "driver reward  1708.5\n",
      "33.0\n",
      "99.5\n",
      "27.0\n",
      "0\n",
      "0\n",
      "130.0\n",
      "7.5\n",
      "0\n",
      "0\n",
      "86.0\n",
      "101.5\n",
      "150.0\n",
      "36.0\n",
      "42.5\n",
      "8.0\n",
      "-1.0\n",
      "33.5\n",
      "46.5\n",
      "3.5\n",
      "48.5\n",
      "0\n",
      "-2.0\n",
      "0\n",
      "0\n",
      "0\n",
      "41.5\n",
      "60.5\n",
      "-2.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57.5\n",
      "-11.0\n",
      "0\n",
      "0\n",
      "-27.0\n",
      "16.5\n",
      "0\n",
      "driver reward  986.0\n",
      "124.0\n",
      "-4.0\n",
      "33.0\n",
      "81.0\n",
      "131.5\n",
      "83.5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "11.0\n",
      "94.5\n",
      "130.0\n",
      "7.5\n",
      "60.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-4.5\n",
      "67.5\n",
      "-5.5\n",
      "0\n",
      "0\n",
      "82.0\n",
      "22.5\n",
      "0\n",
      "47.5\n",
      "58.0\n",
      "15.0\n",
      "140.5\n",
      "0\n",
      "57.0\n",
      "18.5\n",
      "93.0\n",
      "188.0\n",
      "109.5\n",
      "34.0\n",
      "575.5\n",
      "560.5\n",
      "500.5\n",
      "557.0\n",
      "0\n",
      "539.5\n",
      "536.5\n",
      "600.5\n",
      "0\n",
      "520.5\n",
      "driver reward  6065.5\n",
      "0\n",
      "-12.0\n",
      "172.0\n",
      "-9.0\n",
      "-3.0\n",
      "19.5\n",
      "0\n",
      "88.5\n",
      "0\n",
      "0\n",
      "39.0\n",
      "35.0\n",
      "0\n",
      "62.5\n",
      "77.5\n",
      "0\n",
      "25.0\n",
      "40.0\n",
      "34.0\n",
      "39.0\n",
      "6.5\n",
      "0\n",
      "72.5\n",
      "71.5\n",
      "175.5\n",
      "7.5\n",
      "31.0\n",
      "26.5\n",
      "9.0\n",
      "0\n",
      "3.0\n",
      "0\n",
      "83.0\n",
      "0\n",
      "8.5\n",
      "0\n",
      "37.0\n",
      "0\n",
      "71.0\n",
      "536.0\n",
      "0\n",
      "611.0\n",
      "555.5\n",
      "552.0\n",
      "driver reward  3465.0\n",
      "93.0\n",
      "63.5\n",
      "7.5\n",
      "-9.0\n",
      "0\n",
      "0\n",
      "0\n",
      "48.5\n",
      "0\n",
      "41.0\n",
      "0\n",
      "19.5\n",
      "77.5\n",
      "0\n",
      "40.5\n",
      "88.5\n",
      "54.0\n",
      "0\n",
      "36.0\n",
      "18.0\n",
      "64.0\n",
      "54.0\n",
      "0\n",
      "0\n",
      "43.0\n",
      "-15.0\n",
      "14.5\n",
      "89.5\n",
      "0\n",
      "11.0\n",
      "94.0\n",
      "83.0\n",
      "0\n",
      "24.5\n",
      "41.0\n",
      "0\n",
      "40.5\n",
      "11.0\n",
      "0\n",
      "driver reward  1133.5\n",
      "127.5\n",
      "138.5\n",
      "138.5\n",
      "0\n",
      "6.0\n",
      "15.0\n",
      "14.0\n",
      "0\n",
      "123.0\n",
      "112.5\n",
      "16.0\n",
      "55.5\n",
      "90.5\n",
      "0\n",
      "18.5\n",
      "0\n",
      "23.0\n",
      "9.5\n",
      "4.5\n",
      "72.5\n",
      "0\n",
      "-5.5\n",
      "118.0\n",
      "62.5\n",
      "0\n",
      "0\n",
      "-12.0\n",
      "0\n",
      "18.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "42.5\n",
      "4.5\n",
      "91.0\n",
      "0\n",
      "driver reward  1284.0\n",
      "32.5\n",
      "-28.0\n",
      "0\n",
      "0\n",
      "45.5\n",
      "0\n",
      "30.5\n",
      "41.5\n",
      "0\n",
      "85.0\n",
      "-5.5\n",
      "0\n",
      "0\n",
      "21.5\n",
      "78.0\n",
      "98.5\n",
      "102.0\n",
      "3.0\n",
      "0\n",
      "86.0\n",
      "0\n",
      "99.5\n",
      "55.0\n",
      "1.0\n",
      "0\n",
      "73.5\n",
      "0\n",
      "-29.5\n",
      "0\n",
      "12.5\n",
      "94.0\n",
      "149.0\n",
      "23.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57.5\n",
      "driver reward  1126.0\n",
      "119.0\n",
      "59.0\n",
      "104.0\n",
      "0.0\n",
      "0\n",
      "5.5\n",
      "43.5\n",
      "135.0\n",
      "68.5\n",
      "-7.0\n",
      "0\n",
      "13.5\n",
      "50.0\n",
      "120.0\n",
      "119.0\n",
      "0\n",
      "0\n",
      "34.5\n",
      "0\n",
      "139.5\n",
      "48.0\n",
      "86.0\n",
      "141.5\n",
      "96.5\n",
      "179.0\n",
      "34.5\n",
      "53.5\n",
      "-8.0\n",
      "113.0\n",
      "116.5\n",
      "81.0\n",
      "0\n",
      "63.5\n",
      "0\n",
      "6.0\n",
      "62.5\n",
      "70.5\n",
      "0\n",
      "126.5\n",
      "480.5\n",
      "572.5\n",
      "596.5\n",
      "driver reward  3924.0\n",
      "0\n",
      "0\n",
      "85.5\n",
      "60.5\n",
      "0\n",
      "48.5\n",
      "122.5\n",
      "-14.0\n",
      "67.0\n",
      "123.5\n",
      "123.5\n",
      "73.5\n",
      "0\n",
      "44.0\n",
      "65.0\n",
      "0\n",
      "148.0\n",
      "45.0\n",
      "0\n",
      "20.5\n",
      "-5.5\n",
      "0\n",
      "36.0\n",
      "46.0\n",
      "24.0\n",
      "0\n",
      "0\n",
      "0\n",
      "49.0\n",
      "9.0\n",
      "7.0\n",
      "18.5\n",
      "0\n",
      "0\n",
      "0\n",
      "5.0\n",
      "33.5\n",
      "26.0\n",
      "95.0\n",
      "497.0\n",
      "545.5\n",
      "541.5\n",
      "482.5\n",
      "0\n",
      "601.0\n",
      "527.0\n",
      "490.5\n",
      "0\n",
      "0\n",
      "538.0\n",
      "629.0\n",
      "602.5\n",
      "536.5\n",
      "0\n",
      "driver reward  7347.5\n",
      "0\n",
      "61.0\n",
      "69.0\n",
      "33.5\n",
      "34.0\n",
      "0\n",
      "37.0\n",
      "114.0\n",
      "0\n",
      "85.5\n",
      "95.0\n",
      "0\n",
      "0\n",
      "146.0\n",
      "23.0\n",
      "0\n",
      "30.5\n",
      "0\n",
      "-4.5\n",
      "20.0\n",
      "0\n",
      "60.5\n",
      "35.5\n",
      "74.5\n",
      "32.5\n",
      "96.0\n",
      "0\n",
      "32.0\n",
      "4.0\n",
      "84.0\n",
      "-10.5\n",
      "5.5\n",
      "5.0\n",
      "97.5\n",
      "-22.5\n",
      "85.0\n",
      "driver reward  1323.0\n",
      "45.5\n",
      "57.0\n",
      "0\n",
      "0\n",
      "55.5\n",
      "0\n",
      "41.0\n",
      "147.0\n",
      "0\n",
      "0\n",
      "0\n",
      "13.0\n",
      "64.0\n",
      "83.5\n",
      "61.5\n",
      "159.5\n",
      "135.5\n",
      "45.0\n",
      "82.5\n",
      "0\n",
      "12.5\n",
      "0\n",
      "0\n",
      "22.0\n",
      "6.5\n",
      "93.0\n",
      "0\n",
      "101.0\n",
      "0\n",
      "0\n",
      "92.0\n",
      "-1.5\n",
      "55.5\n",
      "25.5\n",
      "0\n",
      "64.0\n",
      "41.0\n",
      "47.0\n",
      "26.0\n",
      "0\n",
      "548.0\n",
      "574.5\n",
      "525.5\n",
      "637.5\n",
      "driver reward  3860.5\n",
      "46.0\n",
      "40.5\n",
      "0\n",
      "46.0\n",
      "146.0\n",
      "0\n",
      "0\n",
      "103.5\n",
      "28.5\n",
      "0\n",
      "74.5\n",
      "0\n",
      "-19.0\n",
      "0\n",
      "41.5\n",
      "2.0\n",
      "-4.0\n",
      "78.0\n",
      "0\n",
      "0\n",
      "0\n",
      "54.5\n",
      "31.5\n",
      "38.5\n",
      "0\n",
      "0\n",
      "-4.5\n",
      "51.5\n",
      "55.5\n",
      "43.0\n",
      "93.0\n",
      "0.0\n",
      "87.0\n",
      "100.5\n",
      "0\n",
      "17.0\n",
      "88.5\n",
      "56.0\n",
      "34.5\n",
      "0\n",
      "550.5\n",
      "599.0\n",
      "484.0\n",
      "0\n",
      "driver reward  2963.5\n",
      "46.5\n",
      "62.0\n",
      "0\n",
      "39.5\n",
      "0\n",
      "-6.0\n",
      "148.0\n",
      "0\n",
      "17.5\n",
      "13.0\n",
      "0\n",
      "95.5\n",
      "80.5\n",
      "0\n",
      "73.5\n",
      "0\n",
      "113.0\n",
      "124.5\n",
      "0\n",
      "21.5\n",
      "0\n",
      "-1.5\n",
      "0\n",
      "-8.0\n",
      "8.0\n",
      "73.0\n",
      "-10.5\n",
      "43.0\n",
      "78.0\n",
      "0\n",
      "27.5\n",
      "43.5\n",
      "11.5\n",
      "-5.5\n",
      "70.0\n",
      "-4.0\n",
      "0\n",
      "111.0\n",
      "driver reward  1265.0\n",
      "49.5\n",
      "0\n",
      "31.0\n",
      "-13.5\n",
      "151.0\n",
      "87.5\n",
      "133.5\n",
      "73.0\n",
      "-12.0\n",
      "24.0\n",
      "11.5\n",
      "0\n",
      "-8.0\n",
      "0\n",
      "9.0\n",
      "165.0\n",
      "144.0\n",
      "45.5\n",
      "0\n",
      "67.5\n",
      "46.0\n",
      "15.5\n",
      "0\n",
      "154.5\n",
      "72.0\n",
      "47.5\n",
      "53.0\n",
      "-16.0\n",
      "16.5\n",
      "0\n",
      "0\n",
      "15.0\n",
      "8.0\n",
      "-6.0\n",
      "23.5\n",
      "-3.5\n",
      "116.5\n",
      "0\n",
      "100.5\n",
      "0\n",
      "driver reward  1601.5\n",
      "total reward  49569.5\n"
     ]
    }
   ],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreward results - \\nrandom policy - around 9.5k\\nlearned policy - 14k\\nalways accept policy - 19.4k\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa715307c8a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
