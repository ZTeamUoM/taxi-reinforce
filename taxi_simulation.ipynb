{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "from random import sample \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import policy_step\n",
    "\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation\n",
    "from RideSimulator import reward_parameters as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 10 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "saver = policy_saver.PolicySaver(eval_policy, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f11d9279640>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catagorical dqn agent\n",
    "gamma = 0.99\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f11c8278bb0>\n"
     ]
    }
   ],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=n_step_update+1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npolicy.action(reset)\\n#tf_env.time_step_spec()\\nprint(reset)\\n#print(env.reset())\\n#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\\nprint(\" \")\\nprint(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\\n\\n#print(tensor_spec.to_array_spec(reset))\\n#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\\n#encoder_func(env.reset())\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "hex count  114\n",
      "Number of trips generated: 1869\n"
     ]
    }
   ],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation([acceptPol])\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#policy which accepts all positive reward trips (for evaluation purposes)\n",
    "#this policy looks at pickup distance & trip distance and calculates trip reward\n",
    "class AcceptPositiveTripsPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    observations = obs.observation.numpy()[0]\n",
    "    trip_reward = (observations[1] * rp.unit_reward) - ((observations[0] + observations[1]) * rp.per_km_cost)\n",
    "    #print(trip_reward)\n",
    "    if (trip_reward >= 0):\n",
    "        return (tf.constant([1]))\n",
    "    else:\n",
    "        return (tf.constant([0]))\n",
    "\n",
    "accpt_positive_trips_policy = AcceptPositiveTripsPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.7        19.4         6.         38.13065633 23.72463914 40.        ]]\n",
      "2.920000000000016\n",
      "[[ 9.3        15.          7.         15.64394863 32.35216921 39.        ]]\n",
      "-15.240000000000009\n",
      "[[15.3         7.3         8.         12.27858908 40.23402489 38.        ]]\n",
      "-80.68\n",
      "[[ 6.2        27.6         8.          1.28675881 21.7954455  37.        ]]\n",
      "46.15999999999997\n",
      "[[20.5        37.4        11.          4.56496532  1.33065333 36.        ]]\n",
      "-19.71999999999997\n",
      "[[ 6.1        31.         15.          5.99278098 34.49980792 35.        ]]\n",
      "57.72\n",
      "[[12.9         4.7        15.          7.68919575 51.90987375 34.        ]]\n",
      "-72.68\n",
      "[[ 5.4        26.2        16.         13.04517826 31.1342781  33.        ]]\n",
      "47.120000000000005\n",
      "[[ 6.          7.4        16.         23.7540709  32.17972756 32.        ]]\n",
      "-17.120000000000005\n",
      "[[ 7.6        26.6        17.         21.98445076 12.22969952 31.        ]]\n",
      "33.44\n",
      "[[ 1.6   25.2   17.    28.006 36.613 30.   ]]\n",
      "69.75999999999999\n",
      "[[ 7.9        18.1        17.         39.47298694 41.70445438 29.        ]]\n",
      "4.200000000000017\n",
      "[[ 8.6         9.5        21.         26.57652771 33.04577751 28.        ]]\n",
      "-28.080000000000013\n",
      "[[ 9.          9.7        21.         26.68096263 31.67927162 27.        ]]\n",
      "-30.159999999999997\n",
      "[[10.2        17.1        22.         12.97657632 48.97594117 26.        ]]\n",
      "-14.639999999999986\n",
      "[[ 5.6        34.6        22.         27.28885592 17.55701926 25.        ]]\n",
      "72.63999999999999\n",
      "[[17.         11.1        10.         17.53605027 20.11872953 24.        ]]\n",
      "-80.08000000000001\n",
      "[[ 8.         28.5        12.         51.78216149 23.3292893  23.        ]]\n",
      "36.80000000000001\n",
      "[[ 7.1   36.8   14.    10.556 18.298 22.   ]]\n",
      "69.48000000000002\n",
      "[[ 4.8        11.3        14.         14.05733979 32.38715642 21.        ]]\n",
      "3.519999999999996\n",
      "[[ 9.6         1.5        15.          6.19759563 37.45383336 20.        ]]\n",
      "-60.47999999999999\n",
      "[[ 7.7        29.7        15.         23.30816996  6.9981076  19.        ]]\n",
      "42.68000000000001\n",
      "[[24.7        29.         18.         36.97527282 30.04374237 18.        ]]\n",
      "-75.16000000000003\n",
      "[[ 9.1   28.5   19.     9.915 56.093 17.   ]]\n",
      "29.319999999999993\n",
      "[[16.9        16.4        19.         20.61199968 50.24027999 16.        ]]\n",
      "-62.43999999999997\n",
      "[[15.8   15.4   19.    17.575 49.426 15.   ]]\n",
      "-58.160000000000025\n",
      "[[12.9        29.6        19.         32.09777814 58.18795286 14.        ]]\n",
      "7.0\n",
      "[[ 5.2         4.7        21.         24.37255659 58.22240653 13.        ]]\n",
      "-20.320000000000007\n",
      "[[ 8.4        14.4         1.         27.6883343  51.38884607 12.        ]]\n",
      "-11.039999999999992\n",
      "[[ 9.7        16.1         5.         28.8150048  41.94980484 11.        ]]\n",
      "-14.439999999999998\n",
      "[[18.6        24.3         6.         12.36047907  5.0556076  10.        ]]\n",
      "-48.72000000000003\n",
      "[[14.9         3.4         7.          2.75361363 20.18658422  9.        ]]\n",
      "-90.44\n",
      "[[ 6.6        29.          7.         35.00059951  5.54753604  8.        ]]\n",
      "47.91999999999999\n",
      "[[17.3   24.3   11.     5.043 34.341  7.   ]]\n",
      "-39.879999999999995\n",
      "[[ 3.2         8.1        11.          8.37800632 39.01386326  6.        ]]\n",
      "4.159999999999997\n",
      "[[ 6.8         9.3        11.          4.96251855 25.31347935  5.        ]]\n",
      "-16.480000000000004\n",
      "[[ 6.9        12.3        11.          7.88535325 17.14493953  4.        ]]\n",
      "-7.560000000000002\n",
      "[[19.4         5.7        11.          0.74136244 38.84078843  3.        ]]\n",
      "-113.67999999999998\n",
      "[[ 7.6        27.2        11.         24.20603856 17.12153836  2.        ]]\n",
      "35.360000000000014\n",
      "[[ 6.    21.9   11.     3.626 13.388  1.   ]]\n",
      "1529.28\n",
      "[[ 2.1        17.7        11.          8.15396392 28.92100985  0.        ]]\n",
      "42.359999999999985\n",
      "[[ 7.         25.6        11.         26.22620263 17.0795537  -1.        ]]\n",
      "34.31999999999999\n",
      "[[ 4.2        35.3        11.         58.89176977  0.97829417 -1.        ]]\n",
      "84.40000000000003\n",
      "driver reward  1323.3599999999997\n",
      "[[ 5.2        39.6         1.         19.05672516 14.76536628 40.        ]]\n",
      "91.35999999999996\n",
      "[[ 5.2         1.6         6.         13.79823574 12.11496852 39.        ]]\n",
      "-30.240000000000002\n",
      "[[18.2         8.5         8.          6.20317438 18.21680476 38.        ]]\n",
      "-96.56\n",
      "[[ 2.7        21.6         8.          4.9105271  37.47731101 37.        ]]\n",
      "50.75999999999999\n",
      "[[ 7.4        12.          8.          2.35328    23.14724079 36.        ]]\n",
      "-11.919999999999987\n",
      "[[ 4.4        18.5         8.          3.38403508  4.3407305  35.        ]]\n",
      "29.28\n",
      "[[ 3.5   10.6   11.     6.09  14.011 34.   ]]\n",
      "10.120000000000005\n",
      "[[ 4.         24.6        12.          4.3876003  41.25369412 33.        ]]\n",
      "51.51999999999998\n",
      "[[ 7.2        31.         12.         20.7370446   7.97016401 32.        ]]\n",
      "50.24000000000001\n",
      "[[24.6         3.8        14.          1.5835935  19.83692218 31.        ]]\n",
      "-155.12\n",
      "[[ 4.1         0.9        15.          5.56690435 19.2464721  30.        ]]\n",
      "-25.0\n",
      "[[ 4.8         8.5        17.         11.60860381 28.30234247 29.        ]]\n",
      "-5.439999999999998\n",
      "[[ 6.7        18.3        17.         11.52839793 42.18371107 28.        ]]\n",
      "13.0\n",
      "[[11.    11.    17.     8.809 57.862 27.   ]]\n",
      "-39.599999999999994\n",
      "[[ 2.8        11.6        17.         15.91948786 50.74396994 26.        ]]\n",
      "18.080000000000013\n",
      "[[12.4         0.6        17.          7.78438615 59.62745726 25.        ]]\n",
      "-82.39999999999999\n",
      "[[10.7        24.7        17.          6.28934737 25.33499432 24.        ]]\n",
      "6.28000000000003\n",
      "[[10.1         5.5        17.         10.56982625 10.42966058 23.        ]]\n",
      "-51.08\n",
      "[[10.3        18.7        18.         13.06863492  2.38846842 22.        ]]\n",
      "-10.199999999999989\n",
      "[[10.1        36.3        22.         24.13638755 44.07450132 21.        ]]\n",
      "47.48000000000002\n",
      "[[18.7        31.6         3.          4.04940692 24.55256495 20.        ]]\n",
      "-26.039999999999964\n",
      "[[ 8.1   11.3    5.     2.213 27.644 19.   ]]\n",
      "-18.919999999999987\n",
      "[[19.7        34.          5.         40.45328872 50.32393754 18.        ]]\n",
      "-25.160000000000025\n",
      "[[20.7   30.     7.     8.185 11.23  17.   ]]\n",
      "-44.75999999999999\n",
      "[[ 8.2   10.8    9.     7.054 21.007 16.   ]]\n",
      "-21.19999999999999\n",
      "[[ 4.5        10.5         9.          0.54619013 19.63951508 15.        ]]\n",
      "3.0\n",
      "[[14.5   14.2   10.     3.483 45.512 14.   ]]\n",
      "-53.16\n",
      "[[ 3.9        39.8        10.         20.9103921   8.19137374 13.        ]]\n",
      "100.84000000000003\n",
      "[[ 8.          2.1        11.         22.19742533 14.69249578 12.        ]]\n",
      "-47.67999999999999\n",
      "[[23.2        16.         11.         17.43146562 44.28026353 11.        ]]\n",
      "-106.56\n",
      "[[10.8         7.         11.         11.6024529  28.35650275 10.        ]]\n",
      "-51.040000000000006\n",
      "[[ 7.2        13.6        11.          3.93198388 47.14998345  9.        ]]\n",
      "-5.439999999999998\n",
      "[[ 8.4        11.         11.          9.01049584 28.80042444  8.        ]]\n",
      "-21.919999999999987\n",
      "[[16.2        12.5        11.          7.42424009 33.74754731  7.        ]]\n",
      "-70.16\n",
      "[[16.5        12.9        11.         14.82906071 37.1597521   6.        ]]\n",
      "-70.91999999999999\n",
      "[[ 9.7         9.1        12.         13.19799729 38.17435915  5.        ]]\n",
      "-36.839999999999975\n",
      "[[12.3    5.1   12.     2.171 24.845  4.   ]]\n",
      "-67.32\n",
      "[[ 4.4         6.         12.          6.17858193 28.59551846  3.        ]]\n",
      "-10.719999999999999\n",
      "[[14.6    7.3   12.     3.388 37.939  2.   ]]\n",
      "-75.91999999999999\n",
      "[[ 4.9        27.9        12.          7.60120855 12.64459618  1.        ]]\n",
      "1555.96\n",
      "[[ 4.7         4.9        13.          6.05186938 18.49562448  0.        ]]\n",
      "-16.28\n",
      "[[19.8         9.1        13.         14.50941849 37.90092358 -1.        ]]\n",
      "-105.51999999999998\n",
      "[[ 8.2         5.1        13.          5.62939155 29.71722223 -1.        ]]\n",
      "-39.43999999999998\n",
      "[[ 4.1        17.8        13.         12.4497565  42.47574363 -1.        ]]\n",
      "29.080000000000013\n",
      "[[ 1.6        27.5        13.         10.56763873 16.24458578 -1.        ]]\n",
      "77.12\n",
      "[[ 5.1        22.5        13.         14.60628992 36.65130044 -1.        ]]\n",
      "37.31999999999999\n",
      "[[18.1        19.9        13.          6.25308757 42.61418113 -1.        ]]\n",
      "-59.39999999999998\n",
      "[[13.4        19.9        13.         13.14122864 47.22324251 -1.        ]]\n",
      "-27.43999999999997\n",
      "[[14.1    8.    14.    18.435 37.059 -1.   ]]\n",
      "-70.28\n",
      "[[19.7        39.3        14.         37.97771901 44.77201185 -1.        ]]\n",
      "-8.199999999999989\n",
      "[[ 6.1        17.8        15.         15.18059642 37.66795005 -1.        ]]\n",
      "15.480000000000018\n",
      "[[13.6        16.         15.         23.81907016 29.58288515 -1.        ]]\n",
      "-41.28\n",
      "[[ 2.6         9.5        15.         13.47942803 34.42188827 -1.        ]]\n",
      "12.719999999999999\n",
      "[[18.6        17.4        15.         16.33834117 58.99785228 -1.        ]]\n",
      "-70.79999999999998\n",
      "[[13.2        22.7        15.          8.18407029 35.36032363 -1.        ]]\n",
      "-17.119999999999976\n",
      "[[ 6.5        26.6        15.         27.67095333 29.01725876 -1.        ]]\n",
      "40.91999999999999\n",
      "[[12.    17.9   17.    11.103 37.462 -1.   ]]\n",
      "-24.319999999999993\n",
      "[[15.9    8.5   17.     3.129 33.028 -1.   ]]\n",
      "-80.91999999999999\n",
      "[[ 3.2        25.6        17.         17.65357794  8.59517795 -1.        ]]\n",
      "60.16\n",
      "[[10.9        14.7        18.         25.71610445 22.21690733 -1.        ]]\n",
      "-27.080000000000013\n",
      "[[22.8        29.5        19.         37.51204112 33.25055444 -1.        ]]\n",
      "-60.639999999999986\n",
      "[[ 7.3        19.9        20.         27.80366842 19.19737931 -1.        ]]\n",
      "14.04000000000002\n",
      "[[23.4   15.5    7.     2.888 49.043 -1.   ]]\n",
      "-109.51999999999998\n",
      "[[16.9        18.6         7.         18.33946957 17.82267624 -1.        ]]\n",
      "-55.400000000000006\n",
      "[[ 6.8        32.4         8.         22.47567097 49.39464551 -1.        ]]\n",
      "57.440000000000055\n",
      "[[15.3        14.4         8.         20.829915   39.27150302 -1.        ]]\n",
      "-57.96000000000001\n",
      "[[16.5        38.8         8.         40.82115521 53.33410967 -1.        ]]\n",
      "11.960000000000036\n",
      "[[ 6.5        25.1        14.         23.19708843 47.21375485 -1.        ]]\n",
      "36.120000000000005\n",
      "[[ 3.6         4.1        14.         17.76562269 46.08340503 -1.        ]]\n",
      "-11.359999999999992\n",
      "[[15.1    6.6   15.     6.147 47.166 -1.   ]]\n",
      "-81.56\n",
      "[[ 3.         14.1        15.          0.20153531 35.77475547 -1.        ]]\n",
      "24.72\n",
      "[[ 6.8        26.4        15.         20.54571501 55.54866727 -1.        ]]\n",
      "38.24000000000004\n",
      "[[22.3        15.9        16.         10.54540482 25.75594241 -1.        ]]\n",
      "-100.75999999999999\n",
      "[[13.2        16.1        17.         18.02229662 30.03355663 -1.        ]]\n",
      "-38.24000000000001\n",
      "[[23.    15.5   17.     5.306 29.777 -1.   ]]\n",
      "-106.80000000000001\n",
      "[[ 9.4        33.6        17.         30.11756209 15.68449004 -1.        ]]\n",
      "43.60000000000002\n",
      "[[15.9        23.5        18.          1.53029848 22.07014589 -1.        ]]\n",
      "-32.91999999999996\n",
      "[[ 8.2         9.5        18.         11.27117983 32.62045347 -1.        ]]\n",
      "-25.359999999999985\n",
      "[[ 3.2        27.4        18.         26.16760877 53.6429197  -1.        ]]\n",
      "65.92000000000002\n",
      "[[21.8         1.7        19.          5.04666979 53.31900424 -1.        ]]\n",
      "-142.79999999999998\n",
      "[[ 7.3        32.3        19.         31.86163261 33.06401063 -1.        ]]\n",
      "53.72000000000003\n",
      "[[ 3.1         7.2        19.         30.94608356 42.95840411 -1.        ]]\n",
      "1.9599999999999937\n",
      "[[ 8.7         2.4        20.         27.75159711 36.87871569 -1.        ]]\n",
      "-51.47999999999999\n",
      "[[ 3.4        15.6        20.         36.83824123 23.57768618 -1.        ]]\n",
      "26.80000000000001\n",
      "[[15.6        22.3        20.         22.76080342 34.66194475 -1.        ]]\n",
      "-34.71999999999997\n",
      "[[ 4.    11.3   21.     9.156 29.647 -1.   ]]\n",
      "8.959999999999994\n",
      "[[11.8        36.         21.         44.67154002 58.85050753 -1.        ]]\n",
      "34.960000000000036\n",
      "[[11.5        22.5        13.         55.18885768 53.59691033 -1.        ]]\n",
      "-6.199999999999989\n",
      "[[ 2.4        22.7        22.         34.60004375 42.32711933 -1.        ]]\n",
      "56.32000000000002\n",
      "[[ 5.3        30.          6.         59.25892541 29.08757131 -1.        ]]\n",
      "59.960000000000036\n",
      "driver reward  70.32000000000099\n",
      "[[ 2.6         3.8         3.         12.6238031  51.79434115 40.        ]]\n",
      "-5.520000000000003\n",
      "[[ 8.2   11.1    6.     5.261 36.695 39.   ]]\n",
      "-20.23999999999998\n",
      "[[ 5.     8.5    6.     4.114 32.748 38.   ]]\n",
      "-6.799999999999997\n",
      "[[ 2.9         7.5         7.          3.81213295 23.76322052 37.        ]]\n",
      "4.280000000000001\n",
      "[[ 4.1        14.7         7.          7.83980138 12.64070439 36.        ]]\n",
      "19.160000000000025\n",
      "[[ 3.5        34.7         7.         35.73001075 37.15274711 35.        ]]\n",
      "87.24000000000001\n",
      "[[ 4.5         9.4         7.         33.08276579 42.09468128 34.        ]]\n",
      "-0.519999999999996\n",
      "[[11.3   21.8    8.    46.256 56.146 33.   ]]\n",
      "-7.0800000000000125\n",
      "[[10.4        11.1         9.         44.14728525 41.54435049 32.        ]]\n",
      "-35.19999999999999\n",
      "[[ 4.4   29.7   17.    13.087 48.502 31.   ]]\n",
      "65.12\n",
      "[[ 2.9         9.         17.         12.16148807 54.82312786 30.        ]]\n",
      "9.079999999999998\n",
      "[[ 5.6        21.9        17.         25.24574514 34.65358825 29.        ]]\n",
      "32.0\n",
      "[[20.8    7.5   17.     5.4   49.861 28.   ]]\n",
      "-117.44\n",
      "[[ 2.6         9.2        17.         16.74711287 52.83879807 27.        ]]\n",
      "11.760000000000005\n",
      "[[ 8.9        30.5        18.         39.19971551 48.77600349 26.        ]]\n",
      "37.08000000000004\n",
      "[[ 1.7   26.8   19.    15.372 57.926 25.   ]]\n",
      "74.20000000000002\n",
      "[[15.3   30.2   20.    32.032 30.856 24.   ]]\n",
      "-7.399999999999977\n",
      "[[ 9.1        10.5        21.         28.75793152 17.59385826 23.        ]]\n",
      "-28.28\n",
      "[[16.2        14.9         6.         14.58582836 17.13875163 22.        ]]\n",
      "-62.48000000000002\n",
      "[[14.3        19.8         7.          1.34356353 44.22572625 21.        ]]\n",
      "-33.879999999999995\n",
      "[[10.6         9.1         8.          5.39726503 29.96520236 20.        ]]\n",
      "-42.95999999999998\n",
      "[[24.         15.5         8.         18.68944528 32.33062299 19.        ]]\n",
      "-113.59999999999997\n",
      "[[12.7    3.     8.    12.649 41.01  18.   ]]\n",
      "-76.75999999999999\n",
      "[[12.5         9.2         9.          8.98118195 42.06351456 17.        ]]\n",
      "-55.56\n",
      "[[ 6.6         2.5         9.          5.26727863 34.09398494 16.        ]]\n",
      "-36.879999999999995\n",
      "[[ 8.5        11.7         9.         11.66843776 31.29003764 15.        ]]\n",
      "-20.359999999999985\n",
      "[[ 4.8        20.          9.         34.34494057 41.14630568 14.        ]]\n",
      "31.360000000000014\n",
      "[[ 2.6   23.5    9.     9.195 47.695 13.   ]]\n",
      "57.51999999999998\n",
      "[[ 1.8         9.8         9.          0.11176767 42.16258845 12.        ]]\n",
      "19.11999999999999\n",
      "[[ 7.2        20.5         9.         24.20358596 51.39385035 11.        ]]\n",
      "16.640000000000015\n",
      "[[ 6.9        29.5        10.         35.60300647 20.88500505 10.        ]]\n",
      "47.48000000000002\n",
      "[[13.3   29.2   14.     4.417 12.847  9.   ]]\n",
      "3.0\n",
      "[[ 1.6         1.5        15.          4.53872024 12.90418123  8.        ]]\n",
      "-6.079999999999998\n",
      "[[ 7.6   14.5   17.     3.97  34.623  7.   ]]\n",
      "-5.280000000000001\n",
      "[[ 3.6        19.         17.         20.45144457 26.55777118  6.        ]]\n",
      "36.31999999999999\n",
      "[[16.4        21.3        18.          4.28201771 10.20172023  5.        ]]\n",
      "-43.360000000000014\n",
      "[[ 8.1        31.5        18.         34.01109761 20.92443832  4.        ]]\n",
      "45.71999999999997\n",
      "[[10.6   15.    10.    52.029 22.248  3.   ]]\n",
      "-24.080000000000013\n",
      "[[10.3   24.4   17.    29.778 48.857  2.   ]]\n",
      "8.039999999999992\n",
      "[[ 2.4        13.5        17.         18.34343865 54.95528301  1.        ]]\n",
      "1526.88\n",
      "[[ 9.7         9.5        17.          4.78920564 49.09744087  0.        ]]\n",
      "-35.56\n",
      "[[ 3.8        26.6        17.         17.90008267 23.12336051 -1.        ]]\n",
      "59.28\n",
      "[[20.6        16.1        18.         18.81347163 43.48528231 -1.        ]]\n",
      "-88.56\n",
      "[[11.7        18.4        18.         21.13173975 30.77859728 -1.        ]]\n",
      "-20.67999999999998\n",
      "[[ 3.4        22.3        18.          1.65760095 44.22749976 -1.        ]]\n",
      "48.24000000000001\n",
      "[[ 4.6        19.1        18.         22.85873285 34.01538985 -1.        ]]\n",
      "29.839999999999975\n",
      "[[ 5.2        22.4        18.         40.79447547 35.00208273 -1.        ]]\n",
      "36.32000000000002\n",
      "[[17.7        27.3        13.          7.32157623 27.42649399 -1.        ]]\n",
      "-33.0\n",
      "[[15.         13.9        13.         20.24281484 35.67617129 -1.        ]]\n",
      "-57.51999999999998\n",
      "[[25.4        30.1        13.         27.56158541 36.99247428 -1.        ]]\n",
      "-76.39999999999998\n",
      "[[16.8         7.4        16.         43.53210971 18.76205387 -1.        ]]\n",
      "-90.56\n",
      "[[12.6   27.8   21.     5.557 25.329 -1.   ]]\n",
      "3.2800000000000296\n",
      "[[11.1        18.8        22.         22.52888246 30.50206466 -1.        ]]\n",
      "-15.319999999999993\n",
      "[[22.1        18.2         4.          1.99245367 24.54700667 -1.        ]]\n",
      "-92.03999999999996\n",
      "[[ 3.2        24.          5.          5.66587807  1.18949828 -1.        ]]\n",
      "55.04000000000002\n",
      "[[16.9        18.7        11.         23.19749831 13.50919713 -1.        ]]\n",
      "-55.079999999999956\n",
      "[[10.2        32.2        12.         45.2039587  26.52406518 -1.        ]]\n",
      "33.67999999999995\n",
      "[[10.4        33.1        16.         21.85000262 36.5352478  -1.        ]]\n",
      "35.19999999999999\n",
      "[[ 5.5        30.         17.         42.93070092 57.01436968 -1.        ]]\n",
      "58.599999999999994\n",
      "[[ 7.3        20.4        18.         54.00677552 44.09573028 -1.        ]]\n",
      "15.640000000000015\n",
      "[[ 6.3         5.          8.         45.00417111 43.33439079 -1.        ]]\n",
      "-26.840000000000003\n",
      "[[22.    22.6    9.     6.998 52.878 -1.   ]]\n",
      "-77.28000000000003\n",
      "[[ 7.4        35.1        10.         14.26937887 23.4593006  -1.        ]]\n",
      "62.0\n",
      "[[ 4.5        13.7        11.          5.2640531  19.89666105 -1.        ]]\n",
      "13.240000000000009\n",
      "[[ 3.3         7.3        11.         10.61303379 25.08480556 -1.        ]]\n",
      "0.9200000000000017\n",
      "[[ 4.4        17.1        11.          9.963332    6.17306061 -1.        ]]\n",
      "24.80000000000001\n",
      "[[ 5.    20.3   11.     0.681 18.963 -1.   ]]\n",
      "30.960000000000008\n",
      "[[15.8         3.7        12.         13.10040498 33.94536231 -1.        ]]\n",
      "-95.6\n",
      "[[ 8.3        30.3        12.         27.57891769 13.31962639 -1.        ]]\n",
      "40.51999999999998\n",
      "[[20.7        19.4        13.         18.42075222 49.41675761 -1.        ]]\n",
      "-78.67999999999995\n",
      "[[13.6        25.8        14.         38.68822296 49.67357361 -1.        ]]\n",
      "-9.919999999999959\n",
      "[[22.4        26.6        14.         41.25423854 45.73309358 -1.        ]]\n",
      "-67.19999999999999\n",
      "[[16.1   28.5   18.     3.219 38.63  -1.   ]]\n",
      "-18.28000000000003\n",
      "[[ 9.         19.9        18.         17.8819954  39.71414512 -1.        ]]\n",
      "2.480000000000018\n",
      "[[15.8        17.5        18.         18.20802596 50.07147642 -1.        ]]\n",
      "-51.43999999999997\n",
      "[[15.8         4.         18.          0.39512896 45.48813047 -1.        ]]\n",
      "-94.64000000000001\n",
      "[[ 9.4    7.    18.     1.471 37.425 -1.   ]]\n",
      "-41.51999999999998\n",
      "[[ 9.7        24.1        18.          4.89362583 21.10227698 -1.        ]]\n",
      "11.160000000000025\n",
      "[[ 9.4         2.1        19.          5.48554749 30.72554645 -1.        ]]\n",
      "-57.2\n",
      "[[ 1.1        18.9        19.         16.85823717 16.97327008 -1.        ]]\n",
      "53.0\n",
      "[[ 5.8        23.6        19.         33.93292575 38.22310531 -1.        ]]\n",
      "36.079999999999984\n",
      "[[13.    29.2   19.     3.26  49.324 -1.   ]]\n",
      "5.039999999999964\n",
      "[[ 9.5        21.7        19.          2.56044988 18.51350505 -1.        ]]\n",
      "4.840000000000003\n",
      "[[20.2        30.1        19.         11.82410565 10.05499113 -1.        ]]\n",
      "-41.039999999999964\n",
      "[[ 6.8         2.9        20.          2.4881716   8.43287889 -1.        ]]\n",
      "-36.959999999999994\n",
      "[[ 3.2   32.3   20.     1.522 42.14  -1.   ]]\n",
      "81.6\n",
      "[[ 3.6        39.         20.         25.91620984  8.859845   -1.        ]]\n",
      "100.32\n",
      "[[25.3         9.9        22.         13.0717291  28.92810073 -1.        ]]\n",
      "-140.36\n",
      "[[21.9        23.9        22.         34.59834685 38.03791015 -1.        ]]\n",
      "-72.44\n",
      "[[12.3        19.5        23.         30.29425952 26.05775161 -1.        ]]\n",
      "-21.24000000000001\n",
      "[[22.8        17.2         1.         20.70904797 58.43187963 -1.        ]]\n",
      "-100.0\n",
      "[[14.9        15.3         5.         10.03932848 41.72372877 -1.        ]]\n",
      "-52.360000000000014\n",
      "[[ 4.4        22.9         5.         32.57928234 28.93280412 -1.        ]]\n",
      "43.360000000000014\n",
      "[[ 9.9   20.     5.     5.422 19.098 -1.   ]]\n",
      "-3.319999999999993\n",
      "[[14.6        11.8         6.         13.51037806 42.10407796 -1.        ]]\n",
      "-61.51999999999998\n",
      "[[14.8   23.4    6.    25.719 50.031 -1.   ]]\n",
      "-25.75999999999999\n",
      "[[ 8.8         6.6         6.         31.18179912 42.28034276 -1.        ]]\n",
      "-38.72\n",
      "[[ 8.3   16.4    7.    50.346 49.318 -1.   ]]\n",
      "-3.9599999999999795\n",
      "[[ 6.6         8.6         9.         52.64666653 58.10795774 -1.        ]]\n",
      "-17.36\n",
      "driver reward  469.3200000000008\n",
      "[[ 9.4        34.6         5.         41.248075   47.12082522 40.        ]]\n",
      "46.80000000000001\n",
      "[[ 6.1         6.8         8.         49.14067055 48.43708847 39.        ]]\n",
      "-19.719999999999985\n",
      "[[ 1.7        21.3        17.         36.83404257 33.09100974 38.        ]]\n",
      "56.599999999999994\n",
      "[[ 5.2        10.         19.         37.68768502 42.25345591 37.        ]]\n",
      "-3.3599999999999994\n",
      "[[20.          5.8        19.         18.73726537 59.68165463 36.        ]]\n",
      "-117.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.6        18.5        20.         32.33148825 55.9180513  35.        ]]\n",
      "-19.680000000000007\n",
      "[[14.5        14.4        20.         14.61036048 42.28603461 34.        ]]\n",
      "-52.51999999999998\n",
      "[[ 3.2         8.5        20.          8.87697149 50.95232723 33.        ]]\n",
      "5.440000000000012\n",
      "[[ 5.7         8.4        21.         22.32706604 51.27508677 32.        ]]\n",
      "-11.88000000000001\n",
      "[[12.4         9.5         6.         12.0139986  35.52085248 31.        ]]\n",
      "-53.91999999999999\n",
      "[[19.9   24.3    6.    23.585 43.453 30.   ]]\n",
      "-57.56\n",
      "[[11.    25.1    6.    15.64  55.155 29.   ]]\n",
      "5.52000000000001\n",
      "[[15.2        22.          7.          5.16707412 22.05597226 28.        ]]\n",
      "-32.96000000000001\n",
      "[[ 6.9   27.7    7.    28.247 12.105 27.   ]]\n",
      "41.72\n",
      "[[ 2.3         6.2        10.         30.90871582  7.40115464 26.        ]]\n",
      "4.200000000000003\n",
      "[[ 9.9   19.    13.     5.117 20.287 25.   ]]\n",
      "-6.519999999999982\n",
      "[[ 4.3    5.2   13.     4.292 20.879 24.   ]]\n",
      "-12.599999999999994\n",
      "[[ 7.1        13.4        13.         21.07901162 32.27178268 23.        ]]\n",
      "-5.400000000000006\n",
      "[[23.2         4.9        13.          8.03482152 50.50918848 22.        ]]\n",
      "-142.08\n",
      "[[ 6.          4.5        13.          7.30134762 48.63070597 21.        ]]\n",
      "-26.39999999999999\n",
      "[[ 4.4        15.1        13.         20.75484191 36.49662828 20.        ]]\n",
      "18.400000000000006\n",
      "[[19.3        13.7        14.         13.97148948 37.66741154 19.        ]]\n",
      "-87.4\n",
      "[[11.9        34.3        14.         38.86052144 40.24508908 18.        ]]\n",
      "28.840000000000032\n",
      "[[22.9        12.5        16.         31.57589665 49.04738437 17.        ]]\n",
      "-115.71999999999997\n",
      "[[10.1        18.6        16.         11.65184646 35.98998394 16.        ]]\n",
      "-9.160000000000025\n",
      "[[ 9.6         4.5        17.          1.30150952 37.92978707 15.        ]]\n",
      "-50.879999999999995\n",
      "[[12.8        12.         17.          0.97104493 39.02962532 14.        ]]\n",
      "-48.639999999999986\n",
      "[[ 9.9        15.         17.         12.98544236 36.78822302 13.        ]]\n",
      "-19.319999999999993\n",
      "[[ 8.1        26.         17.         19.41993889 20.08217035 12.        ]]\n",
      "28.120000000000005\n",
      "[[17.1        15.1        17.         11.1629556  36.92385525 11.        ]]\n",
      "-67.96000000000001\n",
      "[[ 1.         15.8        17.          5.91839007 52.24542541 10.        ]]\n",
      "43.760000000000005\n",
      "[[ 3.7        12.3        17.         19.98134217 44.79527918  9.        ]]\n",
      "14.200000000000003\n",
      "[[ 9.1         9.7        18.          6.68359592 33.96700833  8.        ]]\n",
      "-30.839999999999975\n",
      "[[ 8.2        20.         18.         24.67544169 44.39671959  7.        ]]\n",
      "8.240000000000009\n",
      "[[13.          6.3        18.          8.59792776 50.0561611   6.        ]]\n",
      "-68.24000000000001\n",
      "[[ 5.5        15.2        18.         18.61208831 49.67521513  5.        ]]\n",
      "11.240000000000009\n",
      "[[12.7        15.         18.         21.02843162 55.14221434  4.        ]]\n",
      "-38.359999999999985\n",
      "[[11.6    2.2   18.     8.199 58.162  3.   ]]\n",
      "-71.84\n",
      "[[ 7.4   14.9   18.     2.931 41.531  2.   ]]\n",
      "-2.640000000000015\n",
      "[[12.9        19.3        18.         20.16869686 38.90122701  1.        ]]\n",
      "1474.04\n",
      "[[ 9.4   14.9   18.    25.531 40.676  0.   ]]\n",
      "-16.24000000000001\n",
      "[[19.9         2.4        18.          9.93060782 56.30191007 -1.        ]]\n",
      "-127.63999999999999\n",
      "[[15.2        10.5        19.         15.59513436 31.25793803 -1.        ]]\n",
      "-69.75999999999999\n",
      "[[ 7.5         8.8        19.          9.7060447  42.71856364 -1.        ]]\n",
      "-22.840000000000003\n",
      "[[12.6        17.2        19.         10.48468389 47.39964993 -1.        ]]\n",
      "-30.639999999999986\n",
      "[[ 9.1        10.8        19.         18.01111858 29.09316814 -1.        ]]\n",
      "-27.319999999999993\n",
      "[[12.4        10.7        19.         16.72239209 20.68772323 -1.        ]]\n",
      "-50.08000000000001\n",
      "[[12.2        19.1        19.         15.04783792 41.78225744 -1.        ]]\n",
      "-21.840000000000003\n",
      "[[ 3.3   27.9   19.    35.644 58.475 -1.   ]]\n",
      "66.84\n",
      "[[24.     8.8    8.    11.444 48.067 -1.   ]]\n",
      "-135.03999999999996\n",
      "[[ 8.8        31.7         8.         32.82368928 36.29096944 -1.        ]]\n",
      "41.60000000000002\n",
      "[[ 8.5   19.4    8.    11.314 35.675 -1.   ]]\n",
      "4.280000000000001\n",
      "[[ 5.8   13.5    8.     6.842 46.853 -1.   ]]\n",
      "3.759999999999991\n",
      "[[12.1   13.5    8.     5.708 45.963 -1.   ]]\n",
      "-39.08000000000001\n",
      "[[ 1.5         4.          8.          3.00635598 49.57321287 -1.        ]]\n",
      "2.6000000000000014\n",
      "[[ 8.5         3.          8.          7.319571   59.08369061 -1.        ]]\n",
      "-48.2\n",
      "[[ 4.3        38.2         9.         37.18003521 28.87240247 -1.        ]]\n",
      "93.0\n",
      "[[11.         12.4         9.         30.87376373 34.79987155 -1.        ]]\n",
      "-35.119999999999976\n",
      "[[ 9.5        23.1         9.         27.41829884 59.24599659 -1.        ]]\n",
      "9.319999999999993\n",
      "[[ 2.         39.3         9.         57.58201791 36.73615711 -1.        ]]\n",
      "112.16000000000003\n",
      "[[ 2.7         8.6        12.         56.87569882 29.91197792 -1.        ]]\n",
      "9.159999999999997\n",
      "[[ 8.8   16.6   13.    38.447 46.213 -1.   ]]\n",
      "-6.719999999999999\n",
      "[[ 5.     8.7   13.    31.126 44.118 -1.   ]]\n",
      "-6.159999999999997\n",
      "[[13.4   21.    17.     4.008 44.026 -1.   ]]\n",
      "-23.919999999999987\n",
      "[[13.2        24.1        17.         10.34681978 33.45109586 -1.        ]]\n",
      "-12.639999999999986\n",
      "[[ 4.9        18.6        17.         26.55863685 46.11690753 -1.        ]]\n",
      "26.200000000000017\n",
      "[[18.9        18.3        18.         25.176221   43.46180637 -1.        ]]\n",
      "-69.96000000000001\n",
      "[[26.4        16.         18.          6.85298113 42.20117795 -1.        ]]\n",
      "-128.32\n",
      "[[ 2.5        23.1        18.         25.48136686 30.15808459 -1.        ]]\n",
      "56.91999999999999\n",
      "[[ 3.1        13.8        18.         40.1767611  32.16260255 -1.        ]]\n",
      "23.079999999999984\n",
      "[[ 7.4        39.9        10.         32.08157979  0.33185881 -1.        ]]\n",
      "77.36000000000001\n",
      "[[18.9         1.4        11.         20.21222665 13.41448355 -1.        ]]\n",
      "-124.03999999999996\n",
      "[[20.4        15.5        12.         16.5436643  33.56438371 -1.        ]]\n",
      "-89.11999999999998\n",
      "[[ 8.8        54.8        12.         58.20860968 13.70057444 -1.        ]]\n",
      "115.52000000000004\n",
      "[[ 2.4   14.2   12.    51.133 26.575 -1.   ]]\n",
      "29.12000000000002\n",
      "driver reward  200.32000000000033\n",
      "[[ 3.2         9.4         7.         36.87496537 16.87851324 40.        ]]\n",
      "8.319999999999993\n",
      "[[10.4        17.1         7.         40.33652606 24.18819465 39.        ]]\n",
      "-16.0\n",
      "[[ 4.8         6.2         8.         41.4055939  22.93309182 38.        ]]\n",
      "-12.799999999999997\n",
      "[[ 8.4   23.9   13.    30.274 42.665 37.   ]]\n",
      "19.360000000000014\n",
      "[[16.2        24.1        15.         22.7610435  28.58184419 36.        ]]\n",
      "-33.039999999999964\n",
      "[[ 7.4    9.7   16.     6.456 33.306 35.   ]]\n",
      "-19.28\n",
      "[[12.1   35.2   16.    16.523  9.147 34.   ]]\n",
      "30.359999999999957\n",
      "[[16.          8.1        17.          4.54450839 28.31060165 33.        ]]\n",
      "-82.88\n",
      "[[10.9        41.         18.         51.67309047 34.2104944  32.        ]]\n",
      "57.08000000000004\n",
      "[[ 5.7    7.2   15.    55.803 26.638 31.   ]]\n",
      "-15.719999999999999\n",
      "[[ 4.    10.1   19.    47.791 20.968 30.   ]]\n",
      "5.1200000000000045\n",
      "[[ 6.3        36.         18.         28.84201996 44.77532661 29.        ]]\n",
      "72.36000000000001\n",
      "[[ 4.    24.7   19.     5.166 55.208 28.   ]]\n",
      "51.84\n",
      "[[ 5.7         7.2        19.         13.34954766 45.94294949 27.        ]]\n",
      "-15.719999999999999\n",
      "[[10.6   11.6   19.     7.172 45.167 26.   ]]\n",
      "-34.95999999999998\n",
      "[[ 6.7    1.3   19.    12.826 44.378 25.   ]]\n",
      "-41.4\n",
      "[[ 5.3   19.6   19.     7.107 59.235 24.   ]]\n",
      "26.67999999999998\n",
      "[[ 4.8        15.5        19.         13.44529701 40.12031149 23.        ]]\n",
      "16.960000000000008\n",
      "[[13.5        11.3        19.          2.70552408 37.936561   22.        ]]\n",
      "-55.639999999999986\n",
      "[[10.1         3.4        19.          8.23521114 29.6703156  21.        ]]\n",
      "-57.8\n",
      "[[15.7        13.9        19.         21.86813999 16.77723439 20.        ]]\n",
      "-62.28\n",
      "[[26.5        24.4        20.         31.20559316 42.1306779  19.        ]]\n",
      "-102.12\n",
      "[[ 9.    31.1   20.     8.473  9.066 18.   ]]\n",
      "38.31999999999999\n",
      "[[13.2         5.1        20.         13.0297524  19.13522925 17.        ]]\n",
      "-73.43999999999998\n",
      "[[ 9.         16.1        20.         28.18350612 28.04781087 16.        ]]\n",
      "-9.680000000000007\n",
      "[[10.5   26.7   21.     2.48  42.506 15.   ]]\n",
      "14.039999999999992\n",
      "[[10.7        20.5        21.         11.47282167 33.74170016 14.        ]]\n",
      "-7.159999999999997\n",
      "[[ 8.8        27.8        21.         20.43756166 17.00546649 13.        ]]\n",
      "29.120000000000005\n",
      "[[20.8        11.2        22.          7.25918845 18.09709261 12.        ]]\n",
      "-105.6\n",
      "[[ 6.8         9.7        23.         11.56157241  2.41415883 11.        ]]\n",
      "-15.200000000000003\n",
      "[[19.5        40.9         5.         26.59621808 54.87116494 10.        ]]\n",
      "-1.7199999999999704\n",
      "[[27.3        39.8         5.         16.61490207  0.45494922  9.        ]]\n",
      "-58.27999999999997\n",
      "[[7.1   6.3   8.    5.641 6.245 8.   ]]\n",
      "-28.11999999999999\n",
      "[[17.6        24.5         8.         35.17512984 25.19570728  7.        ]]\n",
      "-41.28000000000003\n",
      "[[ 4.1        30.7         9.         29.63177788 58.13577635  6.        ]]\n",
      "70.36000000000001\n",
      "[[ 5.1   22.2    9.     4.112 57.354  5.   ]]\n",
      "36.360000000000014\n",
      "[[ 5.4        20.3         9.         26.66935071 50.26989597  4.        ]]\n",
      "28.23999999999998\n",
      "[[12.1    9.4    9.    22.817 41.362  3.   ]]\n",
      "-52.19999999999999\n",
      "[[12.6   27.5    9.     4.228 23.959  2.   ]]\n",
      "2.319999999999993\n",
      "[[ 7.3        17.7        10.         25.19184771 33.23986014  1.        ]]\n",
      "1507.0\n",
      "[[21.         26.8        10.         34.39041703 57.58406008  0.        ]]\n",
      "-57.039999999999964\n",
      "[[13.9        13.5        23.         15.22027901 38.16304297 -1.        ]]\n",
      "-51.31999999999999\n",
      "[[ 6.9        1.2        1.         7.6968896 40.4255663 -1.       ]]\n",
      "-43.08\n",
      "[[ 9.1        33.7         1.         35.53523343 36.9087237  -1.        ]]\n",
      "45.95999999999998\n",
      "[[11.6        22.5         7.         29.91251674 17.81080013 -1.        ]]\n",
      "-6.8799999999999955\n",
      "[[ 4.5        14.9         8.         39.01986303  5.36318522 -1.        ]]\n",
      "17.080000000000013\n",
      "[[ 5.5   48.5   12.     6.978 34.841 -1.   ]]\n",
      "117.80000000000001\n",
      "[[ 2.2         4.9        12.          2.51365272 30.24955478 -1.        ]]\n",
      "0.7199999999999989\n",
      "[[ 3.4         6.8        12.          4.48675878 35.25005716 -1.        ]]\n",
      "-1.3599999999999994\n",
      "[[ 7.9         5.9        12.          7.59218128 21.93709757 -1.        ]]\n",
      "-34.84\n",
      "[[ 4.9         4.6        13.          7.36287019 28.63276646 -1.        ]]\n",
      "-18.599999999999994\n",
      "[[ 3.4        12.7        13.          4.74803339 42.3260907  -1.        ]]\n",
      "17.520000000000024\n",
      "[[ 2.4         8.9        13.          3.64634624 35.52459822 -1.        ]]\n",
      "12.159999999999997\n",
      "[[ 2.9        10.8        13.          3.25780481 27.40790122 -1.        ]]\n",
      "14.83999999999999\n",
      "[[11.5        23.2        13.         28.78666877 42.56548564 -1.        ]]\n",
      "-3.960000000000008\n",
      "[[ 9.2   26.3   14.    11.812 47.039 -1.   ]]\n",
      "21.599999999999994\n",
      "[[10.5        20.9        15.         17.44512165 36.08048958 -1.        ]]\n",
      "-4.519999999999982\n",
      "[[ 8.6        14.3        15.         16.63743692 54.59991928 -1.        ]]\n",
      "-12.719999999999999\n",
      "[[10.2        15.8        15.         12.94872401 46.52112251 -1.        ]]\n",
      "-18.799999999999983\n",
      "[[ 8.5   19.9   15.    26.134 49.999 -1.   ]]\n",
      "5.880000000000024\n",
      "[[ 3.1        36.5        16.         34.59285126 13.27928791 -1.        ]]\n",
      "95.71999999999997\n",
      "[[ 6.8        10.5        17.         45.52096297  5.62047523 -1.        ]]\n",
      "-12.64\n",
      "[[ 5.6   20.1   19.    23.049  2.701 -1.   ]]\n",
      "26.23999999999998\n",
      "[[ 8.4        13.6        19.         17.3550547   3.27980612 -1.        ]]\n",
      "-13.599999999999994\n",
      "[[15.8   14.6   21.    13.511 20.081 -1.   ]]\n",
      "-60.72\n",
      "[[ 3.4         7.4        21.         22.65692866 14.49475742 -1.        ]]\n",
      "0.5600000000000023\n",
      "[[17.1    2.     7.    24.413 33.262 -1.   ]]\n",
      "-109.88\n",
      "[[15.7        21.1         7.         11.96832644 24.11192078 -1.        ]]\n",
      "-39.23999999999998\n",
      "[[14.2        25.          7.         31.08668619 31.017707   -1.        ]]\n",
      "-16.560000000000002\n",
      "[[15.1   11.6    8.    25.139 26.473 -1.   ]]\n",
      "-65.56\n",
      "[[13.4        32.6         8.         17.64645955  4.80681477 -1.        ]]\n",
      "13.199999999999989\n",
      "[[ 5.2         6.5         8.         18.94754656  0.30895127 -1.        ]]\n",
      "-14.559999999999988\n",
      "[[17.3         4.7        10.         13.00580423 12.85317897 -1.        ]]\n",
      "-102.6\n",
      "[[16.1        44.6        11.         49.06420315 25.85070489 -1.        ]]\n",
      "33.24000000000001\n",
      "[[ 4.1        18.8        13.         38.21943642  5.73464012 -1.        ]]\n",
      "32.28\n",
      "driver reward  837.8400000000003\n",
      "[[ 4.8         6.         16.         58.27100692 15.29639366 40.        ]]\n",
      "-13.439999999999998\n",
      "[[ 4.1   15.7   19.    58.155 32.214 39.   ]]\n",
      "22.360000000000014\n",
      "[[17.2   37.7    9.    15.225 22.858 38.   ]]\n",
      "3.67999999999995\n",
      "[[19.         10.2         9.         18.4214744  32.74905941 37.        ]]\n",
      "-96.56\n",
      "[[11.6        23.7         9.         30.84585716 47.5003861  36.        ]]\n",
      "-3.0399999999999636\n",
      "[[ 6.2        18.5         9.         17.30649342 41.6228554  35.        ]]\n",
      "17.04000000000002\n",
      "[[11.7        29.3        10.         36.32473038 45.45038756 34.        ]]\n",
      "14.199999999999989\n",
      "[[ 9.5        26.9        10.         50.56286481 34.70371222 33.        ]]\n",
      "21.480000000000018\n",
      "[[ 9.9   12.9   12.    46.778 47.462 32.   ]]\n",
      "-26.039999999999992\n",
      "[[ 6.7         5.1        18.         55.19313157 41.13139257 31.        ]]\n",
      "-29.24000000000001\n",
      "[[10.6   24.5   13.    20.467 44.344 30.   ]]\n",
      "6.319999999999993\n",
      "[[10.6        15.1        13.         26.17345382 34.96006168 29.        ]]\n",
      "-23.75999999999999\n",
      "[[24.2         4.7        13.          1.85834293 50.54687902 28.        ]]\n",
      "-149.51999999999998\n",
      "[[ 5.5        24.8        13.         14.29011698 27.28444377 27.        ]]\n",
      "41.96000000000001\n",
      "[[23.7         2.         13.          0.42280707 45.22880562 26.        ]]\n",
      "-154.76\n",
      "[[ 9.3         5.7        14.         10.37424472 54.41168401 25.        ]]\n",
      "-45.0\n",
      "[[ 5.7         3.8        14.         12.71429338 56.37764132 24.        ]]\n",
      "-26.599999999999994\n",
      "[[ 7.6        11.8        14.          8.88301411 40.02000141 23.        ]]\n",
      "-13.919999999999987\n",
      "[[ 3.5   33.3   14.    37.412 52.147 22.   ]]\n",
      "82.76000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.3   23.9   17.     8.919 38.282 21.   ]]\n",
      "-7.160000000000025\n",
      "[[17.2        11.7        17.         10.07785179 31.66123168 20.        ]]\n",
      "-79.51999999999998\n",
      "[[ 9.3        24.6        17.         23.69564803 18.9162301  19.        ]]\n",
      "15.479999999999961\n",
      "[[ 3.         11.5        18.         35.00554754 11.74460889 18.        ]]\n",
      "16.400000000000006\n",
      "[[20.3   23.5   20.     7.053 44.978 17.   ]]\n",
      "-62.839999999999975\n",
      "[[ 9.5        11.1        20.          9.06034251 29.32765094 16.        ]]\n",
      "-29.080000000000013\n",
      "[[ 3.2        24.5        20.         34.84024713 38.75742604 15.        ]]\n",
      "56.640000000000015\n",
      "[[ 2.1    6.1   20.    30.551 31.769 14.   ]]\n",
      "5.240000000000009\n",
      "[[23.7         6.5        20.         14.34032628 45.67481292 13.        ]]\n",
      "-140.35999999999999\n",
      "[[18.5        42.1        20.         36.74439918 35.36247028 12.        ]]\n",
      "8.920000000000016\n",
      "[[ 5.3   16.5   21.    16.861 44.075 11.   ]]\n",
      "16.75999999999999\n",
      "[[15.9    3.4   21.     2.192 37.177 10.   ]]\n",
      "-97.24000000000001\n",
      "[[ 7.         15.         21.          5.41749903 53.85540111  9.        ]]\n",
      "0.4000000000000057\n",
      "[[ 9.         13.9        22.         21.93524775 43.78657123  8.        ]]\n",
      "-16.72\n",
      "[[20.3         8.5        22.          1.13202917 23.96424307  7.        ]]\n",
      "-110.84\n",
      "[[19.9        35.7         1.         45.58895018 34.20115457  6.        ]]\n",
      "-21.079999999999984\n",
      "[[ 4.6        16.4         7.         53.08819085 46.3218985   5.        ]]\n",
      "21.200000000000017\n",
      "[[16.5    5.3   11.    48.625 60.     4.   ]]\n",
      "-95.24000000000001\n",
      "[[ 6.2        10.7        13.         57.49160892 52.19449471  3.        ]]\n",
      "-7.9199999999999875\n",
      "driver reward  -899.0399999999998\n",
      "[[ 9.4        21.3         5.         20.79900516  4.4955353  40.        ]]\n",
      "4.239999999999981\n",
      "[[10.3        27.1         9.         40.76106336 12.01423723 39.        ]]\n",
      "16.67999999999998\n",
      "[[ 5.4   33.8   13.    11.971 37.294 38.   ]]\n",
      "71.44000000000005\n",
      "[[ 1.7        25.4        13.         28.11151331 59.01755155 37.        ]]\n",
      "69.72000000000003\n",
      "[[ 3.8         5.2        14.         21.35822847 54.57313947 36.        ]]\n",
      "-9.199999999999996\n",
      "[[ 4.2        16.         14.          3.43353056 57.54881884 35.        ]]\n",
      "22.640000000000015\n",
      "[[ 5.1        44.8        14.         29.27607044 20.65753661 34.        ]]\n",
      "108.68\n",
      "[[ 3.1         7.2        19.         27.06216592 24.81308422 33.        ]]\n",
      "1.9599999999999937\n",
      "[[15.         12.         20.         24.74668392  6.44716912 32.        ]]\n",
      "-63.599999999999994\n",
      "[[14.3        26.         20.          0.9335949  38.83810186 31.        ]]\n",
      "-14.039999999999964\n",
      "[[18.1        22.8        22.         17.4764908   1.46538213 30.        ]]\n",
      "-50.120000000000005\n",
      "[[ 2.1        14.6        12.          6.42491219 13.66646657 29.        ]]\n",
      "32.44000000000001\n",
      "[[ 8.3        31.         13.         22.72472677 30.30295807 28.        ]]\n",
      "42.76000000000005\n",
      "[[24.6        17.5        13.         16.39066504 34.38223289 27.        ]]\n",
      "-111.28000000000003\n",
      "[[ 7.3         2.4        13.         15.96448014 26.02381595 26.        ]]\n",
      "-41.959999999999994\n",
      "[[10.7         6.6        13.          9.17716444 23.72546749 25.        ]]\n",
      "-51.63999999999997\n",
      "[[ 6.8        31.3        13.         31.95899423 53.35928457 24.        ]]\n",
      "53.920000000000016\n",
      "[[ 9.4   28.4   14.     6.972 29.639 23.   ]]\n",
      "26.960000000000036\n",
      "[[ 7.1        15.9        14.         14.65519644 24.17572802 22.        ]]\n",
      "2.5999999999999943\n",
      "[[ 8.         26.5        15.          9.4254992  48.05681793 21.        ]]\n",
      "30.400000000000006\n",
      "[[ 1.6         3.6        15.          7.74092748 50.32636721 20.        ]]\n",
      "0.6400000000000006\n",
      "[[ 6.4         7.7        15.         12.44374318 38.5294993  19.        ]]\n",
      "-18.88000000000001\n",
      "[[ 7.8        12.3        16.          6.42029857 38.9734363  18.        ]]\n",
      "-13.680000000000007\n",
      "[[10.         52.6        16.         30.9455729   3.82910153 17.        ]]\n",
      "100.32\n",
      "[[25.         13.1        19.         19.21118168  8.55347859 16.        ]]\n",
      "-128.07999999999998\n",
      "[[22.9        10.3        19.          4.87118079 14.60556322 15.        ]]\n",
      "-122.76000000000002\n",
      "[[ 7.9         9.         20.          9.63377125 28.93711842 14.        ]]\n",
      "-24.919999999999987\n",
      "[[14.6        13.9        20.         15.8994795  41.92495867 13.        ]]\n",
      "-54.79999999999998\n",
      "[[ 3.         15.5        20.         23.01418947 54.9044871  12.        ]]\n",
      "29.200000000000003\n",
      "[[25.5        13.6        20.         16.77820099 37.86805979 11.        ]]\n",
      "-129.88\n",
      "[[13.6        29.3        21.         24.93313131 54.74913979 10.        ]]\n",
      "1.2800000000000296\n",
      "[[15.3         9.         21.         21.3145612  48.77226337  9.        ]]\n",
      "-75.24000000000001\n",
      "[[ 5.1    6.4   21.    17.406 44.094  8.   ]]\n",
      "-14.200000000000003\n",
      "[[15.6        23.         21.         23.01707333 38.87531366  7.        ]]\n",
      "-32.48000000000002\n",
      "[[10.5   10.6   22.     5.216 28.622  6.   ]]\n",
      "-37.48000000000002\n",
      "[[ 2.2        29.3        22.         13.95536725 54.30207637  5.        ]]\n",
      "78.80000000000001\n",
      "[[19.4    4.7    0.     5.76  36.104  4.   ]]\n",
      "-116.87999999999997\n",
      "[[10.         16.1         1.         14.52557755 33.09602977  3.        ]]\n",
      "-16.480000000000018\n",
      "[[ 6.8        39.4         4.         38.87912585 11.41334824  2.        ]]\n",
      "79.84000000000003\n",
      "[[ 8.9   7.5   9.   25.73 10.2   1.  ]]\n",
      "1463.48\n",
      "[[ 9.    13.2    9.    26.144 32.332  0.   ]]\n",
      "-18.95999999999998\n",
      "[[25.9        51.         10.         35.38273742  7.47787005 -1.        ]]\n",
      "-12.920000000000073\n",
      "[[ 6.2   39.8   17.     8.09  43.709 -1.   ]]\n",
      "85.19999999999999\n",
      "[[ 3.5        23.2        18.          1.2889451  20.45783268 -1.        ]]\n",
      "50.44\n",
      "[[ 8.7        37.1        18.         11.62460716 49.16300612 -1.        ]]\n",
      "59.56\n",
      "[[ 7.4        26.2        18.          6.90677661 22.5473652  -1.        ]]\n",
      "33.52000000000001\n",
      "[[ 2.8        13.8        18.         15.12578836  8.33998062 -1.        ]]\n",
      "25.11999999999999\n",
      "[[ 5.8    8.    19.     4.046 15.382 -1.   ]]\n",
      "-13.840000000000003\n",
      "[[ 6.9    9.2   19.     4.584 29.104 -1.   ]]\n",
      "-17.480000000000004\n",
      "[[11.8         4.8        19.          6.23204121 14.65973932 -1.        ]]\n",
      "-64.88000000000001\n",
      "[[ 4.8        17.         19.         18.89072573  7.35208348 -1.        ]]\n",
      "21.75999999999999\n",
      "[[20.7        13.6        20.         13.74111441 12.33556985 -1.        ]]\n",
      "-97.23999999999998\n",
      "[[14.3        27.6        20.         12.08251622 52.48986832 -1.        ]]\n",
      "-8.920000000000016\n",
      "[[16.2        40.4        21.         22.38063099  4.28811491 -1.        ]]\n",
      "19.12000000000006\n",
      "[[19.          3.2        21.          2.49551938  0.66068255 -1.        ]]\n",
      "-118.95999999999998\n",
      "[[13.2        14.9        10.         13.67364673 26.38142262 -1.        ]]\n",
      "-42.08000000000001\n",
      "[[10.9         6.7        11.         10.40486157 34.31109724 -1.        ]]\n",
      "-52.68000000000001\n",
      "[[ 5.4         9.9        11.          0.47315654 45.77862596 -1.        ]]\n",
      "-5.040000000000006\n",
      "[[ 9.6        28.5        11.         33.41169245 59.58376383 -1.        ]]\n",
      "25.920000000000016\n",
      "[[ 3.3   14.    11.    37.496 43.528 -1.   ]]\n",
      "22.36\n",
      "[[ 6.5        22.2        12.         54.42074085 49.94125131 -1.        ]]\n",
      "26.840000000000003\n",
      "[[ 9.4   16.7   16.    42.24  27.415 -1.   ]]\n",
      "-10.480000000000018\n",
      "[[11.9        10.3        17.         48.00142513 44.30831474 -1.        ]]\n",
      "-47.96000000000001\n",
      "[[13.9        18.9        18.         51.43557208 58.44204465 -1.        ]]\n",
      "-34.039999999999964\n",
      "[[ 1.6        18.2         8.         41.46204369 44.43882672 -1.        ]]\n",
      "47.359999999999985\n",
      "[[23.8   18.8    9.     2.739 60.    -1.   ]]\n",
      "-101.68\n",
      "[[13.6        18.6        10.         20.2097489  50.05715009 -1.        ]]\n",
      "-32.96000000000001\n",
      "[[14.1         3.7        10.          6.54121266 55.0833482  -1.        ]]\n",
      "-84.04\n",
      "[[11.4        13.4        11.         16.44003831 39.87264897 -1.        ]]\n",
      "-34.639999999999986\n",
      "[[ 8.3        40.9        11.         54.34555392 24.14643958 -1.        ]]\n",
      "74.44\n",
      "[[ 5.4        36.8        13.         22.70551149 34.58800117 -1.        ]]\n",
      "81.04000000000002\n",
      "[[ 6.2        23.         13.         36.22628878 48.28162774 -1.        ]]\n",
      "31.439999999999998\n",
      "[[15.4        26.         14.         22.963737   57.27537261 -1.        ]]\n",
      "-21.519999999999982\n",
      "[[10.1        19.8        14.         36.1984784  51.44381343 -1.        ]]\n",
      "-5.319999999999993\n",
      "[[ 1.6   25.    14.    59.041 46.098 -1.   ]]\n",
      "69.12\n",
      "driver reward  958.0000000000003\n",
      "[[11.7        27.8         6.          2.6259339  46.03700771 40.        ]]\n",
      "9.400000000000034\n",
      "[[ 7.4        15.2         7.         15.31257671 37.33213476 39.        ]]\n",
      "-1.6800000000000068\n",
      "[[ 4.8   26.9    8.    46.788 36.342 38.   ]]\n",
      "53.44\n",
      "[[18.8   23.     7.     5.051 38.697 37.   ]]\n",
      "-54.23999999999995\n",
      "[[ 4.         33.1         7.         33.02741276 25.36579018 36.        ]]\n",
      "78.72\n",
      "[[ 4.7        25.          7.          6.54496265 17.83869375 35.        ]]\n",
      "48.04000000000002\n",
      "[[ 6.9        35.5         7.         33.61820722 28.80433755 34.        ]]\n",
      "66.68\n",
      "[[ 4.3   24.1    7.    45.634 47.017 33.   ]]\n",
      "47.879999999999995\n",
      "[[ 7.5        32.9        15.         44.92074392 17.67797698 32.        ]]\n",
      "54.28000000000003\n",
      "[[ 3.7        19.1        20.         58.99065376 27.66027517 31.        ]]\n",
      "35.96000000000001\n",
      "[[16.8        13.6        11.         59.57839078 12.08260202 30.        ]]\n",
      "-70.72\n",
      "[[ 5.         19.2        14.         43.75712195 30.3418571  29.        ]]\n",
      "27.439999999999998\n",
      "[[ 7.4         7.2        15.         44.21809066 43.53026196 28.        ]]\n",
      "-27.28\n",
      "[[ 5.9    6.8   16.    32.528 41.229 27.   ]]\n",
      "-18.36\n",
      "[[19.3        33.5        17.         42.04958888 30.61750699 26.        ]]\n",
      "-24.039999999999964\n",
      "[[ 4.9        44.1         5.          2.85048439 50.34350321 25.        ]]\n",
      "107.80000000000001\n",
      "[[14.2         7.2         6.         18.39356715 41.36867357 24.        ]]\n",
      "-73.51999999999998\n",
      "[[ 5.8        11.9         7.         28.88780706 51.87654715 23.        ]]\n",
      "-1.3599999999999852\n",
      "[[22.          8.          7.          5.06790225 40.07623195 22.        ]]\n",
      "-124.0\n",
      "[[ 7.2        19.1         7.         19.94689437 54.42328066 21.        ]]\n",
      "12.159999999999997\n",
      "[[14.2        35.7         7.         36.59518461 26.38574748 20.        ]]\n",
      "17.67999999999995\n",
      "[[23.          4.1         9.         16.93509653 44.25121228 19.        ]]\n",
      "-143.28\n",
      "[[ 9.2         6.6         9.          2.77365239 46.48289443 18.        ]]\n",
      "-41.43999999999998\n",
      "[[ 5.         17.4         9.         17.30178507 33.08382306 17.        ]]\n",
      "21.680000000000007\n",
      "[[15.9        32.7         9.         26.53362073 47.64307377 16.        ]]\n",
      "-3.480000000000018\n",
      "[[ 3.9        12.7         9.         28.25725487 37.57248425 15.        ]]\n",
      "14.120000000000019\n",
      "[[ 7.1        12.7        10.         12.79972718 46.88524486 14.        ]]\n",
      "-7.639999999999986\n",
      "[[ 6.5        37.2        10.         45.29117475 41.37001345 13.        ]]\n",
      "74.83999999999997\n",
      "[[ 6.4        11.6        12.         39.99883178 35.88691211 12.        ]]\n",
      "-6.3999999999999915\n",
      "[[ 3.4   10.1   15.    51.622 29.306 11.   ]]\n",
      "9.200000000000003\n",
      "[[ 3.9        18.9        21.         29.57438835 34.70017446 10.        ]]\n",
      "33.960000000000036\n",
      "[[ 6.5        15.9        21.         30.35669491 24.35556912  9.        ]]\n",
      "6.680000000000007\n",
      "[[ 7.2        13.1        22.         15.74230957 26.88340386  8.        ]]\n",
      "-7.039999999999992\n",
      "[[21.         19.2         0.          2.79460379 24.4482283   7.        ]]\n",
      "-81.36000000000001\n",
      "[[ 3.3        29.4         3.         33.31051275 14.59722801  6.        ]]\n",
      "71.64000000000004\n",
      "[[ 7.8   46.4    8.    22.352 56.874  5.   ]]\n",
      "95.44000000000005\n",
      "[[ 7.6        16.4         8.         18.04337359 36.58571338  4.        ]]\n",
      "0.8000000000000114\n",
      "[[14.6         7.7         8.          4.78664111 28.7628494   3.        ]]\n",
      "-74.64000000000001\n",
      "[[ 2.3         8.9         8.          0.82930487 20.27884421  2.        ]]\n",
      "12.840000000000003\n",
      "[[14.5         9.          9.          9.49465826 19.09030108  1.        ]]\n",
      "1430.2\n",
      "[[11.7         5.8         9.          8.83814241 31.52882952  0.        ]]\n",
      "-61.0\n",
      "[[ 5.4         2.          9.          4.64659307 25.42187925 -1.        ]]\n",
      "-30.32\n",
      "[[ 3.1        13.4         9.         19.78222743 20.32272765 -1.        ]]\n",
      "21.799999999999997\n",
      "[[ 8.8        17.9         9.         11.51604903 45.1316712  -1.        ]]\n",
      "-2.5600000000000023\n",
      "[[10.1        11.2         9.          6.28435573 29.92746447 -1.        ]]\n",
      "-32.839999999999975\n",
      "[[ 4.4        15.8         9.         18.85436229 23.78231818 -1.        ]]\n",
      "20.639999999999986\n",
      "[[16.         33.9        10.         34.03106873 13.04908952 -1.        ]]\n",
      "-0.3199999999999932\n",
      "[[10.3        24.8        13.          9.53465684 38.07272347 -1.        ]]\n",
      "9.319999999999993\n",
      "[[ 7.          9.5        13.          0.49637989 35.59395734 -1.        ]]\n",
      "-17.200000000000003\n",
      "[[ 9.5        13.2        13.          7.51354778 14.73500825 -1.        ]]\n",
      "-22.359999999999985\n",
      "[[ 3.1        24.4        13.         25.99645707 25.46245085 -1.        ]]\n",
      "57.0\n",
      "[[18.1    6.2   13.     3.978 34.774 -1.   ]]\n",
      "-103.24000000000001\n",
      "[[ 4.7        20.2        13.          7.61599263 10.39986823 -1.        ]]\n",
      "32.68000000000001\n",
      "[[ 8.5        16.9        13.         19.62018585 29.48792989 -1.        ]]\n",
      "-3.719999999999999\n",
      "[[13.3   22.5   14.     3.542 24.479 -1.   ]]\n",
      "-18.43999999999997\n",
      "[[ 5.4        29.9        14.         33.7435948  42.77358426 -1.        ]]\n",
      "58.960000000000036\n",
      "[[18.3        11.         14.          9.20537199 56.59156803 -1.        ]]\n",
      "-89.24000000000001\n",
      "[[18.3        12.3        14.          2.70578637 27.12997822 -1.        ]]\n",
      "-85.08000000000001\n",
      "[[26.8        12.9        14.          2.23705923 49.3130671  -1.        ]]\n",
      "-140.96000000000004\n",
      "[[16.2        20.2        14.          8.12421526 40.75125292 -1.        ]]\n",
      "-45.51999999999998\n",
      "[[ 8.8   16.8   15.     3.496 57.635 -1.   ]]\n",
      "-6.0800000000000125\n",
      "[[15.3        13.3        15.          5.60542015 56.85088265 -1.        ]]\n",
      "-61.48000000000002\n",
      "[[10.7        29.4        15.         15.37402636 18.75702423 -1.        ]]\n",
      "21.32000000000005\n",
      "[[12.3        14.5        15.         19.24071291 12.99960621 -1.        ]]\n",
      "-37.24000000000001\n",
      "[[14.         18.3        15.         23.61473604 11.62602742 -1.        ]]\n",
      "-36.639999999999986\n",
      "[[17.4        45.4        15.         46.22525968 30.96723255 -1.        ]]\n",
      "26.960000000000036\n",
      "driver reward  1024.8400000000006\n",
      "[[ 4.2        20.2         7.         13.2054256   3.13802804 40.        ]]\n",
      "36.08000000000001\n",
      "[[ 8.9   16.2    9.    13.513 26.325 39.   ]]\n",
      "-8.680000000000007\n",
      "[[10.6         8.1         9.         19.49715652 10.38729145 38.        ]]\n",
      "-46.16\n",
      "[[10.          6.2         9.         17.3429296  14.17076989 37.        ]]\n",
      "-48.16\n",
      "[[ 3.6   11.2    9.     9.295 18.257 36.   ]]\n",
      "11.360000000000014\n",
      "[[ 4.8         2.9        10.         10.04333958 20.28163005 35.        ]]\n",
      "-23.359999999999992\n",
      "[[11.1        16.6        11.         28.56133868 27.46208233 34.        ]]\n",
      "-22.360000000000014\n",
      "[[ 5.2   30.6   13.    13.716 48.896 33.   ]]\n",
      "62.559999999999974\n",
      "[[ 3.9        27.8        13.         28.73449261 21.29952597 32.        ]]\n",
      "62.44\n",
      "[[13.1         5.         16.         21.43948025 24.88132309 31.        ]]\n",
      "-73.08000000000001\n",
      "[[ 4.9        25.2        17.         40.16808913 42.6739857  30.        ]]\n",
      "47.31999999999999\n",
      "[[ 9.3        15.6        20.         39.36393082 58.65327186 29.        ]]\n",
      "-13.319999999999993\n",
      "[[11.9        17.1        21.         21.12669379 36.38147357 28.        ]]\n",
      "-26.19999999999999\n",
      "[[ 4.7        24.7        21.         37.84255965 20.18041346 27.        ]]\n",
      "47.08000000000001\n",
      "[[20.         16.1        10.         19.29602509 16.58780432 26.        ]]\n",
      "-84.47999999999999\n",
      "[[12.7    2.9   11.    18.561 28.995 25.   ]]\n",
      "-77.08\n",
      "[[ 6.3        28.5        12.         13.42668841  4.82154484 24.        ]]\n",
      "48.360000000000014\n",
      "[[ 9.8   16.9   14.     3.652 29.461 23.   ]]\n",
      "-12.560000000000002\n",
      "[[ 1.9    5.2   14.     7.847 31.996 22.   ]]\n",
      "3.720000000000006\n",
      "[[ 5.4        14.2        14.         19.12314282 33.11091093 21.        ]]\n",
      "8.719999999999999\n",
      "[[13.2         9.6        14.         15.01451193 34.69667439 20.        ]]\n",
      "-59.039999999999964\n",
      "[[ 3.7        35.3        14.         35.42230336  4.3771728  19.        ]]\n",
      "87.80000000000001\n",
      "[[15.9        21.         23.         32.68345612 37.61073862 18.        ]]\n",
      "-40.91999999999999\n",
      "[[23.7         5.5         6.          3.61681047 38.6933706  17.        ]]\n",
      "-143.56\n",
      "[[ 6.5        31.          6.         27.23833128 52.85943519 16.        ]]\n",
      "55.0\n",
      "[[ 3.6   27.9    7.     8.228 34.911 15.   ]]\n",
      "64.80000000000001\n",
      "[[ 6.          7.7         7.          8.76362199 48.31157458 14.        ]]\n",
      "-16.159999999999997\n",
      "[[ 4.9        11.6         7.          9.06136295 36.37713047 13.        ]]\n",
      "3.799999999999997\n",
      "[[10.3        29.2         7.         12.96099258 17.09013371 12.        ]]\n",
      "23.400000000000034\n",
      "[[14.2        11.8         8.          0.12361164 18.26347297 11.        ]]\n",
      "-58.79999999999998\n",
      "[[16.9         8.5         8.          0.86548644 37.77315033 10.        ]]\n",
      "-87.72\n",
      "[[ 4.4        20.9         8.          3.61177312 17.9831518   9.        ]]\n",
      "36.960000000000036\n",
      "[[11.1         4.1         9.          5.45373709 32.87658931  8.        ]]\n",
      "-62.36\n",
      "[[ 8.         22.6         9.         18.30849528 46.22739142  7.        ]]\n",
      "17.919999999999987\n",
      "[[12.6         3.6         9.          9.77762863 50.61073454  6.        ]]\n",
      "-74.16\n",
      "[[ 9.8        14.4         9.         12.5994121  32.58713056  5.        ]]\n",
      "-20.560000000000002\n",
      "[[ 4.5        29.6         9.         17.15281266  2.82333386  4.        ]]\n",
      "64.12\n",
      "[[19.4        12.5        11.          5.08325214  7.49035879  3.        ]]\n",
      "-91.91999999999999\n",
      "[[ 3.8    3.4   12.    11.08  11.502  2.   ]]\n",
      "-14.959999999999994\n",
      "[[ 6.5        14.5        12.          9.41071072 30.42539832  1.        ]]\n",
      "1502.2\n",
      "[[ 5.1        12.3        12.          3.72049468 46.87904781  0.        ]]\n",
      "4.680000000000007\n",
      "[[ 6.9        29.5        12.         20.9402229  21.88480005 -1.        ]]\n",
      "47.48000000000002\n",
      "[[23.9         1.9        12.          3.04687635 36.05469332 -1.        ]]\n",
      "-156.43999999999997\n",
      "[[ 6.3         6.3        12.          9.74941536 26.7803121  -1.        ]]\n",
      "-22.679999999999993\n",
      "[[17.9        23.8        12.         15.66269077 22.40796595 -1.        ]]\n",
      "-45.56\n",
      "[[13.          9.7        12.         14.04152713 33.10378737 -1.        ]]\n",
      "-57.359999999999985\n",
      "[[ 7.9         5.7        12.         11.29990391 43.02599205 -1.        ]]\n",
      "-35.480000000000004\n",
      "[[ 3.9        17.4        12.         16.83741571 57.35787696 -1.        ]]\n",
      "29.160000000000025\n",
      "[[16.1        41.2        13.         44.01133265 27.79551495 -1.        ]]\n",
      "22.359999999999957\n",
      "[[ 4.2   36.4   13.     6.405 26.516 -1.   ]]\n",
      "87.92000000000002\n",
      "[[ 3.1       18.5       13.         7.986373  48.0652438 -1.       ]]\n",
      "38.120000000000005\n",
      "[[ 7.2        11.5        13.          7.23078505 59.51799135 -1.        ]]\n",
      "-12.159999999999997\n",
      "[[ 5.9         5.         13.          8.66952852 52.26285085 -1.        ]]\n",
      "-24.120000000000005\n",
      "[[ 6.2         7.4        13.          7.78017143 55.22494643 -1.        ]]\n",
      "-18.480000000000004\n",
      "[[ 6.4         5.         14.          5.51816925 55.89882214 -1.        ]]\n",
      "-27.519999999999996\n",
      "[[17.5         9.8        14.          3.35928462 48.52763376 -1.        ]]\n",
      "-87.63999999999999\n",
      "[[ 4.4        13.7        14.         11.31882928 37.07659836 -1.        ]]\n",
      "13.919999999999987\n",
      "[[10.1        10.9        15.          1.15813659 39.64026664 -1.        ]]\n",
      "-33.79999999999998\n",
      "[[ 9.2        43.2        15.         31.2049037   9.02050246 -1.        ]]\n",
      "75.67999999999995\n",
      "[[16.2        12.8        16.          8.55088674  3.3756512  -1.        ]]\n",
      "-69.19999999999999\n",
      "[[13.8         9.8        17.          0.92904921 25.2608306  -1.        ]]\n",
      "-62.48000000000002\n",
      "[[ 5.1         4.5        17.          5.88727097 31.97211895 -1.        ]]\n",
      "-20.28\n",
      "[[14.2         6.1        17.          1.21005356 42.80973511 -1.        ]]\n",
      "-77.03999999999996\n",
      "[[ 5.5        18.6        17.         12.28509375 22.70444026 -1.        ]]\n",
      "22.120000000000005\n",
      "[[11.3        25.3        17.         21.86533616 44.42600696 -1.        ]]\n",
      "4.1200000000000045\n",
      "[[ 7.9        18.1        17.         44.04745671 41.97971348 -1.        ]]\n",
      "4.200000000000017\n",
      "[[25.7        24.9        11.         51.02521602 25.83693058 -1.        ]]\n",
      "-95.07999999999993\n",
      "[[14.    24.2    7.    56.992 38.13  -1.   ]]\n",
      "-17.75999999999999\n",
      "[[13.1        24.3        12.         45.55839478  5.26736456 -1.        ]]\n",
      "-11.319999999999993\n",
      "[[ 6.6   32.4   13.    19.446 26.11  -1.   ]]\n",
      "58.80000000000001\n",
      "[[ 6.5         8.9        13.         20.87080638 17.06551972 -1.        ]]\n",
      "-15.719999999999999\n",
      "[[15.6   12.2   14.     5.559 40.224 -1.   ]]\n",
      "-67.03999999999996\n",
      "[[ 2.         13.         14.         17.60369695 37.63954206 -1.        ]]\n",
      "28.0\n",
      "[[ 5.1        14.4        15.         16.16256746 28.10534584 -1.        ]]\n",
      "11.400000000000006\n",
      "[[11.8         6.7        16.          4.80694734 23.38686778 -1.        ]]\n",
      "-58.8\n",
      "[[ 1.2        27.         17.         16.32341857 46.52772534 -1.        ]]\n",
      "78.24000000000001\n",
      "[[11.9        29.         17.          6.35247654 12.10190869 -1.        ]]\n",
      "11.879999999999995\n",
      "[[ 3.3        14.4        17.         20.71089772  7.96803917 -1.        ]]\n",
      "23.640000000000015\n",
      "[[12.7         9.2        17.         11.39879613  4.51428055 -1.        ]]\n",
      "-56.91999999999999\n",
      "[[15.7        22.4        18.         24.35355251 25.28580113 -1.        ]]\n",
      "-35.07999999999993\n",
      "[[25.3        14.5        18.         18.01204661 47.56813498 -1.        ]]\n",
      "-125.63999999999999\n",
      "[[24.2   27.7   19.     5.04  57.985 -1.   ]]\n",
      "-75.91999999999996\n",
      "[[ 8.2        24.1        19.         19.32959726 29.88826073 -1.        ]]\n",
      "21.360000000000014\n",
      "[[23.2        17.8        19.          5.26707812 28.38125923 -1.        ]]\n",
      "-100.80000000000001\n",
      "[[ 1.1        25.         19.         27.715318   41.33937079 -1.        ]]\n",
      "72.51999999999998\n",
      "[[ 5.8         9.7        19.         29.51153097 44.90388967 -1.        ]]\n",
      "-8.399999999999991\n",
      "[[ 6.2        12.3        19.         25.35100798 38.26162336 -1.        ]]\n",
      "-2.799999999999997\n",
      "[[13.1         7.8        19.          4.86061206 36.56313126 -1.        ]]\n",
      "-64.11999999999998\n",
      "[[ 8.3    9.1   19.    11.168 27.818 -1.   ]]\n",
      "-27.319999999999993\n",
      "[[ 5.5        46.6        19.         53.46145356 42.78179706 -1.        ]]\n",
      "111.72000000000003\n",
      "[[ 2.7        30.         11.         46.44349516 14.4931565  -1.        ]]\n",
      "77.63999999999999\n",
      "[[11.8   15.5    6.    23.75  19.099 -1.   ]]\n",
      "-30.639999999999986\n",
      "[[15.          4.6         6.          8.02752254 30.78563142 -1.        ]]\n",
      "-87.28\n",
      "[[11.5         5.          7.         16.92844452 32.79911604 -1.        ]]\n",
      "-62.2\n",
      "[[10.        11.         7.         6.7480646 50.195211  -1.       ]]\n",
      "-32.79999999999998\n",
      "[[ 7.6    5.3    7.    10.161 53.634 -1.   ]]\n",
      "-34.719999999999985\n",
      "[[10.         16.2         7.         32.53035203 46.43062269 -1.        ]]\n",
      "-16.159999999999997\n",
      "[[18.5         6.8         7.         19.9352807  49.26415603 -1.        ]]\n",
      "-104.03999999999999\n",
      "[[15.7        15.8         7.         19.3939779  46.69730779 -1.        ]]\n",
      "-56.19999999999999\n",
      "[[18.2        15.1         8.         16.06560349 52.63978784 -1.        ]]\n",
      "-75.43999999999997\n",
      "[[ 6.1         3.3         8.         17.31426115 47.89924407 -1.        ]]\n",
      "-30.919999999999987\n",
      "[[ 1.    12.4    8.     5.536 45.558 -1.   ]]\n",
      "32.879999999999995\n",
      "[[ 4.1         6.7         8.          2.21308373 41.50864315 -1.        ]]\n",
      "-6.439999999999998\n",
      "[[ 6.8        12.9         8.         10.78913087 37.41255019 -1.        ]]\n",
      "-4.9599999999999795\n",
      "[[ 4.6        35.1         8.         37.87768335  9.43359324 -1.        ]]\n",
      "81.03999999999996\n",
      "[[17.         25.9        12.         47.83921897 12.30859285 -1.        ]]\n",
      "-32.71999999999997\n",
      "[[ 6.5   13.8   17.    34.577 27.654 -1.   ]]\n",
      "-0.03999999999999204\n",
      "[[ 5.2        16.2        18.         33.40154373  6.30307042 -1.        ]]\n",
      "16.480000000000018\n",
      "driver reward  -34.11999999999918\n",
      "[[ 2.3        15.2         1.         24.93903956 31.75784035 40.        ]]\n",
      "33.0\n",
      "[[11.4        24.9         9.          5.53389523  1.21414998 39.        ]]\n",
      "2.160000000000025\n",
      "[[10.1        12.4        11.         26.63730074  8.17680632 38.        ]]\n",
      "-29.0\n",
      "[[ 9.2   12.3   12.    12.929 19.077 37.   ]]\n",
      "-23.19999999999999\n",
      "[[ 6.9   20.9   12.    12.637 33.9   36.   ]]\n",
      "19.960000000000036\n",
      "[[ 3.5        43.9        12.         54.62687566 32.67496865 35.        ]]\n",
      "116.68\n",
      "[[14.6   39.1   19.    12.053 27.627 34.   ]]\n",
      "25.839999999999975\n",
      "[[ 9.3        11.9        19.          7.72174509 23.67336309 33.        ]]\n",
      "-25.160000000000025\n",
      "[[ 1.8    5.8   19.     9.458 17.431 32.   ]]\n",
      "6.32\n",
      "[[17.9        11.5        19.         15.02745179 27.87327373 31.        ]]\n",
      "-84.91999999999999\n",
      "[[ 7.4        28.3        19.         25.30457287 50.06462159 30.        ]]\n",
      "40.23999999999998\n",
      "[[22.8        29.7        19.         26.82644786 58.98241215 29.        ]]\n",
      "-60.0\n",
      "[[17.6        15.5        20.          0.58636765 47.76906272 28.        ]]\n",
      "-70.08000000000001\n",
      "[[12.1   23.9   21.    17.926 40.207 27.   ]]\n",
      "-5.799999999999983\n",
      "[[ 3.7        19.5        21.         36.13902289 37.16938482 26.        ]]\n",
      "37.24000000000001\n",
      "[[ 9.2        18.5         1.         41.39348997 54.81086631 25.        ]]\n",
      "-3.359999999999985\n",
      "[[ 2.8        12.          3.         46.20008683 46.38487765 24.        ]]\n",
      "19.36\n",
      "[[ 5.4   46.3    9.    11.345 18.448 23.   ]]\n",
      "111.44000000000005\n",
      "[[ 2.2   23.2   11.     4.959 43.    22.   ]]\n",
      "59.28\n",
      "[[11.4        36.7        11.         50.54228694 46.16620187 21.        ]]\n",
      "39.920000000000016\n",
      "[[ 9.9         4.1        16.         44.50996931 53.9347686  20.        ]]\n",
      "-54.2\n",
      "[[12.3        23.         20.         56.66641629 22.1243296  19.        ]]\n",
      "-10.039999999999964\n",
      "[[ 2.6   22.7    9.    41.319 35.606 18.   ]]\n",
      "54.96000000000001\n",
      "[[10.9   26.2   11.    42.095 18.17  17.   ]]\n",
      "9.719999999999999\n",
      "[[ 4.8        15.1         6.         22.95002911 23.03372716 16.        ]]\n",
      "15.680000000000007\n",
      "[[23.7        32.6         7.         30.48863601 17.90656105 15.        ]]\n",
      "-56.839999999999975\n",
      "[[26.4        27.8         8.         40.5220701  37.57883793 14.        ]]\n",
      "-90.56\n",
      "[[ 4.    28.2   13.    12.96  31.568 13.   ]]\n",
      "63.03999999999999\n",
      "[[ 5.8        19.1        13.         14.9664308  15.58149264 12.        ]]\n",
      "21.67999999999998\n",
      "[[14.1         5.5        14.          6.99500563 18.85532075 11.        ]]\n",
      "-78.28\n",
      "[[ 4.2         2.8        14.          5.00614254 20.61021498 10.        ]]\n",
      "-19.6\n",
      "[[ 4.7         9.         14.         17.04497259 15.56766711  9.        ]]\n",
      "-3.1599999999999966\n",
      "[[ 5.9   21.    15.     6.578 28.973  8.   ]]\n",
      "27.080000000000013\n",
      "[[11.         20.2        15.         21.14403402 56.55212425  7.        ]]\n",
      "-10.159999999999997\n",
      "[[15.     7.7   17.     2.804 51.521  6.   ]]\n",
      "-77.35999999999999\n",
      "[[10.7    7.1   17.     3.723 47.164  5.   ]]\n",
      "-50.03999999999998\n",
      "[[ 2.5         5.7        17.          2.46108124 55.30479972  4.        ]]\n",
      "1.240000000000009\n",
      "[[ 0.4         6.6        17.          7.31314957 50.21172802  3.        ]]\n",
      "18.4\n",
      "[[ 7.         12.1        17.         21.22472115 49.05112651  2.        ]]\n",
      "-8.879999999999995\n",
      "[[11.3        13.2        17.         11.56284876 30.22364295  1.        ]]\n",
      "1465.4\n",
      "[[ 9.4   20.8   17.     1.948  4.621  0.   ]]\n",
      "2.6399999999999864\n",
      "[[10.2        18.3        18.          7.68853529 32.27630873 -1.        ]]\n",
      "-10.799999999999983\n",
      "[[ 4.5        23.5        18.         25.14145756 40.63946196 -1.        ]]\n",
      "44.599999999999994\n",
      "[[ 9.2        25.1        18.         36.06240025 28.67543388 -1.        ]]\n",
      "17.76000000000002\n",
      "[[ 6.8   26.5   19.     4.011 32.357 -1.   ]]\n",
      "38.56000000000003\n",
      "[[ 1.6        19.3        19.         15.49169136 49.5125283  -1.        ]]\n",
      "50.879999999999995\n",
      "[[ 3.6        13.9        19.         26.20980474 49.79256318 -1.        ]]\n",
      "20.0\n",
      "[[19.3        27.9        19.         26.68400719 15.97273455 -1.        ]]\n",
      "-41.960000000000036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.8         4.2        20.         21.60846672 20.66063954 -1.        ]]\n",
      "-32.8\n",
      "[[24.2        16.8        20.          1.8177692  54.69909118 -1.        ]]\n",
      "-110.80000000000001\n",
      "[[13.5        15.8        21.         20.01202951 45.66322804 -1.        ]]\n",
      "-41.24000000000001\n",
      "[[ 8.         13.6        21.         25.71851709 44.91085228 -1.        ]]\n",
      "-10.879999999999995\n",
      "[[20.2        18.5        21.         23.74031311 44.63248338 -1.        ]]\n",
      "-78.16000000000003\n",
      "[[10.9        18.6        21.         27.19445546 34.69884163 -1.        ]]\n",
      "-14.599999999999994\n",
      "[[ 5.3        23.1        22.         45.76673648 42.19046339 -1.        ]]\n",
      "37.879999999999995\n",
      "[[ 7.    30.3    9.    10.824 30.088 -1.   ]]\n",
      "49.360000000000014\n",
      "[[14.6        30.         10.         25.08760946 18.27410512 -1.        ]]\n",
      "-3.2800000000000296\n",
      "[[ 8.1         7.1        12.         25.01239249 20.89607199 -1.        ]]\n",
      "-32.36\n",
      "[[16.8        19.6        12.         16.5036237  29.87727795 -1.        ]]\n",
      "-51.52000000000004\n",
      "[[ 6.2        26.5        13.         42.45840725 23.4956081  -1.        ]]\n",
      "42.639999999999986\n",
      "[[ 6.         23.5        19.         29.23920091  5.71172368 -1.        ]]\n",
      "34.400000000000006\n",
      "[[ 8.9         2.5         7.         29.66853988 15.09967052 -1.        ]]\n",
      "-52.519999999999996\n",
      "[[17.3         3.5         8.         17.77555674 27.26188351 -1.        ]]\n",
      "-106.44\n",
      "[[25.8        31.1         8.         33.48972368 48.26164388 -1.        ]]\n",
      "-75.92000000000002\n",
      "[[18.4   12.7    9.    10.277 54.803 -1.   ]]\n",
      "-84.47999999999999\n",
      "[[ 5.6         7.5         9.          9.90832055 49.63000603 -1.        ]]\n",
      "-14.079999999999998\n",
      "[[13.9         7.         10.         10.39373151 29.78081273 -1.        ]]\n",
      "-72.11999999999998\n",
      "[[ 3.7    4.4   10.     3.381 33.637 -1.   ]]\n",
      "-11.080000000000005\n",
      "[[12.9        28.1        10.         26.83103146 31.57055175 -1.        ]]\n",
      "2.1999999999999886\n",
      "[[ 5.2         2.7        11.         20.80485645 30.71782708 -1.        ]]\n",
      "-26.72\n",
      "[[18.5        15.9        11.         24.52821935 47.23729    -1.        ]]\n",
      "-74.91999999999999\n",
      "[[ 5.5        12.3        11.         23.53106943 57.1568157  -1.        ]]\n",
      "1.9599999999999937\n",
      "[[19.8        12.6        12.          1.85936277 44.23527467 -1.        ]]\n",
      "-94.32\n",
      "[[ 7.9    9.4   12.     8.805 28.305 -1.   ]]\n",
      "-23.64\n",
      "[[ 4.2        32.         12.         14.04589115 56.86199793 -1.        ]]\n",
      "73.83999999999997\n",
      "[[12.8        16.4        13.          3.91300415 33.99464157 -1.        ]]\n",
      "-34.56\n",
      "[[ 9.4        22.3        13.         19.13397522 21.89640098 -1.        ]]\n",
      "7.439999999999998\n",
      "[[13.6        18.2        13.         14.45020626 31.36889261 -1.        ]]\n",
      "-34.23999999999998\n",
      "[[ 3.6        42.3        13.         48.29279416  2.4023074  -1.        ]]\n",
      "110.88\n",
      "[[ 4.5         3.3        18.         46.57532135  4.57786399 -1.        ]]\n",
      "-20.04\n",
      "driver reward  809.5600000000005\n",
      "[[ 3.3        17.6         6.         13.02072449 20.97807819 40.        ]]\n",
      "33.879999999999995\n",
      "[[ 1.6        16.1         6.         24.57229713  9.30405995 39.        ]]\n",
      "40.639999999999986\n",
      "[[ 9.1   32.7    9.    16.71  42.942 38.   ]]\n",
      "42.75999999999999\n",
      "[[ 2.6        16.2         9.         32.27985763 37.83962416 37.        ]]\n",
      "34.16\n",
      "[[18.2         4.5        12.         17.45668681 40.40237875 36.        ]]\n",
      "-109.35999999999999\n",
      "[[ 5.7        17.8        12.         15.40101974 21.35843877 35.        ]]\n",
      "18.200000000000017\n",
      "[[ 1.3   23.8   12.    12.576 44.286 34.   ]]\n",
      "67.32\n",
      "[[15.7         7.9        12.          5.59099719 31.14073118 33.        ]]\n",
      "-81.48000000000002\n",
      "[[ 9.6        10.9        12.         18.91198842 18.65875463 32.        ]]\n",
      "-30.400000000000006\n",
      "[[ 3.3   19.2   13.    14.516 33.928 31.   ]]\n",
      "39.0\n",
      "[[ 2.1         2.2        13.         13.13586323 35.39595447 30.        ]]\n",
      "-7.2400000000000055\n",
      "[[ 4.1        19.8        13.         14.23401888 11.92964954 29.        ]]\n",
      "35.48000000000002\n",
      "[[12.5         7.2        14.          5.29460617 25.91191159 28.        ]]\n",
      "-61.95999999999998\n",
      "[[ 2.5        25.5        14.         12.46149016 52.45101981 27.        ]]\n",
      "64.6\n",
      "[[ 9.3        19.2        14.         15.04377205 31.71533294 26.        ]]\n",
      "-1.799999999999983\n",
      "[[ 2.4        13.5        14.         14.02564814 44.83595902 25.        ]]\n",
      "26.879999999999995\n",
      "[[ 6.3         6.2        14.         15.51036692 33.47545304 24.        ]]\n",
      "-23.0\n",
      "[[11.2        23.9        15.          0.49423329 15.93705432 23.        ]]\n",
      "0.32000000000005\n",
      "[[ 9.1   13.1   16.    14.045 32.049 22.   ]]\n",
      "-19.95999999999998\n",
      "[[ 0.2   11.4   16.    16.078 20.594 21.   ]]\n",
      "35.120000000000005\n",
      "[[17.5        19.1        17.         26.57426824 33.3106683  20.        ]]\n",
      "-57.879999999999995\n",
      "[[13.2   13.    18.     7.392 44.442 19.   ]]\n",
      "-48.16\n",
      "[[ 4.7    8.4   18.     7.354 49.254 18.   ]]\n",
      "-5.0800000000000125\n",
      "[[ 8.6        28.7        18.         22.40218833 16.53478924 17.        ]]\n",
      "33.360000000000014\n",
      "[[14.9       27.        18.        30.7977762 29.092833  16.       ]]\n",
      "-14.919999999999959\n",
      "[[16.3        29.4        20.         46.29799953 19.98939071 15.        ]]\n",
      "-16.75999999999999\n",
      "[[22.9   49.4   10.    20.989 32.248 14.   ]]\n",
      "2.3600000000000136\n",
      "[[21.8        18.1        11.          4.4910156  57.03340639 13.        ]]\n",
      "-90.32000000000005\n",
      "[[24.         19.5        12.          6.86109879 50.71408329 12.        ]]\n",
      "-100.80000000000001\n",
      "[[ 7.5   40.6   12.    45.848 48.107 11.   ]]\n",
      "78.92000000000002\n",
      "[[12.4   28.    19.     7.567 46.686 10.   ]]\n",
      "5.28000000000003\n",
      "[[ 0.6        16.8        19.         14.98826555 31.96179082  9.        ]]\n",
      "49.67999999999999\n",
      "[[ 8.5    5.3   19.     5.346 24.929  8.   ]]\n",
      "-40.84\n",
      "[[14.2        13.1        19.         20.13441824 42.10158873  7.        ]]\n",
      "-54.639999999999986\n",
      "[[19.6        10.8        19.         17.19719325 24.96873199  6.        ]]\n",
      "-98.72\n",
      "[[ 4.4   17.3   20.     7.258 14.959  5.   ]]\n",
      "25.439999999999998\n",
      "[[ 4.6        13.7        21.          8.62662719  4.91981334  4.        ]]\n",
      "12.560000000000016\n",
      "[[20.5        29.         22.         34.51997015 12.12020136  3.        ]]\n",
      "-46.599999999999966\n",
      "[[ 8.2   28.4   17.     5.846 28.209  2.   ]]\n",
      "35.12000000000003\n",
      "[[ 3.4        11.5        17.         18.53488333 36.14598852  1.        ]]\n",
      "1513.68\n",
      "[[ 2.2        31.9        17.         48.57835351 47.59196667  0.        ]]\n",
      "87.12\n",
      "[[ 6.8    6.5    7.    53.556 55.262 -1.   ]]\n",
      "-25.439999999999998\n",
      "[[ 7.4   11.7   10.    59.809 55.813 -1.   ]]\n",
      "-12.879999999999995\n",
      "[[ 4.1   43.3    8.    17.315 44.141 -1.   ]]\n",
      "110.68\n",
      "[[ 7.3   22.3    8.     3.272 18.818 -1.   ]]\n",
      "21.72\n",
      "[[ 2.2         2.8         8.          7.25563688 18.03688914 -1.        ]]\n",
      "-6.0\n",
      "[[ 8.9         6.4         8.         13.80778406 20.48363135 -1.        ]]\n",
      "-40.040000000000006\n",
      "[[17.3   15.7    9.     7.666 50.137 -1.   ]]\n",
      "-67.4\n",
      "[[11.6    7.6    9.     5.322 38.171 -1.   ]]\n",
      "-54.56\n",
      "[[ 8.5        10.5         9.         11.87978434 26.95860287 -1.        ]]\n",
      "-24.19999999999999\n",
      "[[13.1    3.5    9.    16.564 42.446 -1.   ]]\n",
      "-77.88000000000001\n",
      "[[11.         10.2         9.         14.63019956 56.28601151 -1.        ]]\n",
      "-42.16\n",
      "[[ 9.         16.6         9.         27.91551293 39.32094574 -1.        ]]\n",
      "-8.080000000000013\n",
      "[[21.1         8.7         9.          0.40140643 47.42542069 -1.        ]]\n",
      "-115.63999999999999\n",
      "[[11.6        19.7         9.         23.34239947 36.42004745 -1.        ]]\n",
      "-15.839999999999975\n",
      "[[17.1        16.3        10.         22.19573898 37.29926003 -1.        ]]\n",
      "-64.12000000000003\n",
      "[[14.4   22.    10.     1.574 48.14  -1.   ]]\n",
      "-27.519999999999982\n",
      "[[ 0.9         4.8        11.          0.94162864 53.83151274 -1.        ]]\n",
      "9.240000000000002\n",
      "[[10.         20.9        11.          3.12982592 23.48049047 -1.        ]]\n",
      "-1.1199999999999761\n",
      "[[ 8.9         7.9        12.         17.50006491 22.56535246 -1.        ]]\n",
      "-35.239999999999995\n",
      "[[ 6.         30.6        12.         31.71495835  0.08873327 -1.        ]]\n",
      "57.120000000000005\n",
      "[[16.4         5.4        13.         13.81485591 11.84420722 -1.        ]]\n",
      "-94.23999999999998\n",
      "[[21.         17.1        13.         19.11180233 40.69954002 -1.        ]]\n",
      "-88.07999999999998\n",
      "[[17.8        34.7        13.         19.67833456  2.73392176 -1.        ]]\n",
      "-10.0\n",
      "[[ 6.3         6.9        14.         20.54357367  3.70580223 -1.        ]]\n",
      "-20.75999999999999\n",
      "[[19.3        23.5        14.         18.80069706 35.99623729 -1.        ]]\n",
      "-56.039999999999964\n",
      "[[ 2.6        10.8        14.          5.77943955 33.81687814 -1.        ]]\n",
      "16.879999999999995\n",
      "[[ 9.         17.         14.         19.87797395 31.10414664 -1.        ]]\n",
      "-6.799999999999983\n",
      "[[11.9        20.4        14.         13.39402027 10.89419335 -1.        ]]\n",
      "-15.639999999999986\n",
      "[[19.1        12.4        14.          3.75602942 18.91057311 -1.        ]]\n",
      "-90.19999999999999\n",
      "[[14.1   25.8   14.     2.851 49.31  -1.   ]]\n",
      "-13.319999999999993\n",
      "[[ 6.7    8.3   15.     8.276 42.729 -1.   ]]\n",
      "-19.0\n",
      "[[11.9         8.7        15.         10.82335994 23.06709974 -1.        ]]\n",
      "-53.08000000000001\n",
      "[[ 8.         11.2        15.         14.90952927 39.89843174 -1.        ]]\n",
      "-18.560000000000002\n",
      "[[20.1        12.2        15.         16.27811026 53.83964887 -1.        ]]\n",
      "-97.63999999999999\n",
      "[[ 8.4   20.6   15.     5.529 39.359 -1.   ]]\n",
      "8.800000000000011\n",
      "[[ 4.7        12.7        15.         15.58292199 43.87945917 -1.        ]]\n",
      "8.680000000000007\n",
      "[[ 6.8         7.4        15.         19.62854574 52.98128416 -1.        ]]\n",
      "-22.559999999999988\n",
      "[[24.4         2.         15.          3.56357783 37.20996146 -1.        ]]\n",
      "-159.51999999999998\n",
      "[[13.3        26.9        16.         12.88357197 50.27136118 -1.        ]]\n",
      "-4.360000000000014\n",
      "[[20.9        15.4        16.          0.14043522 16.59331937 -1.        ]]\n",
      "-92.83999999999997\n",
      "[[ 9.7        15.9        16.         18.532935   33.46301642 -1.        ]]\n",
      "-15.080000000000013\n",
      "[[19.3         6.4        16.          9.52314923 49.40596628 -1.        ]]\n",
      "-110.76000000000002\n",
      "[[ 6.3        21.6        16.         20.18660791 35.96579888 -1.        ]]\n",
      "26.28\n",
      "[[18.5        16.8        16.         17.15783678 19.17169399 -1.        ]]\n",
      "-72.03999999999996\n",
      "[[10.9        25.8        16.          2.23691562 47.64069083 -1.        ]]\n",
      "8.439999999999998\n",
      "[[13.1         6.7        16.          5.53790469 56.29705497 -1.        ]]\n",
      "-67.64000000000001\n",
      "[[ 3.8        16.7        17.         20.04474242 52.90254896 -1.        ]]\n",
      "27.599999999999994\n",
      "[[10.1         3.4        17.         26.86085316 41.81369529 -1.        ]]\n",
      "-57.8\n",
      "[[ 2.4        10.3        17.         15.32473277 42.03617088 -1.        ]]\n",
      "16.64\n",
      "[[13.1        11.7        17.         14.22485989 40.82930959 -1.        ]]\n",
      "-51.639999999999986\n",
      "[[ 5.7        13.9        17.         18.4238164  48.02508174 -1.        ]]\n",
      "5.719999999999999\n",
      "[[14.2        13.8        18.         18.77234635 50.72516216 -1.        ]]\n",
      "-52.400000000000006\n",
      "[[21.1        19.5        18.         22.41002534 36.92406652 -1.        ]]\n",
      "-81.07999999999998\n",
      "[[17.1        37.5        19.         22.19446878 12.1908013  -1.        ]]\n",
      "3.7200000000000273\n",
      "[[15.9        22.6        19.         28.79101489  4.35288314 -1.        ]]\n",
      "-35.80000000000001\n",
      "[[ 3.5         4.1        19.         30.22744108  7.95398686 -1.        ]]\n",
      "-10.68\n",
      "[[23.1        24.9        11.          1.10483012 42.32441095 -1.        ]]\n",
      "-77.39999999999998\n",
      "[[ 5.7        15.7        12.         21.08102581 34.92395461 -1.        ]]\n",
      "11.480000000000018\n",
      "[[19.2         5.7        12.          7.37916793 39.46928286 -1.        ]]\n",
      "-112.32\n",
      "[[ 8.5        13.4        12.          4.3837208  59.41042947 -1.        ]]\n",
      "-14.919999999999987\n",
      "[[ 9.9         5.6        12.         12.01239799 51.04937991 -1.        ]]\n",
      "-49.39999999999999\n",
      "[[ 6.8        35.1        12.         40.37402115 49.68754047 -1.        ]]\n",
      "66.08000000000004\n",
      "[[15.8    9.5   12.    21.372 52.343 -1.   ]]\n",
      "-77.03999999999999\n",
      "[[18.2         2.5        12.          4.71677179 57.58447467 -1.        ]]\n",
      "-115.75999999999999\n",
      "[[ 2.6        22.         12.         16.25494087 41.55693889 -1.        ]]\n",
      "52.72\n",
      "[[ 4.6        20.3        12.          7.98344213 57.24501825 -1.        ]]\n",
      "33.68000000000001\n",
      "[[14.2        17.9        12.         10.79907081 25.26553245 -1.        ]]\n",
      "-39.279999999999944\n",
      "[[ 3.7         6.1        12.          1.61130869 21.95722811 -1.        ]]\n",
      "-5.640000000000001\n",
      "[[12.1        18.1        12.         17.36764947 45.2671605  -1.        ]]\n",
      "-24.360000000000014\n",
      "[[15.9        28.5        12.         14.54647248 24.98012073 -1.        ]]\n",
      "-16.91999999999996\n",
      "[[12.9        21.7        12.          5.83177817 37.45566237 -1.        ]]\n",
      "-18.28\n",
      "[[ 2.9         8.         13.         13.16810513 33.2154033  -1.        ]]\n",
      "5.8799999999999955\n",
      "[[ 1.         15.4        13.          8.70281957 48.74971975 -1.        ]]\n",
      "42.48000000000002\n",
      "[[ 8.4         7.         13.         19.26328997 52.34848862 -1.        ]]\n",
      "-34.72\n",
      "[[14.5        29.3        13.         33.64374391 46.50557438 -1.        ]]\n",
      "-4.839999999999975\n",
      "[[ 6.3    7.    13.    46.479 48.628 -1.   ]]\n",
      "-20.439999999999998\n",
      "[[16.7        26.6        18.         45.4231907  31.76854607 -1.        ]]\n",
      "-28.439999999999998\n",
      "driver reward  -769.6399999999992\n",
      "[[ 6.8   20.8    5.    33.317 30.845 40.   ]]\n",
      "20.319999999999993\n",
      "[[ 6.1        28.8         7.         14.26178117 17.76475486 39.        ]]\n",
      "50.68000000000001\n",
      "[[ 4.8        17.5         8.         27.06357775 17.76876455 38.        ]]\n",
      "23.359999999999985\n",
      "[[ 2.2   28.7    8.     4.005 37.927 37.   ]]\n",
      "76.88000000000002\n",
      "[[14.2        25.6         9.         27.92185188  6.58040926 36.        ]]\n",
      "-14.639999999999986\n",
      "[[21.7    7.2   12.    13.95  14.644 35.   ]]\n",
      "-124.51999999999998\n",
      "[[ 1.4        16.8        12.         14.33953136 30.35260072 34.        ]]\n",
      "44.24000000000001\n",
      "[[13.5        42.6        12.         36.10266978 53.27281638 33.        ]]\n",
      "44.51999999999998\n",
      "[[ 1.5         5.4        16.         30.85030311 51.617617   32.        ]]\n",
      "7.079999999999998\n",
      "[[14.6        24.4        17.         43.36346733 42.83809719 31.        ]]\n",
      "-21.19999999999999\n",
      "[[ 0.5        23.4        20.         36.74916609 19.98997783 30.        ]]\n",
      "71.48000000000002\n",
      "[[26.8        21.8         7.         36.25079085 28.3313546  29.        ]]\n",
      "-112.48000000000002\n",
      "[[15.8        23.5         7.         13.52165897 16.39948476 28.        ]]\n",
      "-32.23999999999995\n",
      "[[ 5.3        12.          8.         19.51986921 11.0875369  27.        ]]\n",
      "2.3599999999999994\n",
      "[[ 4.6        37.7         9.         35.31978527 43.27970687 26.        ]]\n",
      "89.35999999999996\n",
      "[[18.7        12.8         9.         14.6186145  56.45868447 25.        ]]\n",
      "-86.19999999999999\n",
      "[[10.7        25.9         9.         33.10140411 55.12565608 24.        ]]\n",
      "10.120000000000033\n",
      "[[12.         14.1        10.         42.27009304 48.56990441 23.        ]]\n",
      "-36.48000000000002\n",
      "[[ 4.         30.8        11.         11.97715085 31.83405176 22.        ]]\n",
      "71.36000000000001\n",
      "[[ 8.8         4.1        12.          6.03901629 37.65019126 21.        ]]\n",
      "-46.72\n",
      "[[ 7.6        16.7        12.         27.97127746 37.66827888 20.        ]]\n",
      "1.7600000000000193\n",
      "[[15.4        27.1        13.         44.94540891 44.67213189 19.        ]]\n",
      "-18.0\n",
      "[[ 4.5        25.1        15.         16.42565609 44.07315388 18.        ]]\n",
      "49.72\n",
      "[[ 5.8   23.6   15.     2.078 29.824 17.   ]]\n",
      "36.079999999999984\n",
      "[[ 1.6        33.5        16.         29.56250335 51.17623051 16.        ]]\n",
      "96.32\n",
      "[[ 7.    18.9   16.    48.654 57.852 15.   ]]\n",
      "12.880000000000024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.2         9.          9.         31.34687615 37.37524082 14.        ]]\n",
      "-122.16\n",
      "[[12.9        21.2        10.         28.04959806 23.95176778 13.        ]]\n",
      "-19.879999999999995\n",
      "[[ 5.5        20.7        11.         48.84993747 37.48083765 12.        ]]\n",
      "28.840000000000003\n",
      "[[ 2.1        20.8        13.         27.02595797 44.15744678 11.        ]]\n",
      "52.28\n",
      "[[ 2.9   22.2   14.     5.785 52.229 10.   ]]\n",
      "51.32000000000002\n",
      "[[12.8    5.2   14.     6.991 39.803  9.   ]]\n",
      "-70.39999999999999\n",
      "[[ 3.8         6.7        14.         10.63869192 38.24081212  8.        ]]\n",
      "-4.3999999999999915\n",
      "[[ 5.9        19.1        14.         35.27340037 42.0253702   7.        ]]\n",
      "21.0\n",
      "[[ 2.2        12.2        17.         31.55389946 31.19267885  6.        ]]\n",
      "24.080000000000013\n",
      "[[25.7        27.1        17.         34.84457469 32.90927915  5.        ]]\n",
      "-88.03999999999996\n",
      "[[19.9   17.2   17.     5.335 26.705  4.   ]]\n",
      "-80.27999999999994\n",
      "[[ 4.4         3.5        17.          8.28620002 24.99223349  3.        ]]\n",
      "-18.72\n",
      "[[ 3.2         7.5        17.         10.78056582 16.57002907  2.        ]]\n",
      "2.240000000000009\n",
      "[[ 7.1        30.3        17.         38.30645804 25.62225682  1.        ]]\n",
      "1548.68\n",
      "[[ 8.8   27.2   18.     8.866 46.16   0.   ]]\n",
      "27.200000000000017\n",
      "[[ 1.7        17.3        18.         16.91173571 30.57900994 -1.        ]]\n",
      "43.80000000000001\n",
      "[[17.8   10.6   18.     4.038 29.396 -1.   ]]\n",
      "-87.11999999999998\n",
      "[[ 3.3         4.6        18.          5.99945987 36.30343069 -1.        ]]\n",
      "-7.719999999999992\n",
      "[[ 9.7        18.2        18.         22.20462098 37.74131516 -1.        ]]\n",
      "-7.719999999999999\n",
      "[[16.8         8.4        18.         11.30629653 53.94380189 -1.        ]]\n",
      "-87.36000000000001\n",
      "[[15.1        19.9        19.          8.61364267 25.59925947 -1.        ]]\n",
      "-39.0\n",
      "[[ 6.6        27.6        19.         20.40122621  2.13824839 -1.        ]]\n",
      "43.44\n",
      "[[15.3   14.9   19.     5.436 28.261 -1.   ]]\n",
      "-56.360000000000014\n",
      "[[ 3.1   19.6   19.     3.32  46.688 -1.   ]]\n",
      "41.639999999999986\n",
      "[[ 9.3    8.7   19.     2.289 45.283 -1.   ]]\n",
      "-35.39999999999999\n",
      "[[11.6        24.         19.         10.4518068  33.23010186 -1.        ]]\n",
      "-2.0800000000000125\n",
      "[[18.         18.9        19.         15.76773832 29.54473467 -1.        ]]\n",
      "-61.91999999999999\n",
      "[[10.7         4.3        20.          7.22973811 26.35185183 -1.        ]]\n",
      "-59.0\n",
      "[[ 6.8        20.5        20.         23.28684872  5.96078149 -1.        ]]\n",
      "19.360000000000014\n",
      "[[17.2    6.1   21.     7.757 14.668 -1.   ]]\n",
      "-97.43999999999997\n",
      "[[ 5.9        11.6        21.         17.17480982 19.47128928 -1.        ]]\n",
      "-3.0\n",
      "[[ 2.4         8.3        22.         23.66501577 26.94812727 -1.        ]]\n",
      "10.239999999999995\n",
      "[[17.1         9.9         0.         18.52368108 21.90391326 -1.        ]]\n",
      "-84.6\n",
      "[[21.2    7.4    4.     5.119 46.76  -1.   ]]\n",
      "-120.48000000000002\n",
      "[[ 2.9         5.3         4.          7.14183998 50.03506931 -1.        ]]\n",
      "-2.759999999999991\n",
      "[[16.2        37.6         5.         50.58650525 36.85815172 -1.        ]]\n",
      "10.160000000000025\n",
      "[[12.8   22.2   13.    60.    39.815 -1.   ]]\n",
      "-16.0\n",
      "[[ 4.1        18.4        18.         42.10658762 48.09243092 -1.        ]]\n",
      "31.0\n",
      "[[ 7.4        33.4        19.         38.01275492  8.6671361  -1.        ]]\n",
      "56.56\n",
      "driver reward  1056.0400000000002\n",
      "[[ 8.1   20.3    5.    11.661 48.691 40.   ]]\n",
      "9.880000000000024\n",
      "[[ 3.9         6.          7.          9.7265236  44.40997662 39.        ]]\n",
      "-7.320000000000007\n",
      "[[13.8         8.1         7.         15.38574258 45.28047053 38.        ]]\n",
      "-67.91999999999999\n",
      "[[ 9.3        27.3         8.          2.76888332 31.70400845 37.        ]]\n",
      "24.120000000000005\n",
      "[[14.         20.4        11.         21.79851934 24.47470969 36.        ]]\n",
      "-29.919999999999987\n",
      "[[ 8.5         9.         11.         22.84760646 26.16646675 35.        ]]\n",
      "-29.0\n",
      "[[ 9.1        17.7        12.         17.84894996 45.78444276 34.        ]]\n",
      "-5.239999999999981\n",
      "[[ 6.          4.1        12.         13.49790336 38.63646276 33.        ]]\n",
      "-27.679999999999993\n",
      "[[11.3        22.4        12.         34.08022084 38.00895752 32.        ]]\n",
      "-5.160000000000025\n",
      "[[ 8.6        30.8        18.          2.61885194 23.79041712 31.        ]]\n",
      "40.08000000000004\n",
      "[[ 3.5        17.5        18.         22.80307247 17.91257076 30.        ]]\n",
      "32.20000000000002\n",
      "[[25.         24.2        18.         30.17808157 41.66208478 29.        ]]\n",
      "-92.56\n",
      "[[ 8.9        17.8        19.         13.57791839 45.82681365 28.        ]]\n",
      "-3.5600000000000023\n",
      "[[ 7.5    9.3   19.     7.38  52.941 27.   ]]\n",
      "-21.239999999999995\n",
      "[[ 4.8   11.4   19.     6.967 42.268 26.   ]]\n",
      "3.8400000000000034\n",
      "[[ 1.         12.2        19.         16.88589302 48.00400876 25.        ]]\n",
      "32.24000000000001\n",
      "[[ 8.2        13.8        19.         23.38135533 42.20836274 24.        ]]\n",
      "-11.599999999999994\n",
      "[[ 8.1        32.4        19.         59.61273179 50.64983072 23.        ]]\n",
      "48.60000000000002\n",
      "[[ 7.7        21.4         9.         42.60192746 27.04575098 22.        ]]\n",
      "16.120000000000033\n",
      "[[ 5.    10.7    9.    51.337 26.052 21.   ]]\n",
      "0.2400000000000091\n",
      "[[17.4   19.7   13.    30.278 21.68  20.   ]]\n",
      "-55.279999999999944\n",
      "[[ 7.8        20.1        14.         42.25464236 26.36864042 19.        ]]\n",
      "11.280000000000001\n",
      "[[21.7         2.1        20.         24.09771844 36.5530231  18.        ]]\n",
      "-140.84\n",
      "[[19.6         8.3        21.          4.12781087 36.34750239 17.        ]]\n",
      "-106.72\n",
      "[[ 3.9         6.5        21.          8.74263917 27.91616135 16.        ]]\n",
      "-5.719999999999999\n",
      "[[ 6.         51.9        21.         55.58517366 15.7784101  15.        ]]\n",
      "125.28000000000003\n",
      "[[ 1.9        13.         14.         44.43378213 22.05373861 14.        ]]\n",
      "28.680000000000007\n",
      "[[21.8        13.3         7.         38.21656221 47.58880197 13.        ]]\n",
      "-105.68\n",
      "[[16.8   33.    10.     3.542 33.579 12.   ]]\n",
      "-8.639999999999986\n",
      "[[ 2.8        13.1        10.          5.28562754 48.91377433 11.        ]]\n",
      "22.88000000000001\n",
      "[[ 6.8        24.6        10.         27.27114023 40.64308007 10.        ]]\n",
      "32.47999999999999\n",
      "[[17.9        18.2        10.         28.75794206 41.34162328  9.        ]]\n",
      "-63.47999999999996\n",
      "[[14.3   13.4   11.     5.864 27.143  8.   ]]\n",
      "-54.360000000000014\n",
      "[[20.2        11.8        11.         16.85412072 57.04560771  7.        ]]\n",
      "-99.6\n",
      "[[13.          4.7        11.          8.01392397 53.93037996  6.        ]]\n",
      "-73.35999999999999\n",
      "[[ 3.1        10.2        12.         10.11943659 43.17489471  5.        ]]\n",
      "11.560000000000016\n",
      "[[ 8.5        14.3        12.          0.37442967 22.64538641  4.        ]]\n",
      "-12.039999999999992\n",
      "[[11.9        25.6        12.          1.10155598 56.73590535  3.        ]]\n",
      "1.0\n",
      "[[13.6        46.6        12.         46.31159771 21.28714719  2.        ]]\n",
      "56.639999999999986\n",
      "[[ 5.7   40.    14.    20.792 51.781  1.   ]]\n",
      "1589.24\n",
      "[[17.         20.         15.         15.37237163 42.26374505  0.        ]]\n",
      "-51.599999999999994\n",
      "[[ 9.9         2.         16.         11.44711487 35.25209484 -1.        ]]\n",
      "-60.92\n",
      "[[ 5.6         5.2        16.         11.45242318 34.58195291 -1.        ]]\n",
      "-21.439999999999998\n",
      "[[12.7        10.7        16.         17.75916173 23.10996032 -1.        ]]\n",
      "-52.119999999999976\n",
      "[[13.1        29.9        16.         26.79325349 37.29710709 -1.        ]]\n",
      "6.600000000000023\n",
      "[[11.7    4.1   16.    20.791 32.603 -1.   ]]\n",
      "-66.43999999999998\n",
      "[[14.3        25.2        16.         33.11453902 22.42550949 -1.        ]]\n",
      "-16.599999999999966\n",
      "[[ 5.9        25.7        17.         46.78407233 50.71780874 -1.        ]]\n",
      "42.120000000000005\n",
      "[[ 8.4        18.9         7.         24.61222912 44.01751707 -1.        ]]\n",
      "3.3600000000000136\n",
      "[[10.4    4.4    7.    16.852 43.737 -1.   ]]\n",
      "-56.64\n",
      "[[15.5   17.1    7.     3.455 35.263 -1.   ]]\n",
      "-50.68000000000001\n",
      "[[12.2   15.     7.     1.523 32.822 -1.   ]]\n",
      "-34.95999999999998\n",
      "[[ 8.3        23.5         7.          0.76733694 15.32990176 -1.        ]]\n",
      "18.75999999999999\n",
      "[[ 1.8   18.5    9.     9.098 33.697 -1.   ]]\n",
      "46.96000000000001\n",
      "[[11.5         5.8         9.         10.19588088 46.24876427 -1.        ]]\n",
      "-59.64\n",
      "[[ 6.8         8.6         9.         10.60117264 50.74079917 -1.        ]]\n",
      "-18.719999999999985\n",
      "[[ 7.9        25.1         9.         27.94131836 55.21091138 -1.        ]]\n",
      "26.599999999999994\n",
      "[[12.    11.3   10.     5.313 58.501 -1.   ]]\n",
      "-45.44\n",
      "[[ 2.5    0.7   10.     6.815 56.898 -1.   ]]\n",
      "-14.760000000000002\n",
      "[[17.9         7.6        10.          2.76329192 31.7441477  -1.        ]]\n",
      "-97.4\n",
      "[[13.6         5.4        10.          8.32996468 42.02272806 -1.        ]]\n",
      "-75.19999999999999\n",
      "[[14.9        32.3        11.         40.61602099 49.2003528  -1.        ]]\n",
      "2.0400000000000205\n",
      "[[ 6.8   45.    12.     2.921 56.405 -1.   ]]\n",
      "97.76000000000005\n",
      "[[ 9.2         4.1        12.         10.96892959 46.18603608 -1.        ]]\n",
      "-49.43999999999998\n",
      "[[ 7.5        26.6        13.         19.65376436 22.3475868  -1.        ]]\n",
      "34.120000000000005\n",
      "[[ 2.6        13.4        13.          9.17686735 10.68577395 -1.        ]]\n",
      "25.200000000000003\n",
      "[[ 4.1   17.4   14.     3.991 31.606 -1.   ]]\n",
      "27.80000000000001\n",
      "[[ 6.5        19.3        14.          7.09663155 14.72651555 -1.        ]]\n",
      "17.560000000000002\n",
      "[[ 2.4    3.2   15.     4.403 16.79  -1.   ]]\n",
      "-6.079999999999998\n",
      "[[ 5.6        20.2        15.         30.07657783 18.20103341 -1.        ]]\n",
      "26.56000000000003\n",
      "[[ 3.3        22.6        16.          5.93215261 26.67664973 -1.        ]]\n",
      "49.879999999999995\n",
      "[[ 0.7        10.8        16.         16.18086267 26.43673271 -1.        ]]\n",
      "29.799999999999997\n",
      "[[ 5.3        17.         17.         34.92798304 15.31029446 -1.        ]]\n",
      "18.359999999999985\n",
      "[[19.9   17.4   19.     1.955 22.662 -1.   ]]\n",
      "-79.63999999999999\n",
      "[[ 7.5        16.2        19.         20.43968576 36.88478988 -1.        ]]\n",
      "0.8400000000000034\n",
      "[[12.2         9.         19.          3.60469711 49.29817304 -1.        ]]\n",
      "-54.16\n",
      "[[ 7.5         8.         19.         16.01803648 51.5806247  -1.        ]]\n",
      "-25.39999999999999\n",
      "[[13.9        12.2        19.          9.27871997 43.59816904 -1.        ]]\n",
      "-55.48000000000002\n",
      "[[ 8.8        17.7        20.          7.83120519 53.10155086 -1.        ]]\n",
      "-3.1999999999999886\n",
      "[[ 5.2        11.1        20.         14.92239928 50.89871955 -1.        ]]\n",
      "0.1599999999999966\n",
      "[[ 6.4        31.6        20.          0.8965574  15.93361556 -1.        ]]\n",
      "57.60000000000002\n",
      "[[10.9         7.4        20.          5.73990842 17.39446119 -1.        ]]\n",
      "-50.44\n",
      "[[ 4.6        28.2        20.         35.27829923  5.85179855 -1.        ]]\n",
      "58.960000000000036\n",
      "[[ 5.4   30.     6.    11.982 13.672 -1.   ]]\n",
      "59.28000000000003\n",
      "[[11.7        29.6         7.         36.23382706 32.55211732 -1.        ]]\n",
      "15.160000000000025\n",
      "[[22.3         8.2         7.         24.01936171 50.59239316 -1.        ]]\n",
      "-125.4\n",
      "[[16.         24.4         7.         31.89562124 47.82807923 -1.        ]]\n",
      "-30.71999999999997\n",
      "[[21.9        14.          8.          2.20708263 56.52146894 -1.        ]]\n",
      "-104.11999999999998\n",
      "[[16.         18.1         8.         30.9136598  41.77425382 -1.        ]]\n",
      "-50.879999999999995\n",
      "[[ 9.9        10.9         9.         35.22458499 43.94638908 -1.        ]]\n",
      "-32.44\n",
      "[[ 7.5        17.2         9.         27.56950338 27.38498566 -1.        ]]\n",
      "4.0400000000000205\n",
      "[[21.1        20.7        11.         14.42774782 30.31919953 -1.        ]]\n",
      "-77.23999999999995\n",
      "[[ 6.9         8.9        11.          0.82953657 23.44465773 -1.        ]]\n",
      "-18.439999999999998\n",
      "[[15.9        31.         12.         29.35002647  8.14450874 -1.        ]]\n",
      "-8.919999999999959\n",
      "[[24.4        25.         14.         34.9484098  26.73912716 -1.        ]]\n",
      "-85.91999999999996\n",
      "[[18.3        33.6        15.         15.74515352 51.14480872 -1.        ]]\n",
      "-16.920000000000016\n",
      "[[15.8        30.1        15.          1.24329195 27.63139451 -1.        ]]\n",
      "-11.120000000000005\n",
      "[[12.6    7.8   15.     6.641 28.304 -1.   ]]\n",
      "-60.72\n",
      "[[ 6.7         9.9        15.         14.08467834 41.25915505 -1.        ]]\n",
      "-13.88000000000001\n",
      "[[ 4.5        20.3        15.         35.84772532 52.88552991 -1.        ]]\n",
      "34.360000000000014\n",
      "[[ 3.8        16.8        16.         28.536327   34.08557687 -1.        ]]\n",
      "27.919999999999987\n",
      "[[15.5        35.8        16.         57.37787609 41.67919509 -1.        ]]\n",
      "9.160000000000025\n",
      "[[10.9         3.4        20.         46.81166508 42.10689925 -1.        ]]\n",
      "-63.24000000000001\n",
      "driver reward  54.040000000000916\n",
      "[[13.1   17.6   10.    26.461 46.321 40.   ]]\n",
      "-32.76000000000002\n",
      "[[19.2         9.8        10.         20.39785704 54.33351586 39.        ]]\n",
      "-99.19999999999999\n",
      "[[12.6        13.1        10.         21.88301618 53.04860196 38.        ]]\n",
      "-43.75999999999999\n",
      "[[ 9.2         9.4        10.         13.18796045 44.75750714 37.        ]]\n",
      "-32.480000000000004\n",
      "[[ 4.7        31.9        11.         45.86291636 52.07700384 36.        ]]\n",
      "70.12\n",
      "[[ 6.1        15.1        18.         50.31389378 43.91380973 35.        ]]\n",
      "6.840000000000003\n",
      "[[ 0.6         8.6         8.         57.65545603 40.03064111 34.        ]]\n",
      "23.440000000000005\n",
      "[[ 9.1        37.3        13.         49.93996076 12.2568824  33.        ]]\n",
      "57.48000000000002\n",
      "[[ 7.3        24.6        20.         28.19034148 22.61053757 32.        ]]\n",
      "29.079999999999984\n",
      "[[21.7        21.9         7.         32.28684391 47.94382701 31.        ]]\n",
      "-77.47999999999996\n",
      "[[21.          4.3         8.          7.27357221 48.47886237 30.        ]]\n",
      "-129.04\n",
      "[[13.9   24.6    8.     4.371 58.775 29.   ]]\n",
      "-15.800000000000011\n",
      "[[10.6   16.7    8.     5.314 31.833 28.   ]]\n",
      "-18.639999999999986\n",
      "[[15.1        26.8         9.         16.44911342 48.08419403 27.        ]]\n",
      "-16.91999999999996\n",
      "[[16.3        11.5         9.          6.1036212  49.79717427 26.        ]]\n",
      "-74.03999999999999\n",
      "[[ 7.3        6.5        9.         9.0552019 41.524927  25.       ]]\n",
      "-28.840000000000003\n",
      "[[ 1.7        15.3         9.         11.17213581 57.31790549 24.        ]]\n",
      "37.400000000000006\n",
      "[[ 4.8         3.5         9.          3.01726956 55.51201586 23.        ]]\n",
      "-21.440000000000005\n",
      "[[17.3        32.7         9.         17.68595766  7.90673008 22.        ]]\n",
      "-13.0\n",
      "[[ 5.9        21.6        12.         28.4119775  28.75796186 21.        ]]\n",
      "29.0\n",
      "[[ 2.4   24.7   12.     7.041 18.279 20.   ]]\n",
      "62.72000000000003\n",
      "[[13.2        13.8        12.          2.04079831 17.23260862 19.        ]]\n",
      "-45.599999999999994\n",
      "[[17.6        13.         12.          4.00401678 46.97825434 18.        ]]\n",
      "-78.08000000000001\n",
      "[[ 3.4         7.7        12.          7.69519056 56.22556816 17.        ]]\n",
      "1.5200000000000102\n",
      "[[ 5.5        25.2        12.          2.02217438 26.85234891 16.        ]]\n",
      "43.24000000000001\n",
      "[[12.4         6.2        12.          7.90552153 33.06138539 15.        ]]\n",
      "-64.48\n",
      "[[ 3.5        19.1        12.         18.58345879 21.31373974 14.        ]]\n",
      "37.31999999999999\n",
      "[[13.2        17.1        12.         20.79786237 16.67808238 13.        ]]\n",
      "-35.03999999999999\n",
      "[[ 9.3   20.1   12.     4.824 32.1   12.   ]]\n",
      "1.079999999999984\n",
      "[[16.1        12.9        13.         16.0240254  38.32880038 11.        ]]\n",
      "-68.19999999999999\n",
      "[[20.9        18.1        13.         12.78015301 39.80079804 10.        ]]\n",
      "-84.19999999999999\n",
      "[[ 8.1        13.2        13.          2.23417611 38.19380156  9.        ]]\n",
      "-12.839999999999975\n",
      "[[ 4.9         8.7        13.         13.15157327 45.3427371   8.        ]]\n",
      "-5.47999999999999\n",
      "[[ 6.4        30.6        13.         37.4208398  42.06600589  7.        ]]\n",
      "54.400000000000006\n",
      "[[ 8.3        10.7        14.         38.33452669 39.86332366  6.        ]]\n",
      "-22.19999999999999\n",
      "[[ 2.1        12.8        17.         40.94557998 50.30016886  5.        ]]\n",
      "26.680000000000007\n",
      "[[ 1.4   45.6   17.     6.159 22.634  4.   ]]\n",
      "136.40000000000003\n",
      "[[ 8.9        19.1        17.         15.9223639  47.02886815  3.        ]]\n",
      "0.5999999999999943\n",
      "[[11.6         5.7        17.          9.91272995 50.95167388  2.        ]]\n",
      "-60.64\n",
      "[[10.5    4.6   17.    11.047 43.66   1.   ]]\n",
      "1443.32\n",
      "[[ 8.9        10.4        17.         29.50155383 46.21557899  0.        ]]\n",
      "-27.24000000000001\n",
      "[[ 5.4        12.         17.         35.23055686 42.87136609 -1.        ]]\n",
      "1.6800000000000068\n",
      "[[ 4.3        22.6        19.         42.52835854 25.9810165  -1.        ]]\n",
      "43.079999999999984\n",
      "[[19.2   11.7    7.    17.896 28.096 -1.   ]]\n",
      "-93.11999999999998\n",
      "[[ 5.2   19.4    8.     1.972 31.06  -1.   ]]\n",
      "26.720000000000027\n",
      "[[15.2         4.6         8.         15.38191659 45.41527049 -1.        ]]\n",
      "-88.63999999999999\n",
      "[[10.3        17.6         8.          5.58530698 26.6656848  -1.        ]]\n",
      "-13.719999999999999\n",
      "[[ 7.4        17.8         8.         19.48934534 36.04594283 -1.        ]]\n",
      "6.639999999999986\n",
      "[[16.5        39.2         8.         38.95130786 24.23645629 -1.        ]]\n",
      "13.240000000000009\n",
      "[[ 7.         13.9        11.         48.27050847 17.93651407 -1.        ]]\n",
      "-3.119999999999976\n",
      "[[ 7.          0.9        17.         52.05438774 24.03939343 -1.        ]]\n",
      "-44.72\n",
      "[[ 3.1         4.8        23.         50.1169835  23.20676648 -1.        ]]\n",
      "-5.719999999999999\n",
      "[[10.9   11.4   12.    50.531 20.131 -1.   ]]\n",
      "-37.640000000000015\n",
      "[[25.          7.8        10.         28.50456847 42.85188001 -1.        ]]\n",
      "-145.03999999999996\n",
      "[[10.1        31.3        10.         49.91112704 46.82655586 -1.        ]]\n",
      "31.480000000000018\n",
      "[[11.7   12.5   12.    25.774 46.362 -1.   ]]\n",
      "-39.56\n",
      "[[26.4         8.1        12.          9.28765775 21.30161654 -1.        ]]\n",
      "-153.6\n",
      "[[ 6.8        16.8        12.         23.95548592 19.53486696 -1.        ]]\n",
      "7.519999999999982\n",
      "[[11.4   16.1   13.     4.356 12.927 -1.   ]]\n",
      "-26.0\n",
      "[[ 3.8         0.6        13.          8.42033567 11.28037708 -1.        ]]\n",
      "-23.919999999999995\n",
      "[[10.4        22.6        13.         27.23466293 24.14168739 -1.        ]]\n",
      "1.5999999999999943\n",
      "[[19.1        39.         13.         48.44983928 51.69386562 -1.        ]]\n",
      "-5.079999999999984\n",
      "[[18.9         5.8        17.         31.52706051 49.77375239 -1.        ]]\n",
      "-109.95999999999998\n",
      "[[ 6.2        18.6        17.          7.11154504 48.94278529 -1.        ]]\n",
      "17.360000000000014\n",
      "[[10.9        11.1        17.         11.34555927 30.52046694 -1.        ]]\n",
      "-38.599999999999994\n",
      "[[ 6.1        23.3        17.         21.34861454 16.39897232 -1.        ]]\n",
      "33.08000000000001\n",
      "[[13.3        10.9        17.          4.56242795 21.9107359  -1.        ]]\n",
      "-55.56\n",
      "[[ 1.1        47.8        17.         53.27996214 20.90872871 -1.        ]]\n",
      "145.48000000000002\n",
      "[[ 6.7   29.8   17.    31.63  29.056 -1.   ]]\n",
      "49.80000000000001\n",
      "[[12.3    8.7   18.    12.889 38.414 -1.   ]]\n",
      "-55.79999999999998\n",
      "[[ 3.1         7.9        18.          1.97806386 37.39400665 -1.        ]]\n",
      "4.200000000000003\n",
      "[[ 7.         11.4        18.          5.57648498 55.14444348 -1.        ]]\n",
      "-11.11999999999999\n",
      "[[ 5.4         0.6        18.          5.34274359 49.34500478 -1.        ]]\n",
      "-34.8\n",
      "[[ 5.8        12.9        18.         20.54974187 41.58432379 -1.        ]]\n",
      "1.8400000000000034\n",
      "[[11.    13.5   19.    12.249 42.395 -1.   ]]\n",
      "-31.599999999999994\n",
      "[[ 7.          6.3        19.          5.04864467 41.31630687 -1.        ]]\n",
      "-27.439999999999998\n",
      "[[15.5        45.3        19.         43.51749436 28.52308327 -1.        ]]\n",
      "39.56\n",
      "[[20.6         6.8        19.         19.18538083 32.51128125 -1.        ]]\n",
      "-118.32000000000002\n",
      "[[21.1        22.8        19.         29.46546811 56.63177721 -1.        ]]\n",
      "-70.52000000000004\n",
      "[[ 4.3         2.         19.         25.17931075 52.66809203 -1.        ]]\n",
      "-22.839999999999996\n",
      "[[21.         19.8        19.          4.04516502 36.02562357 -1.        ]]\n",
      "-79.44\n",
      "[[12.8        12.2        19.          4.60648481 36.76968567 -1.        ]]\n",
      "-48.0\n",
      "[[ 6.2        22.         20.          6.24288937 20.96871778 -1.        ]]\n",
      "28.24000000000001\n",
      "[[ 3.7         8.9        20.         12.2522013  25.04339921 -1.        ]]\n",
      "3.319999999999993\n",
      "[[ 7.2        32.         20.         31.10989473  1.24106328 -1.        ]]\n",
      "53.44\n",
      "driver reward  77.63999999999993\n",
      "[[ 1.9   27.7    8.    14.492 60.    40.   ]]\n",
      "75.72000000000003\n",
      "[[ 2.1         8.6         8.         16.90961739 50.48410157 39.        ]]\n",
      "13.240000000000009\n",
      "[[ 4.7         6.9         9.         20.81523127 49.279411   38.        ]]\n",
      "-9.88000000000001\n",
      "[[ 7.1        17.3         9.         22.34903085 33.55102187 37.        ]]\n",
      "7.0800000000000125\n",
      "[[ 5.2        11.1        10.         21.42004184 47.00305473 36.        ]]\n",
      "0.1599999999999966\n",
      "[[12.3        27.5        10.         32.49181953 36.4790786  35.        ]]\n",
      "4.360000000000014\n",
      "[[19.3    5.2   13.    12.656 50.284 34.   ]]\n",
      "-114.6\n",
      "[[ 6.7        18.7        13.         24.1567011  27.78216967 33.        ]]\n",
      "14.280000000000001\n",
      "[[ 2.9   22.8   13.    13.996 51.293 32.   ]]\n",
      "53.24000000000001\n",
      "[[ 8.          9.8        14.          3.25525608 40.79350939 31.        ]]\n",
      "-23.040000000000006\n",
      "[[ 3.6        33.9        14.         27.55205046 19.8657774  30.        ]]\n",
      "84.0\n",
      "[[ 8.2         2.5        15.         20.62546281 11.78918702 29.        ]]\n",
      "-47.75999999999999\n",
      "[[ 5.6   33.1   17.     7.165 43.351 28.   ]]\n",
      "67.83999999999997\n",
      "[[ 1.         13.6        17.          6.79511711 30.17604333 27.        ]]\n",
      "36.72\n",
      "[[ 7.9        15.9        17.         21.29821772 23.97109856 26.        ]]\n",
      "-2.8400000000000034\n",
      "[[23.5        25.5        17.         31.73029484 34.54333492 25.        ]]\n",
      "-78.19999999999999\n",
      "[[17.    10.4   18.     5.467 41.739 24.   ]]\n",
      "-82.32\n",
      "[[ 3.7        16.         18.         22.17723111 44.89507415 23.        ]]\n",
      "26.04000000000002\n",
      "[[22.3        12.1        18.         15.61342048 20.6022851  22.        ]]\n",
      "-112.91999999999999\n",
      "[[19.8         3.5        19.          5.94234656  5.69848639 21.        ]]\n",
      "-123.44\n",
      "[[17.4         4.2        20.         21.05660124 20.73224681 20.        ]]\n",
      "-104.88\n",
      "[[22.8        18.2        20.         23.66183459 31.61443107 19.        ]]\n",
      "-96.80000000000001\n",
      "[[ 9.9         5.5        20.         16.58194465 32.38633042 18.        ]]\n",
      "-49.72\n",
      "[[13.7        13.         20.          6.32567021 26.12622776 17.        ]]\n",
      "-51.56\n",
      "[[ 7.9         9.2        20.         14.25071555 14.23938997 16.        ]]\n",
      "-24.28\n",
      "[[18.4        19.9        23.         15.60714743 49.79447287 15.        ]]\n",
      "-61.44\n",
      "[[16.1        13.7         5.         12.44611742 21.71125736 14.        ]]\n",
      "-65.63999999999999\n",
      "[[11.4        17.9         5.          2.97971792 11.65753629 13.        ]]\n",
      "-20.23999999999998\n",
      "[[ 4.9        41.5         7.         39.03195744 22.9096902  12.        ]]\n",
      "99.48000000000002\n",
      "[[10.2        17.9         8.         24.67832184 20.37100971 11.        ]]\n",
      "-12.079999999999984\n",
      "[[ 5.4        38.6        11.         40.96984863 54.8777167  10.        ]]\n",
      "86.80000000000001\n",
      "[[ 8.5    0.8   12.    34.616 49.625  9.   ]]\n",
      "-55.24\n",
      "[[15.4         1.8        15.         23.30979484 57.77704525  8.        ]]\n",
      "-98.96\n",
      "[[ 6.3        14.1        15.         19.88296987 38.00564758  7.        ]]\n",
      "2.280000000000001\n",
      "[[13.3        28.1        16.         16.28476121 21.10816342  6.        ]]\n",
      "-0.5200000000000387\n",
      "[[ 7.     4.3   16.    15.938 28.938  5.   ]]\n",
      "-33.84\n",
      "[[ 2.6         5.         16.         18.06693496 30.21349623  4.        ]]\n",
      "-1.6799999999999997\n",
      "[[ 5.6         8.2        17.         25.55570183 31.64917424  3.        ]]\n",
      "-11.83999999999999\n",
      "[[18.2        26.4        18.         20.10843134  6.35268413  2.        ]]\n",
      "-39.27999999999997\n",
      "[[18.9        23.8        18.         20.75605287  0.75734965  1.        ]]\n",
      "1447.6399999999999\n",
      "[[16.3         6.9         7.          3.88136623 14.82096706  0.        ]]\n",
      "-88.76000000000002\n",
      "[[11.7         5.1         8.          7.31842107 27.49887093 -1.        ]]\n",
      "-63.23999999999998\n",
      "[[ 6.1        17.6         8.         19.21570013 29.72815017 -1.        ]]\n",
      "14.839999999999975\n",
      "[[ 3.9   11.1    8.     8.519 38.516 -1.   ]]\n",
      "9.0\n",
      "[[ 6.8        40.3         8.         47.17024328 51.96935179 -1.        ]]\n",
      "82.72000000000003\n",
      "[[20.8   13.1   19.    32.241 45.534 -1.   ]]\n",
      "-99.51999999999998\n",
      "[[ 9.6         2.3        19.         24.34885935 52.35462861 -1.        ]]\n",
      "-57.91999999999999\n",
      "[[19.3        23.4        20.         14.3311424  21.52306129 -1.        ]]\n",
      "-56.360000000000014\n",
      "[[ 2.5        33.9        20.         39.01225051 42.0816239  -1.        ]]\n",
      "91.48000000000002\n",
      "[[ 4.         15.4        12.         54.13190759 38.8893723  -1.        ]]\n",
      "22.080000000000013\n",
      "[[ 2.2         9.4        17.         48.3046387  43.25573848 -1.        ]]\n",
      "15.11999999999999\n",
      "[[12.9        23.4         8.         32.28413591 25.06139321 -1.        ]]\n",
      "-12.839999999999975\n",
      "[[ 6.5   12.4    9.    27.508 36.692 -1.   ]]\n",
      "-4.519999999999982\n",
      "[[ 7.5    5.6    9.    25.267 46.689 -1.   ]]\n",
      "-33.08\n",
      "[[16.9   28.2    9.     3.926 26.456 -1.   ]]\n",
      "-24.67999999999995\n",
      "[[ 3.2   11.8    9.     4.819 34.997 -1.   ]]\n",
      "16.0\n",
      "[[ 3.6        17.6         9.         18.77302576 28.63472557 -1.        ]]\n",
      "31.839999999999975\n",
      "[[20.6        25.9        10.         29.11668473 33.55813856 -1.        ]]\n",
      "-57.19999999999999\n",
      "[[ 4.3       29.4       12.        11.8541165 55.6914082 -1.       ]]\n",
      "64.84000000000003\n",
      "[[ 8.9        19.7        13.         16.87033663 41.2616185  -1.        ]]\n",
      "2.519999999999982\n",
      "[[13.3        19.8        13.         23.36394702 42.26571219 -1.        ]]\n",
      "-27.080000000000013\n",
      "[[15.6        29.8        13.         26.31264844 17.59157033 -1.        ]]\n",
      "-10.71999999999997\n",
      "[[ 8.3   19.8   14.    20.567 39.044 -1.   ]]\n",
      "6.9199999999999875\n",
      "[[ 3.2        19.7        14.          6.31978268 56.73121664 -1.        ]]\n",
      "41.28\n",
      "[[ 3.4         1.8        14.          3.80596915 59.4392028  -1.        ]]\n",
      "-17.36\n",
      "[[ 9.4        12.9        17.          1.19707164 41.47529155 -1.        ]]\n",
      "-22.640000000000015\n",
      "[[ 4.3         2.1        17.          5.00690672 43.74825295 -1.        ]]\n",
      "-22.520000000000003\n",
      "[[ 7.8        35.2        17.         26.51257915  8.94647988 -1.        ]]\n",
      "59.60000000000002\n",
      "[[22.6        29.5        18.         33.86874093  7.35031869 -1.        ]]\n",
      "-59.27999999999997\n",
      "[[23.5        20.6        21.         35.31134676 33.80458456 -1.        ]]\n",
      "-93.88\n",
      "[[24.3        23.9        23.         42.22221299 42.52408333 -1.        ]]\n",
      "-88.75999999999999\n",
      "[[ 5.9        13.8         8.         46.81986089 50.45460943 -1.        ]]\n",
      "4.039999999999992\n",
      "[[17.8   13.6   16.    24.717 28.896 -1.   ]]\n",
      "-77.51999999999998\n",
      "[[18.8         5.7        17.          9.84963814 45.88415179 -1.        ]]\n",
      "-109.6\n",
      "[[ 3.5        14.2        17.          5.73286122 35.60901998 -1.        ]]\n",
      "21.640000000000015\n",
      "[[ 6.5        14.4        17.         15.29288569 46.67940263 -1.        ]]\n",
      "1.8800000000000239\n",
      "[[11.6        16.3        17.         13.40449466 30.41595858 -1.        ]]\n",
      "-26.72\n",
      "[[21.3        15.7        17.         16.35333812  0.98299042 -1.        ]]\n",
      "-94.6\n",
      "[[21.3        24.9        17.         13.60426494 41.78233276 -1.        ]]\n",
      "-65.16000000000003\n",
      "[[11.9        7.2       17.         6.9019978 41.1698973 -1.       ]]\n",
      "-57.879999999999995\n",
      "[[ 5.5        27.         18.          9.14231136 18.78427787 -1.        ]]\n",
      "49.0\n",
      "[[ 6.6        22.3        18.         26.27068991 30.29221961 -1.        ]]\n",
      "26.480000000000018\n",
      "[[18.3   10.3   18.     1.611 18.319 -1.   ]]\n",
      "-91.48000000000002\n",
      "[[13.2   13.    18.    18.293 31.581 -1.   ]]\n",
      "-48.16\n",
      "[[19.1        18.4        18.         17.71598099  5.97030667 -1.        ]]\n",
      "-71.0\n",
      "[[ 9.1         9.9        18.          4.47431991 19.70156135 -1.        ]]\n",
      "-30.19999999999999\n",
      "[[ 1.1       25.7       18.        30.2521175 21.7638176 -1.       ]]\n",
      "74.75999999999999\n",
      "[[ 2.2        11.8        18.         26.47253359  9.6413702  -1.        ]]\n",
      "22.799999999999997\n",
      "[[ 9.5        7.6       18.         9.3600121  9.4801672 -1.       ]]\n",
      "-40.28\n",
      "[[14.         42.6        18.         50.43247007 32.48660683 -1.        ]]\n",
      "41.120000000000005\n",
      "[[16.4        30.9        20.          6.55805482 50.23382051 -1.        ]]\n",
      "-12.639999999999986\n",
      "[[ 5.          3.4        20.          6.58652012 45.55840416 -1.        ]]\n",
      "-23.119999999999997\n",
      "[[ 5.5         8.8        20.         11.57527501 39.36913454 -1.        ]]\n",
      "-9.240000000000009\n",
      "[[12.8         5.8        20.          8.28438842 31.35000087 -1.        ]]\n",
      "-68.48\n",
      "[[15.         27.         21.         38.02933115 58.80825538 -1.        ]]\n",
      "-15.599999999999966\n",
      "[[12.4        19.6         8.         34.15497044 28.4326665  -1.        ]]\n",
      "-21.599999999999994\n",
      "[[14.7        17.5         9.         27.31684034 43.03199133 -1.        ]]\n",
      "-43.96000000000001\n",
      "[[11.4   15.2    9.     5.574 51.895 -1.   ]]\n",
      "-28.879999999999995\n",
      "[[18.8        10.7         9.          6.0270124  43.57168563 -1.        ]]\n",
      "-93.6\n",
      "[[15.6        13.          9.         22.95862369 27.14219975 -1.        ]]\n",
      "-64.48000000000002\n",
      "[[11.5        29.4        10.         49.49310894 29.24019998 -1.        ]]\n",
      "15.879999999999995\n",
      "[[ 6.1         7.3        16.         40.26618489 38.74379602 -1.        ]]\n",
      "-18.11999999999999\n",
      "[[11.8    8.7   17.    25.792 31.697 -1.   ]]\n",
      "-52.400000000000006\n",
      "[[21.2         9.6        17.         14.06649318 30.01413141 -1.        ]]\n",
      "-113.43999999999997\n",
      "[[ 4.8   12.2   17.     6.447 43.316 -1.   ]]\n",
      "6.400000000000006\n",
      "[[13.2   10.7   17.     8.643 45.665 -1.   ]]\n",
      "-55.51999999999998\n",
      "[[ 3.4         8.9        17.         15.83368964 50.73191244 -1.        ]]\n",
      "5.359999999999999\n",
      "[[13.8    6.5   18.    10.254 35.597 -1.   ]]\n",
      "-73.03999999999999\n",
      "[[ 9.          5.8        18.          7.31383158 34.42506423 -1.        ]]\n",
      "-42.64\n",
      "[[15.5        18.6        18.          2.37873438 31.63463385 -1.        ]]\n",
      "-45.879999999999995\n",
      "[[14.1         4.4        18.          5.02860547 41.32158658 -1.        ]]\n",
      "-81.8\n",
      "[[ 8.1        13.         18.          6.82256745 36.60002926 -1.        ]]\n",
      "-13.480000000000018\n",
      "[[ 7.          6.1        18.          7.43630904 35.63570618 -1.        ]]\n",
      "-28.08\n",
      "[[11.         20.6        18.          2.06835655  4.89573121 -1.        ]]\n",
      "-8.879999999999995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.3         9.2        18.          2.30528591 28.80691203 -1.        ]]\n",
      "-74.6\n",
      "[[ 8.1        23.4        18.         10.66636    13.87538693 -1.        ]]\n",
      "19.80000000000001\n",
      "[[10.    16.7   19.     7.216 26.372 -1.   ]]\n",
      "-14.560000000000002\n",
      "[[ 6.         20.9        19.         19.75880494 15.85576125 -1.        ]]\n",
      "26.080000000000013\n",
      "[[ 8.2        21.         20.          4.94519289 11.87822231 -1.        ]]\n",
      "11.439999999999998\n",
      "[[11.7         9.8        20.          9.05151733 13.55731501 -1.        ]]\n",
      "-48.19999999999999\n",
      "[[ 8.9        23.9        20.         23.65770678  6.59425757 -1.        ]]\n",
      "15.960000000000036\n",
      "driver reward  -1108.4399999999996\n",
      "[[ 4.8        20.3         9.         32.30992297 54.26509248 40.        ]]\n",
      "32.31999999999999\n",
      "[[20.4        16.6        11.         25.19512649 48.35272262 39.        ]]\n",
      "-85.6\n",
      "[[10.8        18.4        11.         32.16302333 42.72319452 38.        ]]\n",
      "-14.560000000000002\n",
      "[[26.6         2.8        13.         12.47882528 57.26856966 37.        ]]\n",
      "-171.92000000000002\n",
      "[[ 4.    15.3   13.     4.41  39.854 36.   ]]\n",
      "21.75999999999999\n",
      "[[18.9   11.    13.    11.906 60.    35.   ]]\n",
      "-93.32\n",
      "[[13.5        10.1        14.         11.22938799 40.68218878 34.        ]]\n",
      "-59.48000000000002\n",
      "[[ 3.    10.4   14.    13.864 51.747 33.   ]]\n",
      "12.879999999999995\n",
      "[[ 7.1        16.         14.         11.43143741 34.35208904 32.        ]]\n",
      "2.9199999999999875\n",
      "[[ 8.3        14.1        14.         27.07137257 50.18565803 31.        ]]\n",
      "-11.319999999999993\n",
      "[[12.3         5.         15.         22.59998137 57.7463727  30.        ]]\n",
      "-67.64\n",
      "[[ 4.2        27.8        15.         45.45695948 51.74676998 29.        ]]\n",
      "60.400000000000006\n",
      "[[ 8.         21.          9.         45.27991012 28.64975588 28.        ]]\n",
      "12.800000000000011\n",
      "[[ 1.8        15.9         9.         32.67743136 35.77009546 27.        ]]\n",
      "38.640000000000015\n",
      "[[19.5        12.9         9.         13.85055165 37.43819866 26.        ]]\n",
      "-91.32\n",
      "[[16.9   30.5   10.    24.112 28.315 25.   ]]\n",
      "-17.319999999999993\n",
      "[[11.3        11.8        10.         16.77563035 48.73413438 24.        ]]\n",
      "-39.08000000000001\n",
      "[[ 9.5         1.         10.         21.2121773  39.27001178 23.        ]]\n",
      "-61.39999999999999\n",
      "[[ 8.2    7.5   10.    20.017 53.672 22.   ]]\n",
      "-31.75999999999999\n",
      "[[14.8        15.8        11.         21.28106577 53.53767558 21.        ]]\n",
      "-50.08000000000001\n",
      "[[14.2         9.         11.         34.2163198  50.53500759 20.        ]]\n",
      "-67.75999999999999\n",
      "[[ 7.1         8.1        11.         42.83995184 59.29999229 19.        ]]\n",
      "-22.36\n",
      "[[21.7         7.1        18.         27.93056281 57.96539002 18.        ]]\n",
      "-124.83999999999997\n",
      "[[11.8        21.8        18.         40.43406961 31.7562389  17.        ]]\n",
      "-10.47999999999999\n",
      "[[13.8   18.9    7.    10.737 25.784 16.   ]]\n",
      "-33.360000000000014\n",
      "[[10.         30.1         7.         29.67736689 10.5486055  15.        ]]\n",
      "28.319999999999993\n",
      "[[16.    12.7    8.     6.799 19.003 14.   ]]\n",
      "-68.16\n",
      "[[ 2.8         5.6         8.          9.55041497 19.90515619 13.        ]]\n",
      "-1.1199999999999903\n",
      "[[ 3.1        36.8         9.         10.44786095 59.22723281 12.        ]]\n",
      "96.68\n",
      "[[ 9.3        32.5         9.         34.84066364 28.25557846 11.        ]]\n",
      "40.76000000000005\n",
      "[[22.2   24.6   11.     6.453 18.258 10.   ]]\n",
      "-72.23999999999995\n",
      "[[ 6.         32.6        11.         28.38029199 39.32056244  9.        ]]\n",
      "63.51999999999998\n",
      "[[ 1.7        13.1        11.         32.69197366 51.57009833  8.        ]]\n",
      "30.360000000000014\n",
      "[[23.4        19.4        12.         30.72000365 46.02035105  7.        ]]\n",
      "-97.03999999999996\n",
      "[[ 6.7        22.         14.         10.23703998 26.09864785  6.        ]]\n",
      "24.840000000000003\n",
      "[[13.2         9.7        14.          4.19235614 24.16718543  5.        ]]\n",
      "-58.72\n",
      "[[10.4        14.6        14.          4.58866586 19.9284964   4.        ]]\n",
      "-24.0\n",
      "[[17.5   10.5   14.     8.347 13.663  3.   ]]\n",
      "-85.4\n",
      "[[ 3.6        10.5        14.          8.02218108 22.49195539  2.        ]]\n",
      "9.120000000000005\n",
      "[[ 7.5        19.6        14.         25.1074995  22.56144684  1.        ]]\n",
      "1511.72\n",
      "[[16.7        25.6        15.         20.38638073  4.58428148  0.        ]]\n",
      "-31.639999999999986\n",
      "[[27.          6.2        15.          9.59458641 22.85864347 -1.        ]]\n",
      "-163.76000000000002\n",
      "[[17.          7.4        15.          4.58704729 12.51735051 -1.        ]]\n",
      "-91.91999999999999\n",
      "[[ 5.2        35.3        15.         42.56738584 19.35270657 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 6.5   40.2    8.     9.019 30.436 -1.   ]]\n",
      "84.44\n",
      "[[11.2    1.7    9.     3.426 41.98  -1.   ]]\n",
      "-70.71999999999998\n",
      "[[ 5.1        31.7         9.         36.91912382 57.01097464 -1.        ]]\n",
      "66.76000000000002\n",
      "[[11.5    6.5   18.    24.97  49.809 -1.   ]]\n",
      "-57.39999999999999\n",
      "[[14.8    2.4   18.    12.729 51.493 -1.   ]]\n",
      "-92.96\n",
      "[[ 4.7        16.4        18.         24.62874688 47.64730065 -1.        ]]\n",
      "20.52000000000001\n",
      "[[26.8        31.3        18.         26.864213    7.71643909 -1.        ]]\n",
      "-82.07999999999998\n",
      "[[21.9         8.8        19.         16.3660459  15.63835443 -1.        ]]\n",
      "-120.75999999999999\n",
      "[[18.6         8.5        19.          0.74768174 22.47101355 -1.        ]]\n",
      "-99.28\n",
      "[[ 7.         26.4        19.         20.12121533  5.66670173 -1.        ]]\n",
      "36.880000000000024\n",
      "[[11.6   26.8   20.    11.095 39.292 -1.   ]]\n",
      "6.8799999999999955\n",
      "[[ 7.3   10.    20.     2.193 42.612 -1.   ]]\n",
      "-17.64\n",
      "[[ 3.9        32.4        20.         33.68697164 31.40708516 -1.        ]]\n",
      "77.16000000000003\n",
      "[[11.5    9.7   22.    19.13  25.187 -1.   ]]\n",
      "-47.16\n",
      "[[15.8        18.9         0.         24.53346908 34.47901185 -1.        ]]\n",
      "-46.96000000000001\n",
      "[[16.4        12.3         5.         31.72139293 43.97673334 -1.        ]]\n",
      "-72.16\n",
      "[[ 1.4    5.9    6.    26.613 39.061 -1.   ]]\n",
      "9.36\n",
      "[[29.7        18.1         6.          8.06704889 39.95344571 -1.        ]]\n",
      "-144.03999999999996\n",
      "[[ 3.5        13.7         7.         15.67530809 50.13580015 -1.        ]]\n",
      "20.040000000000006\n",
      "[[12.7        29.3         7.         36.95323225 59.17856342 -1.        ]]\n",
      "7.400000000000034\n",
      "[[17.1        11.8         8.         17.38132457 37.81783255 -1.        ]]\n",
      "-78.52000000000001\n",
      "[[ 6.6   17.3    8.     4.221 43.586 -1.   ]]\n",
      "10.480000000000018\n",
      "[[ 0.6        18.          8.          9.84592984 25.87197691 -1.        ]]\n",
      "53.519999999999996\n",
      "[[ 5.3   23.     8.     4.678 47.865 -1.   ]]\n",
      "37.56\n",
      "[[ 3.5        24.7         8.          5.42978686 23.96164719 -1.        ]]\n",
      "55.24000000000001\n",
      "[[14.4         8.9         9.          9.93040375 28.0355616  -1.        ]]\n",
      "-69.44\n",
      "[[ 3.         30.1         9.         32.4369614  43.84742933 -1.        ]]\n",
      "75.91999999999999\n",
      "[[15.8        12.         11.         28.57188098 46.67001474 -1.        ]]\n",
      "-69.03999999999999\n",
      "[[ 4.9         6.1        11.         31.00516224 52.55525491 -1.        ]]\n",
      "-13.799999999999997\n",
      "[[15.2   22.    11.     4.84  38.824 -1.   ]]\n",
      "-32.96000000000001\n",
      "[[ 2.2        18.         11.         10.6595841  53.59718523 -1.        ]]\n",
      "42.640000000000015\n",
      "[[ 5.2        12.2        12.         10.87301342 45.00342405 -1.        ]]\n",
      "3.680000000000007\n",
      "[[ 5.8         2.2        12.          7.58200795 39.34311099 -1.        ]]\n",
      "-32.4\n",
      "[[17.         21.5        12.         23.13317867 21.37720057 -1.        ]]\n",
      "-46.80000000000001\n",
      "[[ 9.9        15.7        12.          4.26971595  6.56644854 -1.        ]]\n",
      "-17.080000000000013\n",
      "[[ 7.3         6.3        14.          5.07448041  8.61598926 -1.        ]]\n",
      "-29.47999999999999\n",
      "[[ 6.5         9.9        16.         15.69415927  6.38195321 -1.        ]]\n",
      "-12.519999999999982\n",
      "[[ 6.2         2.6        16.         10.78809345 11.06423109 -1.        ]]\n",
      "-33.84\n",
      "[[ 6.9        16.         16.         23.01096654 12.82058837 -1.        ]]\n",
      "4.280000000000001\n",
      "[[23.6   11.1   18.     1.383 21.82  -1.   ]]\n",
      "-124.96000000000001\n",
      "[[ 7.2        13.5        18.         13.90866528 38.3001829  -1.        ]]\n",
      "-5.759999999999991\n",
      "[[ 8.5        15.6        18.         24.01715186 46.13743384 -1.        ]]\n",
      "-7.8799999999999955\n",
      "[[20.5   16.    18.     9.602 13.276 -1.   ]]\n",
      "-88.19999999999999\n",
      "[[18.8         7.         18.         11.07934838 38.14579003 -1.        ]]\n",
      "-105.44\n",
      "[[ 8.5        14.6        18.         10.58523244 21.55108555 -1.        ]]\n",
      "-11.080000000000013\n",
      "[[ 7.3       15.4       18.        30.0639995 32.2795987 -1.       ]]\n",
      "-0.3599999999999852\n",
      "[[ 6.4   34.    18.    31.689  4.699 -1.   ]]\n",
      "65.28000000000003\n",
      "[[22.         12.5        21.         27.23151363 13.75848411 -1.        ]]\n",
      "-109.6\n",
      "[[ 6.8    6.1    1.    17.725 21.939 -1.   ]]\n",
      "-26.719999999999985\n",
      "[[18.5        19.7         5.          5.93681893 29.871514   -1.        ]]\n",
      "-62.75999999999999\n",
      "[[ 2.9    8.7    5.     5.522 41.079 -1.   ]]\n",
      "8.120000000000005\n",
      "[[12.5         8.5         5.          5.50994188 45.45312484 -1.        ]]\n",
      "-57.79999999999998\n",
      "[[ 5.8        10.6         5.          1.85429649 37.21325159 -1.        ]]\n",
      "-5.519999999999982\n",
      "[[ 3.7        19.6         6.          5.31070741 56.2310067  -1.        ]]\n",
      "37.56\n",
      "[[ 6.4        13.3         6.         18.8281707  41.99662404 -1.        ]]\n",
      "-0.960000000000008\n",
      "[[10.         26.8         7.         52.93656265 50.66680839 -1.        ]]\n",
      "17.76000000000002\n",
      "[[12.2        21.3        15.         46.32849791 19.06776801 -1.        ]]\n",
      "-14.799999999999983\n",
      "[[16.4        14.         20.         20.85393063 25.70199323 -1.        ]]\n",
      "-66.72\n",
      "[[ 7.6        15.1        20.         23.78060949  8.07194107 -1.        ]]\n",
      "-3.359999999999985\n",
      "driver reward  -920.4399999999997\n",
      "[[ 6.8   21.3    9.     1.126 23.789 40.   ]]\n",
      "21.919999999999987\n",
      "[[13.8         3.6        11.         14.78364241 25.79535419 39.        ]]\n",
      "-82.32000000000001\n",
      "[[ 4.8         7.5        11.         21.14615271 34.54318084 38.        ]]\n",
      "-8.64\n",
      "[[ 9.5   14.4   11.    12.644 56.793 37.   ]]\n",
      "-18.519999999999982\n",
      "[[ 1.6        20.1        12.         33.16158278 53.34811225 36.        ]]\n",
      "53.44\n",
      "[[ 3.3   22.1   13.    13.867 36.995 35.   ]]\n",
      "48.28\n",
      "[[ 5.5         5.9        13.          9.68971461 47.52726159 34.        ]]\n",
      "-18.519999999999996\n",
      "[[ 4.9        32.3        13.         41.7433306  30.04729416 33.        ]]\n",
      "70.04000000000005\n",
      "[[ 7.7        28.8        20.         53.22145488  8.64956664 32.        ]]\n",
      "39.80000000000001\n",
      "[[ 2.9         6.1         8.         48.27135911 15.75757141 31.        ]]\n",
      "-0.19999999999999574\n",
      "[[14.1   42.6   11.    22.036 30.844 30.   ]]\n",
      "40.44\n",
      "[[ 1.5        18.7        12.         16.27869454 47.85361327 29.        ]]\n",
      "49.640000000000015\n",
      "[[ 8.3         7.8        12.         22.59007153 49.77598184 28.        ]]\n",
      "-31.480000000000004\n",
      "[[11.         14.6        12.         19.15214625 55.69217982 27.        ]]\n",
      "-28.080000000000013\n",
      "[[10.1        24.7        12.         21.61443451 36.60901182 26.        ]]\n",
      "10.360000000000014\n",
      "[[11.2        13.4        12.          3.12380997 22.03191833 25.        ]]\n",
      "-33.28\n",
      "[[ 8.2        13.         12.         14.37308493 29.16278626 24.        ]]\n",
      "-14.159999999999997\n",
      "[[18.1         4.5        12.          5.22902946 41.16643921 23.        ]]\n",
      "-108.68\n",
      "[[ 6.3        15.9        12.         26.1461981  47.51552312 22.        ]]\n",
      "8.04000000000002\n",
      "[[19.8        25.6        12.          8.96903197 23.11418275 21.        ]]\n",
      "-52.72000000000003\n",
      "[[ 7.2        13.9        13.         10.86089333 30.79029203 20.        ]]\n",
      "-4.480000000000018\n",
      "[[19.4        18.         13.         13.96608847 34.8553894  19.        ]]\n",
      "-74.32\n",
      "[[16.6        19.3        13.          8.34318118 29.4916591  18.        ]]\n",
      "-51.12000000000003\n",
      "[[ 8.         17.3        13.         25.27311976 31.90668563 17.        ]]\n",
      "0.960000000000008\n",
      "[[23.          4.2        13.         12.65977818 47.1214096  16.        ]]\n",
      "-142.95999999999998\n",
      "[[ 4.8        10.5        13.         11.44283077 37.6390886  15.        ]]\n",
      "0.9599999999999937\n",
      "[[11.4        34.1        13.         37.73893491 33.12362746 14.        ]]\n",
      "31.600000000000023\n",
      "[[ 1.6    7.7   14.    43.951 38.607 13.   ]]\n",
      "13.759999999999998\n",
      "[[15.5        26.5        19.          7.03255661 58.61991353 12.        ]]\n",
      "-20.599999999999966\n",
      "[[ 5.5        20.1        19.         22.34247298 49.7049571  11.        ]]\n",
      "26.919999999999987\n",
      "[[19.7        19.         20.         15.88018179 58.34948176 10.        ]]\n",
      "-73.16000000000003\n",
      "[[21.4         7.5        20.          3.59019178 48.83586562  9.        ]]\n",
      "-121.51999999999998\n",
      "[[12.9        16.9        20.         19.46750896 49.54186021  8.        ]]\n",
      "-33.639999999999986\n",
      "[[13.4        10.8        20.          8.66780096 44.64004973  7.        ]]\n",
      "-56.56\n",
      "[[ 9.2        12.         20.          2.47979469 49.29496521  6.        ]]\n",
      "-24.159999999999997\n",
      "[[11.3         8.2        20.         14.65910384 59.84012288  5.        ]]\n",
      "-50.599999999999994\n",
      "[[14.1        49.3        21.         35.56274631 10.77262667  4.        ]]\n",
      "61.879999999999995\n",
      "[[10.8        19.3        12.         41.81051164 22.77650908  3.        ]]\n",
      "-11.680000000000007\n",
      "[[ 7.     7.8   12.    39.094 21.868  2.   ]]\n",
      "-22.64\n",
      "[[ 3.6        17.7        17.         24.95137896 24.56335119  1.        ]]\n",
      "1532.16\n",
      "[[20.1        19.6        17.         17.25224282 16.84343245  0.        ]]\n",
      "-73.96000000000004\n",
      "[[24.5        23.3        18.         23.21939617 22.96328577 -1.        ]]\n",
      "-92.03999999999996\n",
      "[[12.7         0.7        19.         20.34716829 34.74731531 -1.        ]]\n",
      "-84.11999999999999\n",
      "[[21.2        22.1        19.          5.36443925 48.17876052 -1.        ]]\n",
      "-73.44\n",
      "[[ 3.3        18.4        19.         21.47491045 41.83325292 -1.        ]]\n",
      "36.44\n",
      "[[23.         18.5        20.         24.33505237 57.08607607 -1.        ]]\n",
      "-97.19999999999999\n",
      "[[22.9         8.7        20.          9.70190721 53.5460454  -1.        ]]\n",
      "-127.87999999999997\n",
      "[[10.2        35.1        20.         24.50378597 15.70292933 -1.        ]]\n",
      "42.960000000000036\n",
      "[[27.3         5.1        20.          3.3512532  39.12874935 -1.        ]]\n",
      "-169.32\n",
      "[[23.6        21.5        21.         26.39749613 39.20172615 -1.        ]]\n",
      "-91.68\n",
      "[[19.9        10.3        21.          1.82648444 54.9238975  -1.        ]]\n",
      "-102.35999999999999\n",
      "[[ 5.1        16.8        21.         16.51257317 39.53162301 -1.        ]]\n",
      "19.080000000000013\n",
      "[[22.7         4.8        21.         10.4668921  57.12417604 -1.        ]]\n",
      "-139.0\n",
      "[[10.4         7.8        22.          0.25864886 43.49369647 -1.        ]]\n",
      "-45.75999999999999\n",
      "[[ 6.4        15.7        22.         18.902406   48.89878296 -1.        ]]\n",
      "6.719999999999999\n",
      "[[17.1        32.9         1.         23.73020425 10.05966442 -1.        ]]\n",
      "-11.0\n",
      "[[18.8        13.2         6.          1.15694116 16.74542938 -1.        ]]\n",
      "-85.6\n",
      "[[ 8.6   25.9    7.     9.643 32.642 -1.   ]]\n",
      "24.400000000000006\n",
      "[[ 0.4         9.9         7.         18.8815343  34.94135315 -1.        ]]\n",
      "28.959999999999994\n",
      "[[18.2        10.6         7.          4.89998453 57.61750428 -1.        ]]\n",
      "-89.83999999999997\n",
      "[[ 8.1         2.9         7.          4.22884801 48.19972672 -1.        ]]\n",
      "-45.8\n",
      "[[11.         31.8         7.         36.79112015 20.46832595 -1.        ]]\n",
      "26.960000000000036\n",
      "[[13.9   20.7    9.     7.709 16.69  -1.   ]]\n",
      "-28.28\n",
      "[[ 5.          4.2         9.          3.95906561 14.95269176 -1.        ]]\n",
      "-20.559999999999995\n",
      "[[ 3.8        14.3         9.         18.53128096  5.11875981 -1.        ]]\n",
      "19.919999999999987\n",
      "[[18.6        18.8        12.         22.18875497 28.91251534 -1.        ]]\n",
      "-66.32000000000002\n",
      "[[ 3.2   17.8   13.     3.244 19.73  -1.   ]]\n",
      "35.20000000000002\n",
      "[[ 7.2        17.3        13.          0.53410642 32.10193098 -1.        ]]\n",
      "6.400000000000006\n",
      "[[ 9.7         2.         14.          8.67168804 36.37432159 -1.        ]]\n",
      "-59.55999999999999\n",
      "[[19.          6.         14.          3.16485597 53.89189251 -1.        ]]\n",
      "-110.0\n",
      "[[ 6.5         7.7        14.          2.3362883  52.95608609 -1.        ]]\n",
      "-19.559999999999988\n",
      "[[ 7.6        26.6        14.         26.97549094 29.23582542 -1.        ]]\n",
      "33.44\n",
      "[[ 3.3  13.1  14.   13.66 36.26 -1.  ]]\n",
      "19.480000000000018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.5        18.2        14.         16.7233936  59.05080428 -1.        ]]\n",
      "-40.360000000000014\n",
      "[[10.4         1.4        15.          6.78199975 57.82229632 -1.        ]]\n",
      "-66.24000000000001\n",
      "[[ 7.1        16.7        17.          3.64532307 34.2854898  -1.        ]]\n",
      "5.160000000000025\n",
      "[[12.5         4.3        17.         14.14738446 47.1967225  -1.        ]]\n",
      "-71.24\n",
      "[[12.1        42.4        17.         38.94424905 18.64295261 -1.        ]]\n",
      "53.400000000000034\n",
      "[[21.         17.9        19.         10.74259549 13.04087416 -1.        ]]\n",
      "-85.51999999999998\n",
      "[[13.5   12.5   19.     6.895 33.974 -1.   ]]\n",
      "-51.79999999999998\n",
      "[[15.8        15.6        19.          9.90501082 35.06803713 -1.        ]]\n",
      "-57.51999999999998\n",
      "[[ 9.         26.6        20.         30.13072857 29.42071058 -1.        ]]\n",
      "23.919999999999987\n",
      "[[ 6.2         1.9        20.         33.20491113 33.38934222 -1.        ]]\n",
      "-36.08\n",
      "[[ 7.2        12.1        22.         49.55693472 36.21583069 -1.        ]]\n",
      "-10.240000000000009\n",
      "[[ 3.4        11.4         7.         38.01267262 35.58598613 -1.        ]]\n",
      "13.36\n",
      "[[ 7.5        43.          7.          1.66712865  0.64328555 -1.        ]]\n",
      "86.60000000000002\n",
      "[[17.9        16.9        12.          0.49395362 24.13876948 -1.        ]]\n",
      "-67.63999999999999\n",
      "[[ 8.7         6.         12.          5.56922477 35.94310134 -1.        ]]\n",
      "-39.959999999999994\n",
      "[[ 5.5        31.7        12.         22.88190127 12.99812741 -1.        ]]\n",
      "64.03999999999999\n",
      "[[ 4.2   13.4   14.     5.374 13.397 -1.   ]]\n",
      "14.319999999999993\n",
      "[[ 3.2        14.3        14.          3.64513722  1.48997072 -1.        ]]\n",
      "24.0\n",
      "[[ 1.6        32.2        18.         32.00255721 19.90878154 -1.        ]]\n",
      "92.15999999999997\n",
      "[[21.6   14.3   19.     4.415 42.828 -1.   ]]\n",
      "-101.12000000000003\n",
      "[[ 5.         10.2        19.         19.26526735 39.83272157 -1.        ]]\n",
      "-1.3599999999999994\n",
      "[[19.9        15.5        19.          4.66466757 38.2829598  -1.        ]]\n",
      "-85.71999999999997\n",
      "[[ 7.9        19.5        19.         13.08951646 26.50840711 -1.        ]]\n",
      "8.680000000000007\n",
      "[[14.5         5.4        19.          6.96968355  8.80327198 -1.        ]]\n",
      "-81.32\n",
      "[[ 6.         11.8        19.         20.73871048 10.71191705 -1.        ]]\n",
      "-3.0400000000000063\n",
      "[[ 9.4        31.5        19.         32.82506277 45.30042964 -1.        ]]\n",
      "36.879999999999995\n",
      "[[12.8        10.1        20.         18.42053292 48.94191218 -1.        ]]\n",
      "-54.72\n",
      "[[ 9.4   23.5   20.     9.988 21.047 -1.   ]]\n",
      "11.28000000000003\n",
      "[[ 7.3   26.7   20.    26.138  1.812 -1.   ]]\n",
      "35.80000000000001\n",
      "[[20.9        14.6        21.         11.18619875 17.34078544 -1.        ]]\n",
      "-95.4\n",
      "[[13.6        31.8        21.         31.71925571  8.8805083  -1.        ]]\n",
      "9.28000000000003\n",
      "[[13.         24.5         8.         28.44253731 37.06350008 -1.        ]]\n",
      "-10.0\n",
      "[[ 1.7        12.5         8.         23.03567673 24.23116427 -1.        ]]\n",
      "28.440000000000012\n",
      "[[ 4.3   15.4    8.     5.29  28.918 -1.   ]]\n",
      "20.04000000000002\n",
      "[[ 5.9         8.          8.         12.7098953  39.13605681 -1.        ]]\n",
      "-14.519999999999996\n",
      "[[ 3.3        26.9         8.         32.75312083 23.74094657 -1.        ]]\n",
      "63.640000000000015\n",
      "[[ 7.6        29.6         9.         44.11811247  3.60712727 -1.        ]]\n",
      "43.03999999999999\n",
      "[[29.7        14.7        15.         30.80689778 11.51708724 -1.        ]]\n",
      "-154.91999999999996\n",
      "[[16.2        17.2        16.         29.43859795 24.47700355 -1.        ]]\n",
      "-55.119999999999976\n",
      "[[ 8.7   26.2   17.     0.134 37.586 -1.   ]]\n",
      "24.680000000000007\n",
      "[[13.8        16.5        17.         14.57556478 32.91919845 -1.        ]]\n",
      "-41.03999999999999\n",
      "[[10.7        10.4        17.         13.41895233 34.41374374 -1.        ]]\n",
      "-39.48000000000002\n",
      "[[17.1         5.2        17.          1.79923679 43.8778617  -1.        ]]\n",
      "-99.64000000000001\n",
      "[[15.5        21.5        17.          7.30175088  7.43449465 -1.        ]]\n",
      "-36.599999999999994\n",
      "[[10.9        15.4        18.          8.65261221  3.23225697 -1.        ]]\n",
      "-24.840000000000003\n",
      "[[13.8   17.1   18.    23.006 30.602 -1.   ]]\n",
      "-39.120000000000005\n",
      "[[ 2.7        15.3        18.         35.00429627 26.47183565 -1.        ]]\n",
      "30.60000000000001\n",
      "[[ 5.2   15.9   18.    22.143 17.407 -1.   ]]\n",
      "15.519999999999982\n",
      "[[18.7        20.7        19.         29.14535144 15.37754878 -1.        ]]\n",
      "-60.91999999999996\n",
      "[[ 6.8        17.6        21.         17.96224165 12.77064964 -1.        ]]\n",
      "10.079999999999984\n",
      "[[24.         16.1        21.         13.65548988 43.41406586 -1.        ]]\n",
      "-111.68\n",
      "[[12.7        27.4        21.         22.25814524 30.98182109 -1.        ]]\n",
      "1.32000000000005\n",
      "[[24.8        10.1        21.          8.06781954 38.56756187 -1.        ]]\n",
      "-136.32\n",
      "[[10.9        14.1        22.         19.21664555 20.5093956  -1.        ]]\n",
      "-29.0\n",
      "[[ 4.          6.1        22.         10.90357297 16.09331662 -1.        ]]\n",
      "-7.679999999999993\n",
      "driver reward  -1501.0799999999995\n",
      "[[ 4.3        35.6         6.         16.45340871 28.87283074 40.        ]]\n",
      "84.68\n",
      "[[ 5.          2.4         7.         17.60899296 26.472182   39.        ]]\n",
      "-26.32\n",
      "[[ 5.2        21.4         8.         24.94559287 40.91664577 38.        ]]\n",
      "33.12000000000003\n",
      "[[ 1.7         5.7         9.         30.88540023 45.30227992 37.        ]]\n",
      "6.68\n",
      "[[21.5        14.1        12.         19.41065876 45.33486769 36.        ]]\n",
      "-101.08000000000001\n",
      "[[ 6.7         5.6        12.          9.35010326 40.58613623 35.        ]]\n",
      "-27.64\n",
      "[[ 7.4         9.4        13.          6.45898145 40.24724    34.        ]]\n",
      "-20.239999999999995\n",
      "[[ 6.         26.5        13.         11.91317199 16.18787663 33.        ]]\n",
      "44.0\n",
      "[[14.6        11.2        14.         11.32092175 41.63051772 32.        ]]\n",
      "-63.43999999999997\n",
      "[[ 5.1        13.8        14.         13.61454627 29.98145518 31.        ]]\n",
      "9.480000000000018\n",
      "[[ 6.5        17.4        14.          8.61625287 52.21444173 30.        ]]\n",
      "11.480000000000018\n",
      "[[11.9        18.5        15.         25.04044924 40.70497158 29.        ]]\n",
      "-21.72\n",
      "[[ 9.7        23.6        15.          1.90367551 30.8054939  28.        ]]\n",
      "9.56000000000003\n",
      "[[11.6        18.3        16.          8.84397943 13.47743974 27.        ]]\n",
      "-20.319999999999993\n",
      "[[ 8.8   24.2   17.    19.971 31.36  26.   ]]\n",
      "17.599999999999994\n",
      "[[ 0.6        43.5        17.         51.2321607   0.83942539 25.        ]]\n",
      "135.12\n",
      "[[ 8.5        39.3         0.          6.07808237 16.48721313 24.        ]]\n",
      "67.96000000000004\n",
      "[[ 3.6        13.4         7.         11.55727385  8.00453779 23.        ]]\n",
      "18.400000000000006\n",
      "[[12.4         2.9         9.          4.56880182 21.2406216  22.        ]]\n",
      "-75.04\n",
      "[[ 9.6         7.4        11.          3.7637914  23.35088174 21.        ]]\n",
      "-41.599999999999994\n",
      "[[ 8.1        22.4        12.         27.86553095 25.97758058 20.        ]]\n",
      "16.599999999999994\n",
      "[[ 5.3    5.6   12.    28.602 26.366 19.   ]]\n",
      "-18.11999999999999\n",
      "[[ 4.5        10.5        14.         32.64940019 35.29895291 18.        ]]\n",
      "3.0\n",
      "[[11.5        24.5        15.         10.96655801 23.00744904 17.        ]]\n",
      "0.20000000000001705\n",
      "[[10.7    6.1   16.    12.663 28.769 16.   ]]\n",
      "-53.23999999999998\n",
      "[[ 7.         12.6        16.         11.40505853 39.10302143 15.        ]]\n",
      "-7.280000000000001\n",
      "[[ 7.    12.8   17.    26.467 32.431 14.   ]]\n",
      "-6.640000000000015\n",
      "[[ 4.3   23.5   17.    26.225 13.209 13.   ]]\n",
      "45.96000000000001\n",
      "[[ 6.6        20.         18.         22.08300399 39.44628893 12.        ]]\n",
      "19.120000000000005\n",
      "[[18.9        39.5        18.         24.96664547 15.56553275 11.        ]]\n",
      "-2.1200000000000045\n",
      "[[14.2        32.7        19.         44.8788339  18.24727028 10.        ]]\n",
      "8.079999999999984\n",
      "[[ 9.    48.8   13.     5.251 60.     9.   ]]\n",
      "94.96000000000004\n",
      "[[12.9         3.6        13.          2.54258266 44.65998289  8.        ]]\n",
      "-76.2\n",
      "[[ 6.         25.8        13.         30.51068821 31.71417081  7.        ]]\n",
      "41.75999999999999\n",
      "[[16.7        11.9        14.         30.04922414 23.17944693  6.        ]]\n",
      "-75.48000000000002\n",
      "[[ 5.2   36.1   15.     5.695 42.959  5.   ]]\n",
      "80.15999999999997\n",
      "[[ 6.4        12.5        16.          6.69093244 36.77567758  4.        ]]\n",
      "-3.519999999999982\n",
      "[[ 9.7        19.5        16.         14.30514506 30.17357739  3.        ]]\n",
      "-3.5600000000000023\n",
      "[[ 5.1         7.5        16.          7.23107923 27.75705403  2.        ]]\n",
      "-10.679999999999993\n",
      "[[ 1.6        16.4        16.         18.89090985 38.71394482  1.        ]]\n",
      "1541.6\n",
      "[[ 4.         26.7        16.         41.15427928 44.55867395  0.        ]]\n",
      "58.24000000000001\n",
      "[[ 3.4   20.1   20.    51.334 58.185 -1.   ]]\n",
      "41.20000000000002\n",
      "[[ 1.6        25.8        12.         38.83210238 35.69157478 -1.        ]]\n",
      "71.67999999999998\n",
      "[[18.3        20.         13.         39.86136326 51.40586198 -1.        ]]\n",
      "-60.44\n",
      "[[11.4        26.2        17.          4.2562781  59.15272949 -1.        ]]\n",
      "6.319999999999993\n",
      "[[ 0.1        27.6        17.         22.47826111 38.57886051 -1.        ]]\n",
      "87.63999999999999\n",
      "[[19.5        21.5        17.         22.61197285 40.57641236 -1.        ]]\n",
      "-63.80000000000001\n",
      "[[24.2   14.    19.     2.553 43.255 -1.   ]]\n",
      "-119.75999999999999\n",
      "[[ 4.9         1.3        19.          3.22547065 39.5327714  -1.        ]]\n",
      "-29.159999999999997\n",
      "[[11.     9.    19.     4.777 29.897 -1.   ]]\n",
      "-46.0\n",
      "[[ 0.8        16.8        19.         18.93398421 20.42893839 -1.        ]]\n",
      "48.31999999999999\n",
      "[[24.6        12.9        19.          2.48005498 49.72737677 -1.        ]]\n",
      "-126.0\n",
      "[[ 6.1        21.8        19.         21.25453623 29.70856972 -1.        ]]\n",
      "28.28\n",
      "[[ 7.3        11.9        19.         25.69008431 28.17941713 -1.        ]]\n",
      "-11.560000000000002\n",
      "[[21.7         4.8        19.          4.98757678 40.87134323 -1.        ]]\n",
      "-132.2\n",
      "[[ 2.4         2.9        20.          8.21884751 42.56154816 -1.        ]]\n",
      "-7.039999999999999\n",
      "[[19.4        30.2        20.         53.06080248 52.88227574 -1.        ]]\n",
      "-35.27999999999997\n",
      "[[ 3.8   11.1   18.    47.64  46.962 -1.   ]]\n",
      "9.680000000000007\n",
      "[[ 1.8        37.5        19.         22.25667285 19.15666121 -1.        ]]\n",
      "107.76000000000005\n",
      "[[21.         11.5        20.         15.4246664  33.56029569 -1.        ]]\n",
      "-106.0\n",
      "[[ 9.9         7.7        20.          2.99664442 35.18570814 -1.        ]]\n",
      "-42.68000000000001\n",
      "[[11.6        21.8        20.         13.82547566 26.98321807 -1.        ]]\n",
      "-9.119999999999976\n",
      "[[14.9   22.4   20.     3.991 42.468 -1.   ]]\n",
      "-29.639999999999986\n",
      "[[ 3.4        19.         21.         16.30894315 52.21742718 -1.        ]]\n",
      "37.68000000000001\n",
      "[[ 4.3        20.5        21.         34.61663513 44.94175346 -1.        ]]\n",
      "36.360000000000014\n",
      "[[ 4.9         8.8        21.         29.98723794 49.93827629 -1.        ]]\n",
      "-5.160000000000011\n",
      "[[21.         41.8        22.         46.69311802 36.15387726 -1.        ]]\n",
      "-9.039999999999964\n",
      "driver reward  1335.5600000000002\n",
      "[[13.1        11.8         1.         35.85631912 26.71715054 40.        ]]\n",
      "-51.31999999999999\n",
      "[[10.2   24.1    7.    13.137 24.582 39.   ]]\n",
      "7.760000000000019\n",
      "[[ 5.2        17.7         8.         10.86263877 44.86478411 38.        ]]\n",
      "21.28\n",
      "[[ 5.6        26.2         8.          3.74133453 17.97640896 37.        ]]\n",
      "45.76000000000002\n",
      "[[ 4.1    9.1    9.    11.656 12.199 36.   ]]\n",
      "1.240000000000009\n",
      "[[ 7.         31.4         9.         33.77318952 40.93843746 35.        ]]\n",
      "52.879999999999995\n",
      "[[ 2.2   20.3   14.    15.088 44.082 34.   ]]\n",
      "50.0\n",
      "[[ 9.2         4.7        14.          6.92525551 35.22915822 33.        ]]\n",
      "-47.51999999999998\n",
      "[[17.7         7.4        15.         19.80450188 43.01110657 32.        ]]\n",
      "-96.68\n",
      "[[10.4         9.8        15.         17.26940993 46.50909291 31.        ]]\n",
      "-39.360000000000014\n",
      "[[ 4.3   29.1   15.    42.764 54.087 30.   ]]\n",
      "63.880000000000024\n",
      "[[ 6.4   35.8   18.     5.361 41.839 29.   ]]\n",
      "71.04000000000002\n",
      "[[18.2        25.4        18.         27.82565541 45.49284639 28.        ]]\n",
      "-42.47999999999996\n",
      "[[ 8.1   32.    18.     5.875 55.38  27.   ]]\n",
      "47.31999999999999\n",
      "[[ 4.3        11.2        18.         11.32727035 41.5330002  26.        ]]\n",
      "6.6000000000000085\n",
      "[[12.1    6.9   19.     6.711 23.607 25.   ]]\n",
      "-60.19999999999999\n",
      "[[ 1.5   11.2   19.     6.679 12.248 24.   ]]\n",
      "25.64\n",
      "[[19.8        42.4        20.         48.47303232 32.35691802 23.        ]]\n",
      "1.0399999999999636\n",
      "[[ 6.3    5.7    7.    45.602 35.787 22.   ]]\n",
      "-24.599999999999994\n",
      "[[ 6.2         9.6         9.         36.80205196 22.80547467 21.        ]]\n",
      "-11.439999999999998\n",
      "[[ 3.8         7.3        11.         42.23763605 29.6914953  20.        ]]\n",
      "-2.4799999999999898\n",
      "[[ 5.8         5.         14.         41.00318929 26.42955439 19.        ]]\n",
      "-23.439999999999998\n",
      "[[ 4.          6.3         6.         36.43639735 18.90396507 18.        ]]\n",
      "-7.040000000000006\n",
      "[[17.6         3.3        13.         22.11464111 27.91672332 17.        ]]\n",
      "-109.12\n",
      "[[22.9        41.7        13.         42.61901078 33.66988241 16.        ]]\n",
      "-22.279999999999973\n",
      "[[ 0.1        13.6        14.         36.13994334 45.60280766 15.        ]]\n",
      "42.84\n",
      "[[ 8.6        14.         14.         30.31185995 31.96373942 14.        ]]\n",
      "-13.680000000000007\n",
      "[[19.1        22.2        16.         15.55866911 55.32808959 13.        ]]\n",
      "-58.839999999999975\n",
      "[[ 3.9        31.1        16.         19.36032489 21.98371349 12.        ]]\n",
      "73.0\n",
      "[[16.6        25.5        16.         31.15078364 23.90037834 11.        ]]\n",
      "-31.28000000000003\n",
      "[[11.2   21.8   17.     3.934 25.559 10.   ]]\n",
      "-6.400000000000006\n",
      "[[ 9.         15.1        17.         24.91048861 14.36084862  9.        ]]\n",
      "-12.879999999999995\n",
      "[[ 1.4         9.2        18.         34.69893732 13.35546741  8.        ]]\n",
      "19.92\n",
      "[[24.5         8.7         9.         10.89935435 29.42460071  7.        ]]\n",
      "-138.76000000000002\n",
      "[[ 6.6        38.9         9.         31.82195924  4.53450822  6.        ]]\n",
      "79.60000000000002\n",
      "[[27.3   28.    12.     8.14  53.255  5.   ]]\n",
      "-96.03999999999996\n",
      "[[15.7        33.1        12.         16.65576226 10.31311149  4.        ]]\n",
      "-0.839999999999975\n",
      "[[ 8.6   17.2   13.     7.77  25.305  3.   ]]\n",
      "-3.4399999999999693\n",
      "[[ 7.         10.9        13.         11.39872977 37.31648574  2.        ]]\n",
      "-12.719999999999985\n",
      "[[ 7.2         6.4        13.         10.77572474 50.08522707  1.        ]]\n",
      "1471.52\n",
      "[[ 6.9        21.3        13.         20.5881804  29.65111333  0.        ]]\n",
      "21.23999999999998\n",
      "[[15.4        17.7        14.          2.92973519 44.71958154 -1.        ]]\n",
      "-48.08000000000001\n",
      "[[12.7         4.3        14.          9.65799785 33.29561557 -1.        ]]\n",
      "-72.6\n",
      "[[ 1.2        10.         14.         16.53626197 27.66844682 -1.        ]]\n",
      "23.840000000000003\n",
      "[[19.6        23.7        14.         22.82739487 27.55037741 -1.        ]]\n",
      "-57.44\n",
      "[[13.9         0.9        15.         14.60024948 37.84966758 -1.        ]]\n",
      "-91.64\n",
      "[[ 2.7        21.3        15.         28.32304888 50.56921269 -1.        ]]\n",
      "49.80000000000001\n",
      "[[ 7.1        11.6        17.         37.10390996 56.14990431 -1.        ]]\n",
      "-11.159999999999997\n",
      "[[10.7        21.3         5.          5.67253816 50.28822672 -1.        ]]\n",
      "-4.599999999999994\n",
      "[[ 6.5        15.8         5.          7.22768811 29.70819211 -1.        ]]\n",
      "6.359999999999985\n",
      "[[14.8        25.8         5.         25.05282703 20.68057659 -1.        ]]\n",
      "-18.079999999999984\n",
      "[[22.1        23.9         6.          4.77967425 55.71839185 -1.        ]]\n",
      "-73.80000000000001\n",
      "[[20.4         8.7         6.         10.6507884  41.24068213 -1.        ]]\n",
      "-110.87999999999997\n",
      "[[20.6         9.          6.          4.26333076 12.32869975 -1.        ]]\n",
      "-111.28\n",
      "[[ 8.7         8.6         7.          2.88589495 18.93168969 -1.        ]]\n",
      "-31.639999999999972\n",
      "[[ 7.1        11.7         8.          5.90361972 23.4892689  -1.        ]]\n",
      "-10.839999999999975\n",
      "[[17.1        15.9         9.          7.24983169 56.2864762  -1.        ]]\n",
      "-65.4\n",
      "[[14.8         5.8         9.          4.00013514 46.78586253 -1.        ]]\n",
      "-82.08000000000001\n",
      "[[15.8         4.7         9.         15.83487143 38.44149301 -1.        ]]\n",
      "-92.4\n",
      "[[ 2.8         3.3         9.         12.81019138 34.28474099 -1.        ]]\n",
      "-8.479999999999997\n",
      "[[ 8.7    6.1    9.    11.033 31.312 -1.   ]]\n",
      "-39.639999999999986\n",
      "[[10.7        19.6         9.         22.25355818 34.11171011 -1.        ]]\n",
      "-10.039999999999992\n",
      "[[ 4.7         5.9        10.         26.95301184 24.58235527 -1.        ]]\n",
      "-13.080000000000013\n",
      "[[ 7.7         6.9        11.         22.8233085  35.57091432 -1.        ]]\n",
      "-30.28\n",
      "[[ 0.6        34.9        12.         47.0297968  11.20136108 -1.        ]]\n",
      "107.6\n",
      "[[ 7.1         9.         18.         44.1865085  22.92084511 -1.        ]]\n",
      "-19.480000000000004\n",
      "[[20.3        26.9        11.         17.07496837 36.36605137 -1.        ]]\n",
      "-51.960000000000036\n",
      "[[21.4        33.1        11.         54.22914689 43.38050446 -1.        ]]\n",
      "-39.599999999999966\n",
      "[[ 5.5        16.2        13.         38.77687194 47.43724363 -1.        ]]\n",
      "14.439999999999998\n",
      "[[ 5.5   17.8   14.    21.072 45.358 -1.   ]]\n",
      "19.560000000000002\n",
      "[[15.5         8.6        14.          5.95394969 35.86082567 -1.        ]]\n",
      "-77.88\n",
      "[[ 0.8        22.9        14.         27.06759082 42.79017461 -1.        ]]\n",
      "67.84\n",
      "[[ 8.8        13.1        14.          9.48474228 40.3091546  -1.        ]]\n",
      "-17.919999999999987\n",
      "[[ 9.9        18.         14.         26.10656604 29.57202892 -1.        ]]\n",
      "-9.719999999999999\n",
      "[[12.6        30.2        14.         37.73860562  1.78885425 -1.        ]]\n",
      "10.960000000000036\n",
      "[[23.1        10.         16.         12.06343695 16.5177209  -1.        ]]\n",
      "-125.08000000000001\n",
      "[[16.4        23.5        17.         18.74337624 48.1325802  -1.        ]]\n",
      "-36.31999999999999\n",
      "[[ 4.2         2.1        17.         16.51520136 44.41827368 -1.        ]]\n",
      "-21.840000000000003\n",
      "[[20.6        14.9        17.         17.31225057 30.58759573 -1.        ]]\n",
      "-92.4\n",
      "[[14.7         4.7        17.          5.77996444 33.5956674  -1.        ]]\n",
      "-84.91999999999999\n",
      "[[ 7.1        19.8        17.         25.10116203 50.56049228 -1.        ]]\n",
      "15.080000000000013\n",
      "[[26.3        15.         18.         20.69757904 21.77653676 -1.        ]]\n",
      "-130.83999999999997\n",
      "[[25.6        36.2        18.         13.30085974  4.33937223 -1.        ]]\n",
      "-58.24000000000001\n",
      "[[ 9.8        11.         19.         10.48057801 11.87789581 -1.        ]]\n",
      "-31.439999999999998\n",
      "[[19.6   18.7   19.     2.589 49.125 -1.   ]]\n",
      "-73.44\n",
      "[[ 4.2         4.4        20.          7.74447275 51.09089448 -1.        ]]\n",
      "-14.480000000000011\n",
      "[[16.8        13.5        20.          1.06149278 21.59705364 -1.        ]]\n",
      "-71.03999999999999\n",
      "[[ 4.1    3.2   20.     4.455 17.242 -1.   ]]\n",
      "-17.64\n",
      "[[ 6.2        33.2        20.         40.67617972  4.28808661 -1.        ]]\n",
      "64.07999999999998\n",
      "[[ 7.5   34.7   18.     9.076 19.772 -1.   ]]\n",
      "60.039999999999964\n",
      "[[ 8.1         0.8        18.          8.05962807 11.56166958 -1.        ]]\n",
      "-52.52\n",
      "[[ 7.6         9.9        18.          9.41812883 15.82977265 -1.        ]]\n",
      "-20.0\n",
      "[[13.1    5.5   19.     1.534 27.551 -1.   ]]\n",
      "-71.48\n",
      "[[ 5.8    7.4   19.     7.338 31.636 -1.   ]]\n",
      "-15.759999999999991\n",
      "[[14.2        27.2        19.          6.43935815 18.46584303 -1.        ]]\n",
      "-9.519999999999982\n",
      "[[ 5.2        20.2        19.         21.13793929 30.18053202 -1.        ]]\n",
      "29.28\n",
      "[[11.5        21.         20.         21.34099688 46.68324041 -1.        ]]\n",
      "-11.0\n",
      "[[15.3        18.6        20.         22.81115527 24.56067634 -1.        ]]\n",
      "-44.52000000000004\n",
      "[[25.         11.2        22.         12.52319379 46.38369605 -1.        ]]\n",
      "-134.16000000000003\n",
      "[[12.8        18.4        23.         32.00591629 38.82073723 -1.        ]]\n",
      "-28.159999999999997\n",
      "driver reward  -686.1999999999995\n",
      "[[ 5.3    3.3    8.    45.31  17.631 40.   ]]\n",
      "-25.479999999999997\n",
      "[[12.    12.9    8.    44.74  18.843 39.   ]]\n",
      "-40.31999999999999\n",
      "[[ 9.7   26.5    9.    13.792 30.765 38.   ]]\n",
      "18.839999999999975\n",
      "[[ 6.5        11.7         9.          5.12097303 33.59808169 37.        ]]\n",
      "-6.759999999999991\n",
      "[[ 9.5    4.8    9.     8.128 27.453 36.   ]]\n",
      "-49.24000000000001\n",
      "[[11.8        13.9        11.          2.02850098 43.35154998 35.        ]]\n",
      "-35.76000000000002\n",
      "[[11.6        11.2        11.         16.76550723 58.48946623 34.        ]]\n",
      "-43.039999999999964\n",
      "[[11.6    7.1   11.    13.772 55.034 33.   ]]\n",
      "-56.16\n",
      "[[10.2         4.8        11.         14.31453705 40.02888267 32.        ]]\n",
      "-54.0\n",
      "[[ 6.8        28.2        11.         19.61763798 19.26199528 31.        ]]\n",
      "44.0\n",
      "[[ 8.6        14.2        12.         25.604538   19.01838323 30.        ]]\n",
      "-13.039999999999964\n",
      "[[14.9         6.3        12.         19.14197984 29.00319295 29.        ]]\n",
      "-81.16\n",
      "[[ 8.1         7.4        12.         18.67735354 29.48536474 28.        ]]\n",
      "-31.39999999999999\n",
      "[[ 5.8        13.2        12.         12.43513198 41.45402496 27.        ]]\n",
      "2.8000000000000114\n",
      "[[ 0.4        22.8        12.         31.71866027 29.30992968 26.        ]]\n",
      "70.24000000000001\n",
      "[[ 7.9         4.2        15.         29.37115257 33.82589077 25.        ]]\n",
      "-40.28\n",
      "[[14.5        14.1        16.         28.94803589 51.35465911 24.        ]]\n",
      "-53.48000000000002\n",
      "[[ 4.         13.3        17.         22.79641933 58.52092288 23.        ]]\n",
      "15.36\n",
      "[[15.8        17.5        18.         26.37496808 50.89992923 22.        ]]\n",
      "-51.43999999999997\n",
      "[[ 3.2   40.5   18.     5.586 17.648 21.   ]]\n",
      "107.83999999999997\n",
      "[[ 8.8        19.6        18.          8.22646142 45.94317559 20.        ]]\n",
      "2.8799999999999955\n",
      "[[12.2        16.8        18.         22.09959352 50.40418191 19.        ]]\n",
      "-29.19999999999999\n",
      "[[ 4.5        20.6        18.         40.30429437 50.51111691 18.        ]]\n",
      "35.31999999999999\n",
      "[[10.3         3.4        21.         26.63817399 51.36928384 17.        ]]\n",
      "-59.16000000000001\n",
      "[[ 6.6        25.5        21.          0.06092643 47.65194015 16.        ]]\n",
      "36.72\n",
      "[[ 4.3    6.3    5.     8.074 49.296 15.   ]]\n",
      "-9.079999999999998\n",
      "[[ 4.4        22.1         5.         26.08084736 45.959791   14.        ]]\n",
      "40.80000000000001\n",
      "[[ 2.4        20.9         6.          5.95385586 57.67922999 13.        ]]\n",
      "50.56000000000003\n",
      "[[11.4        11.2         7.         12.28952603 49.25906354 12.        ]]\n",
      "-41.68000000000001\n",
      "[[10.9        20.1         7.         15.69034439 36.48135564 11.        ]]\n",
      "-9.799999999999983\n",
      "[[13.3        15.8         8.         13.39082364 44.33452973 10.        ]]\n",
      "-39.879999999999995\n",
      "[[ 6.         25.9         8.         36.75273743 41.35156241  9.        ]]\n",
      "42.08000000000001\n",
      "[[ 8.8        35.4         8.          2.92689405 57.6011785   8.        ]]\n",
      "53.44\n",
      "[[ 4.7         8.9         9.         13.37092344 51.73838422  7.        ]]\n",
      "-3.480000000000004\n",
      "[[ 3.5        40.3         9.         45.67799491 33.67968422  6.        ]]\n",
      "105.16000000000003\n",
      "[[ 6.8         7.7        18.         41.86388241 31.97984729  5.        ]]\n",
      "-21.599999999999994\n",
      "[[ 6.    43.3   19.     5.233 24.889  4.   ]]\n",
      "97.76000000000005\n",
      "[[ 3.7        24.5        19.         23.60821681 36.93828215  3.        ]]\n",
      "53.24000000000001\n",
      "[[ 9.4        27.2        20.         47.77532696 17.21686937  2.        ]]\n",
      "23.120000000000005\n",
      "[[ 0.4   45.1   19.     5.081 32.742  1.   ]]\n",
      "1641.6\n",
      "[[ 6.5        39.3        19.         46.40049173 29.54733996  0.        ]]\n",
      "81.56\n",
      "[[19.     7.9   13.    26.129 43.098 -1.   ]]\n",
      "-103.91999999999999\n",
      "[[18.9    8.    13.     3.834 55.943 -1.   ]]\n",
      "-102.91999999999999\n",
      "[[ 6.9        23.8        13.         30.22382549 41.28135426 -1.        ]]\n",
      "29.23999999999998\n",
      "[[ 5.9        30.9        13.         21.64624082  9.49773124 -1.        ]]\n",
      "58.76000000000002\n",
      "[[16.4         5.4        14.         19.92041186 24.90124124 -1.        ]]\n",
      "-94.23999999999998\n",
      "[[16.3         7.6        14.          6.52207245 25.73408384 -1.        ]]\n",
      "-86.51999999999998\n",
      "[[ 3.          7.6        14.          1.34291124 26.43030157 -1.        ]]\n",
      "3.9200000000000017\n",
      "[[ 4.7        20.9        15.          3.05910561 46.40918479 -1.        ]]\n",
      "34.920000000000016\n",
      "[[ 8.5        22.3        15.         26.87619417 40.64941716 -1.        ]]\n",
      "13.560000000000002\n",
      "[[14.4        20.         16.          7.8215699  17.82383043 -1.        ]]\n",
      "-33.91999999999999\n",
      "[[ 3.8        29.2        16.         33.58703108 15.40891838 -1.        ]]\n",
      "67.6\n",
      "[[15.9        27.3        18.          4.75377529 25.91377338 -1.        ]]\n",
      "-20.75999999999999\n",
      "[[ 2.8        12.9        18.          1.99359984 41.42450968 -1.        ]]\n",
      "22.24000000000001\n",
      "[[16.4    5.5   18.     7.975 60.    -1.   ]]\n",
      "-93.91999999999999\n",
      "[[ 5.4        17.6        18.         20.49449701 48.52410486 -1.        ]]\n",
      "19.599999999999994\n",
      "[[15.1    6.3   19.    12.693 29.987 -1.   ]]\n",
      "-82.51999999999998\n",
      "[[16.          5.9        19.          1.42016505 39.97694987 -1.        ]]\n",
      "-89.91999999999999\n",
      "[[ 5.6        28.1        19.         19.12997963 20.01841138 -1.        ]]\n",
      "51.839999999999975\n",
      "[[19.8        14.8        19.         10.89477487 20.3595198  -1.        ]]\n",
      "-87.28\n",
      "[[ 9.1         7.7        19.          9.6102395  21.05790327 -1.        ]]\n",
      "-37.239999999999995\n",
      "[[ 6.         37.         19.         40.94328259 31.74008357 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 8.1        18.7         8.         30.45739459 20.65091679 -1.        ]]\n",
      "4.760000000000019\n",
      "[[ 8.7         5.          8.         20.92564306 13.20589542 -1.        ]]\n",
      "-43.16\n",
      "[[ 8.    24.1    9.     8.777 36.521 -1.   ]]\n",
      "22.72\n",
      "[[ 4.7         1.2         9.          4.24696805 32.80411833 -1.        ]]\n",
      "-28.120000000000005\n",
      "[[ 2.2        36.9         9.         36.0224772  55.39441013 -1.        ]]\n",
      "103.12\n",
      "[[ 4.6        10.4        11.         28.37768002 44.29997853 -1.        ]]\n",
      "2.0\n",
      "[[ 2.7         3.8        12.         26.38518101 50.48971572 -1.        ]]\n",
      "-6.199999999999996\n",
      "[[ 9.9        12.4        12.         27.44755591 35.39416128 -1.        ]]\n",
      "-27.640000000000015\n",
      "[[ 1.3         5.5        13.         24.52112345 29.4614269  -1.        ]]\n",
      "8.760000000000005\n",
      "[[13.3         6.7        15.         11.59632153 16.6570684  -1.        ]]\n",
      "-69.0\n",
      "[[ 8.    30.5   15.     3.415 47.344 -1.   ]]\n",
      "43.19999999999999\n",
      "[[ 8.1        11.6        15.         14.10337216 39.96860934 -1.        ]]\n",
      "-17.95999999999998\n",
      "[[10.6         7.1        15.         17.35563865 48.45761019 -1.        ]]\n",
      "-49.359999999999985\n",
      "[[ 0.8         6.3        15.         12.2677657  43.60292119 -1.        ]]\n",
      "14.720000000000006\n",
      "[[ 9.3        16.4        15.         15.12145507 33.38053869 -1.        ]]\n",
      "-10.759999999999991\n",
      "[[16.3        14.3        16.         12.84763656 19.20057421 -1.        ]]\n",
      "-65.08000000000001\n",
      "[[ 9.6        16.2        17.         20.29486002 22.22459661 -1.        ]]\n",
      "-13.43999999999997\n",
      "[[10.6        23.         17.         46.54213265 18.92318986 -1.        ]]\n",
      "1.5200000000000102\n",
      "[[22.5        18.4        12.          9.80862876 26.33556996 -1.        ]]\n",
      "-94.12\n",
      "[[ 6.         24.3        12.         17.81067092 43.39367517 -1.        ]]\n",
      "36.96000000000001\n",
      "[[10.2        11.2        12.         19.76146566 42.37048509 -1.        ]]\n",
      "-33.51999999999998\n",
      "[[ 4.8        16.5        12.         23.29301498 22.76869675 -1.        ]]\n",
      "20.159999999999997\n",
      "[[ 5.5        16.8        12.         33.18407071 42.27020438 -1.        ]]\n",
      "16.359999999999985\n",
      "[[17.2        24.5        12.          0.90342949 45.64679259 -1.        ]]\n",
      "-38.56\n",
      "[[14.4         8.1        13.          5.08439037 36.74349976 -1.        ]]\n",
      "-72.0\n",
      "[[ 7.6         9.9        13.          6.01525623 40.44053907 -1.        ]]\n",
      "-20.0\n",
      "[[ 4.2    7.5   13.     9.798 49.135 -1.   ]]\n",
      "-4.559999999999988\n",
      "[[12.6        26.2        13.         19.20812337 39.17162922 -1.        ]]\n",
      "-1.839999999999975\n",
      "[[ 3.1        10.4        13.         21.05281452 48.67662219 -1.        ]]\n",
      "12.200000000000003\n",
      "[[ 0.9        37.9        13.         50.38104603 25.20045873 -1.        ]]\n",
      "115.16000000000003\n",
      "[[26.5         1.2        20.         32.70312429 46.07148054 -1.        ]]\n",
      "-176.35999999999999\n",
      "[[22.    12.3   20.     0.845 55.572 -1.   ]]\n",
      "-110.23999999999998\n",
      "[[ 3.5        16.7        21.         18.25426142 45.58950492 -1.        ]]\n",
      "29.640000000000015\n",
      "[[ 6.9        41.2        21.         30.4107725   9.31176178 -1.        ]]\n",
      "84.92000000000002\n",
      "[[19.3   36.1    7.     1.998 56.617 -1.   ]]\n",
      "-15.720000000000027\n",
      "[[22.4        13.          7.         10.14081715 22.18721789 -1.        ]]\n",
      "-110.71999999999997\n",
      "[[16.6         2.8         7.          4.55975566 36.13846796 -1.        ]]\n",
      "-103.92000000000002\n",
      "[[ 2.2        27.6         7.         14.99089979 11.29981019 -1.        ]]\n",
      "73.36000000000001\n",
      "[[22.9        24.5         8.         18.75931946 12.01922733 -1.        ]]\n",
      "-77.32\n",
      "[[ 7.6        10.9         8.         18.64744269  7.88637312 -1.        ]]\n",
      "-16.799999999999997\n",
      "[[ 2.1   10.7    9.     6.767 12.249 -1.   ]]\n",
      "19.960000000000008\n",
      "[[ 6.3        39.          9.         25.54447261 44.70441097 -1.        ]]\n",
      "81.96000000000004\n",
      "[[17.          8.9         9.         14.143324   50.78992767 -1.        ]]\n",
      "-87.11999999999998\n",
      "[[12.4         5.          9.          8.12055629 58.06730208 -1.        ]]\n",
      "-68.32\n",
      "[[ 6.2         3.4        10.         13.75889479 52.05866368 -1.        ]]\n",
      "-31.28\n",
      "[[11.     9.1   10.    16.92  56.105 -1.   ]]\n",
      "-45.68000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.4         6.         10.          9.72103005 43.58738335 -1.        ]]\n",
      "-37.92\n",
      "[[19.4         2.6        10.         25.59035029 51.56042439 -1.        ]]\n",
      "-123.6\n",
      "[[20.         32.7        11.         34.71517053 36.76045829 -1.        ]]\n",
      "-31.360000000000014\n",
      "[[25.7   37.2   12.     8.071 20.542 -1.   ]]\n",
      "-55.72000000000003\n",
      "[[18.1        33.9        12.         38.72850415 27.73733932 -1.        ]]\n",
      "-14.599999999999966\n",
      "[[17.6        16.3        12.         36.49993456 59.83828461 -1.        ]]\n",
      "-67.52000000000004\n",
      "[[16.2         3.7        14.         28.49046349 45.14100782 -1.        ]]\n",
      "-98.32\n",
      "[[ 5.    25.6   14.     3.806 38.741 -1.   ]]\n",
      "47.91999999999999\n",
      "[[ 3.1         8.7        14.          2.99333896 33.09030822 -1.        ]]\n",
      "6.760000000000005\n",
      "[[12.5        34.         14.         42.05856294 47.07411239 -1.        ]]\n",
      "23.80000000000001\n",
      "[[24.5        14.2        15.         13.43428721 41.07740436 -1.        ]]\n",
      "-121.16000000000003\n",
      "[[11.7        23.         15.         19.26762912 30.81630256 -1.        ]]\n",
      "-5.960000000000008\n",
      "[[14.1         6.2        15.         10.67969281 33.16140654 -1.        ]]\n",
      "-76.03999999999999\n",
      "[[ 3.3        17.1        15.          4.18210198 51.55597244 -1.        ]]\n",
      "32.28\n",
      "[[ 2.5         2.6        15.          2.85778095 52.28149066 -1.        ]]\n",
      "-8.68\n",
      "[[17.1         4.9        15.          6.77390279 34.2067716  -1.        ]]\n",
      "-100.6\n",
      "[[ 3.8        20.7        15.         17.86110535 13.18958188 -1.        ]]\n",
      "40.400000000000006\n",
      "[[18.2        12.         16.         16.05287009 31.74985224 -1.        ]]\n",
      "-85.35999999999999\n",
      "[[12.3        12.1        16.         11.28181195 27.3001329  -1.        ]]\n",
      "-44.91999999999999\n",
      "[[21.8         5.1        16.          6.68397339 44.84081087 -1.        ]]\n",
      "-131.92\n",
      "[[ 7.8        14.         16.         16.19263064 26.00298833 -1.        ]]\n",
      "-8.240000000000009\n",
      "[[ 9.6        15.2        16.         20.94642821 28.9479737  -1.        ]]\n",
      "-16.639999999999986\n",
      "[[20.8        24.8        16.         23.41544054 21.1901275  -1.        ]]\n",
      "-62.079999999999984\n",
      "[[22.4         4.1        16.          3.24344421 32.09706313 -1.        ]]\n",
      "-139.2\n",
      "[[18.1   16.2   16.     3.138 39.865 -1.   ]]\n",
      "-71.23999999999998\n",
      "[[ 4.8        42.5        16.         48.93724649 38.62521003 -1.        ]]\n",
      "103.36000000000001\n",
      "[[ 1.         15.5        21.         34.27827646 42.88013589 -1.        ]]\n",
      "42.8\n",
      "[[10.9         3.9        21.         31.04215077 36.60381196 -1.        ]]\n",
      "-61.64\n",
      "[[  7.1         31.3          0.          59.2892586   23.58461374\n",
      "  140.        ]]\n",
      "51.879999999999995\n",
      "driver reward  -486.8400000000006\n",
      "total reward  1811.0400000000081\n",
      "trips [  6.  12.   0.   4.   4.  27.  34.  87. 103. 136.  59.  91. 144. 139.\n",
      " 120.  94.  76. 148. 143. 152. 103.  61.  25.   8.]\n",
      "[1.0, 1.0, nan, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-ff47d4624acb>:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hrly_acceptance_rates.append(hrly_accepted_trips[j]/hrly_trip_counts[j])\n"
     ]
    }
   ],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    hrly_accepted_trips = np.zeros(24)\n",
    "    hrly_trip_counts = np.zeros(24)\n",
    "    hrly_acceptance_rates = []\n",
    "    for state_list in eval_env[0]:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "                print(np.array([state_list[i][\"observation\"]]))\n",
    "                hrly_accepted_trips[int(np.array([state_list[i][\"observation\"]])[0][2])] +=1\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "            hrly_trip_counts[int(np.array([state_list[i][\"observation\"]])[0][2])] +=1\n",
    "            \n",
    "            \n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "    \n",
    "    #find average acceptance for each hour\n",
    "    print(\"trips\", hrly_trip_counts )\n",
    "    for j in range(24):\n",
    "        hrly_acceptance_rates.append(hrly_accepted_trips[j]/hrly_trip_counts[j])\n",
    "    print (hrly_acceptance_rates)\n",
    "\n",
    "evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.7        19.4         6.         38.13065633 23.72463914 40.        ]]\n",
      "2.920000000000016\n",
      "[[ 9.3        15.          7.         15.64394863 32.35216921 39.        ]]\n",
      "-15.240000000000009\n",
      "[[15.3         7.3         8.         12.27858908 40.23402489 38.        ]]\n",
      "-80.68\n",
      "[[ 6.2        27.6         8.          1.28675881 21.7954455  37.        ]]\n",
      "46.15999999999997\n",
      "[[20.5        37.4        11.          4.56496532  1.33065333 36.        ]]\n",
      "-19.71999999999997\n",
      "[[ 6.1        31.         15.          5.99278098 34.49980792 35.        ]]\n",
      "57.72\n",
      "[[12.9         4.7        15.          7.68919575 51.90987375 34.        ]]\n",
      "-72.68\n",
      "[[ 5.4        26.2        16.         13.04517826 31.1342781  33.        ]]\n",
      "47.120000000000005\n",
      "[[ 6.          7.4        16.         23.7540709  32.17972756 32.        ]]\n",
      "-17.120000000000005\n",
      "[[ 7.6        26.6        17.         21.98445076 12.22969952 31.        ]]\n",
      "33.44\n",
      "[[ 1.6   25.2   17.    28.006 36.613 30.   ]]\n",
      "69.75999999999999\n",
      "[[ 7.9        18.1        17.         39.47298694 41.70445438 29.        ]]\n",
      "4.200000000000017\n",
      "[[ 8.6         9.5        21.         26.57652771 33.04577751 28.        ]]\n",
      "-28.080000000000013\n",
      "[[ 9.          9.7        21.         26.68096263 31.67927162 27.        ]]\n",
      "-30.159999999999997\n",
      "[[10.2        17.1        22.         12.97657632 48.97594117 26.        ]]\n",
      "-14.639999999999986\n",
      "[[ 5.6        34.6        22.         27.28885592 17.55701926 25.        ]]\n",
      "72.63999999999999\n",
      "[[17.         11.1        10.         17.53605027 20.11872953 24.        ]]\n",
      "-80.08000000000001\n",
      "[[ 8.         28.5        12.         51.78216149 23.3292893  23.        ]]\n",
      "36.80000000000001\n",
      "[[ 7.1   36.8   14.    10.556 18.298 22.   ]]\n",
      "69.48000000000002\n",
      "[[ 4.8        11.3        14.         14.05733979 32.38715642 21.        ]]\n",
      "3.519999999999996\n",
      "[[ 9.6         1.5        15.          6.19759563 37.45383336 20.        ]]\n",
      "-60.47999999999999\n",
      "[[ 7.7        29.7        15.         23.30816996  6.9981076  19.        ]]\n",
      "42.68000000000001\n",
      "[[24.7        29.         18.         36.97527282 30.04374237 18.        ]]\n",
      "-75.16000000000003\n",
      "[[ 9.1   28.5   19.     9.915 56.093 17.   ]]\n",
      "29.319999999999993\n",
      "[[16.9        16.4        19.         20.61199968 50.24027999 16.        ]]\n",
      "-62.43999999999997\n",
      "[[15.8   15.4   19.    17.575 49.426 15.   ]]\n",
      "-58.160000000000025\n",
      "[[12.9        29.6        19.         32.09777814 58.18795286 14.        ]]\n",
      "7.0\n",
      "[[ 5.2         4.7        21.         24.37255659 58.22240653 13.        ]]\n",
      "-20.320000000000007\n",
      "[[ 8.4        14.4         1.         27.6883343  51.38884607 12.        ]]\n",
      "-11.039999999999992\n",
      "[[ 9.7        16.1         5.         28.8150048  41.94980484 11.        ]]\n",
      "-14.439999999999998\n",
      "[[18.6        24.3         6.         12.36047907  5.0556076  10.        ]]\n",
      "-48.72000000000003\n",
      "[[14.9         3.4         7.          2.75361363 20.18658422  9.        ]]\n",
      "-90.44\n",
      "[[ 6.6        29.          7.         35.00059951  5.54753604  8.        ]]\n",
      "47.91999999999999\n",
      "[[17.3   24.3   11.     5.043 34.341  7.   ]]\n",
      "-39.879999999999995\n",
      "[[ 3.2         8.1        11.          8.37800632 39.01386326  6.        ]]\n",
      "4.159999999999997\n",
      "[[ 6.8         9.3        11.          4.96251855 25.31347935  5.        ]]\n",
      "-16.480000000000004\n",
      "[[ 6.9        12.3        11.          7.88535325 17.14493953  4.        ]]\n",
      "-7.560000000000002\n",
      "[[19.4         5.7        11.          0.74136244 38.84078843  3.        ]]\n",
      "-113.67999999999998\n",
      "[[ 7.6        27.2        11.         24.20603856 17.12153836  2.        ]]\n",
      "35.360000000000014\n",
      "[[ 6.    21.9   11.     3.626 13.388  1.   ]]\n",
      "1529.28\n",
      "[[ 2.1        17.7        11.          8.15396392 28.92100985  0.        ]]\n",
      "42.359999999999985\n",
      "[[ 7.         25.6        11.         26.22620263 17.0795537  -1.        ]]\n",
      "34.31999999999999\n",
      "[[ 4.2        35.3        11.         58.89176977  0.97829417 -1.        ]]\n",
      "84.40000000000003\n",
      "driver reward  1323.3599999999997\n",
      "[[ 5.2        39.6         1.         19.05672516 14.76536628 40.        ]]\n",
      "91.35999999999996\n",
      "[[ 5.2         1.6         6.         13.79823574 12.11496852 39.        ]]\n",
      "-30.240000000000002\n",
      "[[18.2         8.5         8.          6.20317438 18.21680476 38.        ]]\n",
      "-96.56\n",
      "[[ 2.7        21.6         8.          4.9105271  37.47731101 37.        ]]\n",
      "50.75999999999999\n",
      "[[ 7.4        12.          8.          2.35328    23.14724079 36.        ]]\n",
      "-11.919999999999987\n",
      "[[ 4.4        18.5         8.          3.38403508  4.3407305  35.        ]]\n",
      "29.28\n",
      "[[ 3.5   10.6   11.     6.09  14.011 34.   ]]\n",
      "10.120000000000005\n",
      "[[ 4.         24.6        12.          4.3876003  41.25369412 33.        ]]\n",
      "51.51999999999998\n",
      "[[ 7.2        31.         12.         20.7370446   7.97016401 32.        ]]\n",
      "50.24000000000001\n",
      "[[24.6         3.8        14.          1.5835935  19.83692218 31.        ]]\n",
      "-155.12\n",
      "[[ 4.1         0.9        15.          5.56690435 19.2464721  30.        ]]\n",
      "-25.0\n",
      "[[ 4.8         8.5        17.         11.60860381 28.30234247 29.        ]]\n",
      "-5.439999999999998\n",
      "[[ 6.7        18.3        17.         11.52839793 42.18371107 28.        ]]\n",
      "13.0\n",
      "[[11.    11.    17.     8.809 57.862 27.   ]]\n",
      "-39.599999999999994\n",
      "[[ 2.8        11.6        17.         15.91948786 50.74396994 26.        ]]\n",
      "18.080000000000013\n",
      "[[12.4         0.6        17.          7.78438615 59.62745726 25.        ]]\n",
      "-82.39999999999999\n",
      "[[10.7        24.7        17.          6.28934737 25.33499432 24.        ]]\n",
      "6.28000000000003\n",
      "[[10.1         5.5        17.         10.56982625 10.42966058 23.        ]]\n",
      "-51.08\n",
      "[[10.3        18.7        18.         13.06863492  2.38846842 22.        ]]\n",
      "-10.199999999999989\n",
      "[[10.1        36.3        22.         24.13638755 44.07450132 21.        ]]\n",
      "47.48000000000002\n",
      "[[18.7        31.6         3.          4.04940692 24.55256495 20.        ]]\n",
      "-26.039999999999964\n",
      "[[ 8.1   11.3    5.     2.213 27.644 19.   ]]\n",
      "-18.919999999999987\n",
      "[[19.7        34.          5.         40.45328872 50.32393754 18.        ]]\n",
      "-25.160000000000025\n",
      "[[20.7   30.     7.     8.185 11.23  17.   ]]\n",
      "-44.75999999999999\n",
      "[[ 8.2   10.8    9.     7.054 21.007 16.   ]]\n",
      "-21.19999999999999\n",
      "[[ 4.5        10.5         9.          0.54619013 19.63951508 15.        ]]\n",
      "3.0\n",
      "[[14.5   14.2   10.     3.483 45.512 14.   ]]\n",
      "-53.16\n",
      "[[ 3.9        39.8        10.         20.9103921   8.19137374 13.        ]]\n",
      "100.84000000000003\n",
      "[[ 8.          2.1        11.         22.19742533 14.69249578 12.        ]]\n",
      "-47.67999999999999\n",
      "[[23.2        16.         11.         17.43146562 44.28026353 11.        ]]\n",
      "-106.56\n",
      "[[10.8         7.         11.         11.6024529  28.35650275 10.        ]]\n",
      "-51.040000000000006\n",
      "[[ 7.2        13.6        11.          3.93198388 47.14998345  9.        ]]\n",
      "-5.439999999999998\n",
      "[[ 8.4        11.         11.          9.01049584 28.80042444  8.        ]]\n",
      "-21.919999999999987\n",
      "[[16.2        12.5        11.          7.42424009 33.74754731  7.        ]]\n",
      "-70.16\n",
      "[[16.5        12.9        11.         14.82906071 37.1597521   6.        ]]\n",
      "-70.91999999999999\n",
      "[[ 9.7         9.1        12.         13.19799729 38.17435915  5.        ]]\n",
      "-36.839999999999975\n",
      "[[12.3    5.1   12.     2.171 24.845  4.   ]]\n",
      "-67.32\n",
      "[[ 4.4         6.         12.          6.17858193 28.59551846  3.        ]]\n",
      "-10.719999999999999\n",
      "[[14.6    7.3   12.     3.388 37.939  2.   ]]\n",
      "-75.91999999999999\n",
      "[[ 4.9        27.9        12.          7.60120855 12.64459618  1.        ]]\n",
      "1555.96\n",
      "[[ 4.7         4.9        13.          6.05186938 18.49562448  0.        ]]\n",
      "-16.28\n",
      "[[19.8         9.1        13.         14.50941849 37.90092358 -1.        ]]\n",
      "-105.51999999999998\n",
      "[[ 8.2         5.1        13.          5.62939155 29.71722223 -1.        ]]\n",
      "-39.43999999999998\n",
      "[[ 4.1        17.8        13.         12.4497565  42.47574363 -1.        ]]\n",
      "29.080000000000013\n",
      "[[ 1.6        27.5        13.         10.56763873 16.24458578 -1.        ]]\n",
      "77.12\n",
      "[[ 5.1        22.5        13.         14.60628992 36.65130044 -1.        ]]\n",
      "37.31999999999999\n",
      "[[18.1        19.9        13.          6.25308757 42.61418113 -1.        ]]\n",
      "-59.39999999999998\n",
      "[[13.4        19.9        13.         13.14122864 47.22324251 -1.        ]]\n",
      "-27.43999999999997\n",
      "[[14.1    8.    14.    18.435 37.059 -1.   ]]\n",
      "-70.28\n",
      "[[19.7        39.3        14.         37.97771901 44.77201185 -1.        ]]\n",
      "-8.199999999999989\n",
      "[[ 6.1        17.8        15.         15.18059642 37.66795005 -1.        ]]\n",
      "15.480000000000018\n",
      "[[13.6        16.         15.         23.81907016 29.58288515 -1.        ]]\n",
      "-41.28\n",
      "[[ 2.6         9.5        15.         13.47942803 34.42188827 -1.        ]]\n",
      "12.719999999999999\n",
      "[[18.6        17.4        15.         16.33834117 58.99785228 -1.        ]]\n",
      "-70.79999999999998\n",
      "[[13.2        22.7        15.          8.18407029 35.36032363 -1.        ]]\n",
      "-17.119999999999976\n",
      "[[ 6.5        26.6        15.         27.67095333 29.01725876 -1.        ]]\n",
      "40.91999999999999\n",
      "[[12.    17.9   17.    11.103 37.462 -1.   ]]\n",
      "-24.319999999999993\n",
      "[[15.9    8.5   17.     3.129 33.028 -1.   ]]\n",
      "-80.91999999999999\n",
      "[[ 3.2        25.6        17.         17.65357794  8.59517795 -1.        ]]\n",
      "60.16\n",
      "[[10.9        14.7        18.         25.71610445 22.21690733 -1.        ]]\n",
      "-27.080000000000013\n",
      "[[22.8        29.5        19.         37.51204112 33.25055444 -1.        ]]\n",
      "-60.639999999999986\n",
      "[[ 7.3        19.9        20.         27.80366842 19.19737931 -1.        ]]\n",
      "14.04000000000002\n",
      "[[23.4   15.5    7.     2.888 49.043 -1.   ]]\n",
      "-109.51999999999998\n",
      "[[16.9        18.6         7.         18.33946957 17.82267624 -1.        ]]\n",
      "-55.400000000000006\n",
      "[[ 6.8        32.4         8.         22.47567097 49.39464551 -1.        ]]\n",
      "57.440000000000055\n",
      "[[15.3        14.4         8.         20.829915   39.27150302 -1.        ]]\n",
      "-57.96000000000001\n",
      "[[16.5        38.8         8.         40.82115521 53.33410967 -1.        ]]\n",
      "11.960000000000036\n",
      "[[ 6.5        25.1        14.         23.19708843 47.21375485 -1.        ]]\n",
      "36.120000000000005\n",
      "[[ 3.6         4.1        14.         17.76562269 46.08340503 -1.        ]]\n",
      "-11.359999999999992\n",
      "[[15.1    6.6   15.     6.147 47.166 -1.   ]]\n",
      "-81.56\n",
      "[[ 3.         14.1        15.          0.20153531 35.77475547 -1.        ]]\n",
      "24.72\n",
      "[[ 6.8        26.4        15.         20.54571501 55.54866727 -1.        ]]\n",
      "38.24000000000004\n",
      "[[22.3        15.9        16.         10.54540482 25.75594241 -1.        ]]\n",
      "-100.75999999999999\n",
      "[[13.2        16.1        17.         18.02229662 30.03355663 -1.        ]]\n",
      "-38.24000000000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23.    15.5   17.     5.306 29.777 -1.   ]]\n",
      "-106.80000000000001\n",
      "[[ 9.4        33.6        17.         30.11756209 15.68449004 -1.        ]]\n",
      "43.60000000000002\n",
      "[[15.9        23.5        18.          1.53029848 22.07014589 -1.        ]]\n",
      "-32.91999999999996\n",
      "[[ 8.2         9.5        18.         11.27117983 32.62045347 -1.        ]]\n",
      "-25.359999999999985\n",
      "[[ 3.2        27.4        18.         26.16760877 53.6429197  -1.        ]]\n",
      "65.92000000000002\n",
      "[[21.8         1.7        19.          5.04666979 53.31900424 -1.        ]]\n",
      "-142.79999999999998\n",
      "[[ 7.3        32.3        19.         31.86163261 33.06401063 -1.        ]]\n",
      "53.72000000000003\n",
      "[[ 3.1         7.2        19.         30.94608356 42.95840411 -1.        ]]\n",
      "1.9599999999999937\n",
      "[[ 8.7         2.4        20.         27.75159711 36.87871569 -1.        ]]\n",
      "-51.47999999999999\n",
      "[[ 3.4        15.6        20.         36.83824123 23.57768618 -1.        ]]\n",
      "26.80000000000001\n",
      "[[15.6        22.3        20.         22.76080342 34.66194475 -1.        ]]\n",
      "-34.71999999999997\n",
      "[[ 4.    11.3   21.     9.156 29.647 -1.   ]]\n",
      "8.959999999999994\n",
      "[[11.8        36.         21.         44.67154002 58.85050753 -1.        ]]\n",
      "34.960000000000036\n",
      "[[11.5        22.5        13.         55.18885768 53.59691033 -1.        ]]\n",
      "-6.199999999999989\n",
      "[[ 2.4        22.7        22.         34.60004375 42.32711933 -1.        ]]\n",
      "56.32000000000002\n",
      "[[ 5.3        30.          6.         59.25892541 29.08757131 -1.        ]]\n",
      "59.960000000000036\n",
      "driver reward  70.32000000000099\n",
      "[[ 2.6         3.8         3.         12.6238031  51.79434115 40.        ]]\n",
      "-5.520000000000003\n",
      "[[ 8.2   11.1    6.     5.261 36.695 39.   ]]\n",
      "-20.23999999999998\n",
      "[[ 5.     8.5    6.     4.114 32.748 38.   ]]\n",
      "-6.799999999999997\n",
      "[[ 2.9         7.5         7.          3.81213295 23.76322052 37.        ]]\n",
      "4.280000000000001\n",
      "[[ 4.1        14.7         7.          7.83980138 12.64070439 36.        ]]\n",
      "19.160000000000025\n",
      "[[ 3.5        34.7         7.         35.73001075 37.15274711 35.        ]]\n",
      "87.24000000000001\n",
      "[[ 4.5         9.4         7.         33.08276579 42.09468128 34.        ]]\n",
      "-0.519999999999996\n",
      "[[11.3   21.8    8.    46.256 56.146 33.   ]]\n",
      "-7.0800000000000125\n",
      "[[10.4        11.1         9.         44.14728525 41.54435049 32.        ]]\n",
      "-35.19999999999999\n",
      "[[ 4.4   29.7   17.    13.087 48.502 31.   ]]\n",
      "65.12\n",
      "[[ 2.9         9.         17.         12.16148807 54.82312786 30.        ]]\n",
      "9.079999999999998\n",
      "[[ 5.6        21.9        17.         25.24574514 34.65358825 29.        ]]\n",
      "32.0\n",
      "[[20.8    7.5   17.     5.4   49.861 28.   ]]\n",
      "-117.44\n",
      "[[ 2.6         9.2        17.         16.74711287 52.83879807 27.        ]]\n",
      "11.760000000000005\n",
      "[[ 8.9        30.5        18.         39.19971551 48.77600349 26.        ]]\n",
      "37.08000000000004\n",
      "[[ 1.7   26.8   19.    15.372 57.926 25.   ]]\n",
      "74.20000000000002\n",
      "[[15.3   30.2   20.    32.032 30.856 24.   ]]\n",
      "-7.399999999999977\n",
      "[[ 9.1        10.5        21.         28.75793152 17.59385826 23.        ]]\n",
      "-28.28\n",
      "[[16.2        14.9         6.         14.58582836 17.13875163 22.        ]]\n",
      "-62.48000000000002\n",
      "[[14.3        19.8         7.          1.34356353 44.22572625 21.        ]]\n",
      "-33.879999999999995\n",
      "[[10.6         9.1         8.          5.39726503 29.96520236 20.        ]]\n",
      "-42.95999999999998\n",
      "[[24.         15.5         8.         18.68944528 32.33062299 19.        ]]\n",
      "-113.59999999999997\n",
      "[[12.7    3.     8.    12.649 41.01  18.   ]]\n",
      "-76.75999999999999\n",
      "[[12.5         9.2         9.          8.98118195 42.06351456 17.        ]]\n",
      "-55.56\n",
      "[[ 6.6         2.5         9.          5.26727863 34.09398494 16.        ]]\n",
      "-36.879999999999995\n",
      "[[ 8.5        11.7         9.         11.66843776 31.29003764 15.        ]]\n",
      "-20.359999999999985\n",
      "[[ 4.8        20.          9.         34.34494057 41.14630568 14.        ]]\n",
      "31.360000000000014\n",
      "[[ 2.6   23.5    9.     9.195 47.695 13.   ]]\n",
      "57.51999999999998\n",
      "[[ 1.8         9.8         9.          0.11176767 42.16258845 12.        ]]\n",
      "19.11999999999999\n",
      "[[ 7.2        20.5         9.         24.20358596 51.39385035 11.        ]]\n",
      "16.640000000000015\n",
      "[[ 6.9        29.5        10.         35.60300647 20.88500505 10.        ]]\n",
      "47.48000000000002\n",
      "[[13.3   29.2   14.     4.417 12.847  9.   ]]\n",
      "3.0\n",
      "[[ 1.6         1.5        15.          4.53872024 12.90418123  8.        ]]\n",
      "-6.079999999999998\n",
      "[[ 7.6   14.5   17.     3.97  34.623  7.   ]]\n",
      "-5.280000000000001\n",
      "[[ 3.6        19.         17.         20.45144457 26.55777118  6.        ]]\n",
      "36.31999999999999\n",
      "[[16.4        21.3        18.          4.28201771 10.20172023  5.        ]]\n",
      "-43.360000000000014\n",
      "[[ 8.1        31.5        18.         34.01109761 20.92443832  4.        ]]\n",
      "45.71999999999997\n",
      "[[10.6   15.    10.    52.029 22.248  3.   ]]\n",
      "-24.080000000000013\n",
      "[[10.3   24.4   17.    29.778 48.857  2.   ]]\n",
      "8.039999999999992\n",
      "[[ 2.4        13.5        17.         18.34343865 54.95528301  1.        ]]\n",
      "1526.88\n",
      "[[ 9.7         9.5        17.          4.78920564 49.09744087  0.        ]]\n",
      "-35.56\n",
      "[[ 3.8        26.6        17.         17.90008267 23.12336051 -1.        ]]\n",
      "59.28\n",
      "[[20.6        16.1        18.         18.81347163 43.48528231 -1.        ]]\n",
      "-88.56\n",
      "[[11.7        18.4        18.         21.13173975 30.77859728 -1.        ]]\n",
      "-20.67999999999998\n",
      "[[ 3.4        22.3        18.          1.65760095 44.22749976 -1.        ]]\n",
      "48.24000000000001\n",
      "[[ 4.6        19.1        18.         22.85873285 34.01538985 -1.        ]]\n",
      "29.839999999999975\n",
      "[[ 5.2        22.4        18.         40.79447547 35.00208273 -1.        ]]\n",
      "36.32000000000002\n",
      "[[17.7        27.3        13.          7.32157623 27.42649399 -1.        ]]\n",
      "-33.0\n",
      "[[15.         13.9        13.         20.24281484 35.67617129 -1.        ]]\n",
      "-57.51999999999998\n",
      "[[25.4        30.1        13.         27.56158541 36.99247428 -1.        ]]\n",
      "-76.39999999999998\n",
      "[[16.8         7.4        16.         43.53210971 18.76205387 -1.        ]]\n",
      "-90.56\n",
      "[[12.6   27.8   21.     5.557 25.329 -1.   ]]\n",
      "3.2800000000000296\n",
      "[[11.1        18.8        22.         22.52888246 30.50206466 -1.        ]]\n",
      "-15.319999999999993\n",
      "[[22.1        18.2         4.          1.99245367 24.54700667 -1.        ]]\n",
      "-92.03999999999996\n",
      "[[ 3.2        24.          5.          5.66587807  1.18949828 -1.        ]]\n",
      "55.04000000000002\n",
      "[[16.9        18.7        11.         23.19749831 13.50919713 -1.        ]]\n",
      "-55.079999999999956\n",
      "[[10.2        32.2        12.         45.2039587  26.52406518 -1.        ]]\n",
      "33.67999999999995\n",
      "[[10.4        33.1        16.         21.85000262 36.5352478  -1.        ]]\n",
      "35.19999999999999\n",
      "[[ 5.5        30.         17.         42.93070092 57.01436968 -1.        ]]\n",
      "58.599999999999994\n",
      "[[ 7.3        20.4        18.         54.00677552 44.09573028 -1.        ]]\n",
      "15.640000000000015\n",
      "[[ 6.3         5.          8.         45.00417111 43.33439079 -1.        ]]\n",
      "-26.840000000000003\n",
      "[[22.    22.6    9.     6.998 52.878 -1.   ]]\n",
      "-77.28000000000003\n",
      "[[ 7.4        35.1        10.         14.26937887 23.4593006  -1.        ]]\n",
      "62.0\n",
      "[[ 4.5        13.7        11.          5.2640531  19.89666105 -1.        ]]\n",
      "13.240000000000009\n",
      "[[ 3.3         7.3        11.         10.61303379 25.08480556 -1.        ]]\n",
      "0.9200000000000017\n",
      "[[ 4.4        17.1        11.          9.963332    6.17306061 -1.        ]]\n",
      "24.80000000000001\n",
      "[[ 5.    20.3   11.     0.681 18.963 -1.   ]]\n",
      "30.960000000000008\n",
      "[[15.8         3.7        12.         13.10040498 33.94536231 -1.        ]]\n",
      "-95.6\n",
      "[[ 8.3        30.3        12.         27.57891769 13.31962639 -1.        ]]\n",
      "40.51999999999998\n",
      "[[20.7        19.4        13.         18.42075222 49.41675761 -1.        ]]\n",
      "-78.67999999999995\n",
      "[[13.6        25.8        14.         38.68822296 49.67357361 -1.        ]]\n",
      "-9.919999999999959\n",
      "[[22.4        26.6        14.         41.25423854 45.73309358 -1.        ]]\n",
      "-67.19999999999999\n",
      "[[16.1   28.5   18.     3.219 38.63  -1.   ]]\n",
      "-18.28000000000003\n",
      "[[ 9.         19.9        18.         17.8819954  39.71414512 -1.        ]]\n",
      "2.480000000000018\n",
      "[[15.8        17.5        18.         18.20802596 50.07147642 -1.        ]]\n",
      "-51.43999999999997\n",
      "[[15.8         4.         18.          0.39512896 45.48813047 -1.        ]]\n",
      "-94.64000000000001\n",
      "[[ 9.4    7.    18.     1.471 37.425 -1.   ]]\n",
      "-41.51999999999998\n",
      "[[ 9.7        24.1        18.          4.89362583 21.10227698 -1.        ]]\n",
      "11.160000000000025\n",
      "[[ 9.4         2.1        19.          5.48554749 30.72554645 -1.        ]]\n",
      "-57.2\n",
      "[[ 1.1        18.9        19.         16.85823717 16.97327008 -1.        ]]\n",
      "53.0\n",
      "[[ 5.8        23.6        19.         33.93292575 38.22310531 -1.        ]]\n",
      "36.079999999999984\n",
      "[[13.    29.2   19.     3.26  49.324 -1.   ]]\n",
      "5.039999999999964\n",
      "[[ 9.5        21.7        19.          2.56044988 18.51350505 -1.        ]]\n",
      "4.840000000000003\n",
      "[[20.2        30.1        19.         11.82410565 10.05499113 -1.        ]]\n",
      "-41.039999999999964\n",
      "[[ 6.8         2.9        20.          2.4881716   8.43287889 -1.        ]]\n",
      "-36.959999999999994\n",
      "[[ 3.2   32.3   20.     1.522 42.14  -1.   ]]\n",
      "81.6\n",
      "[[ 3.6        39.         20.         25.91620984  8.859845   -1.        ]]\n",
      "100.32\n",
      "[[25.3         9.9        22.         13.0717291  28.92810073 -1.        ]]\n",
      "-140.36\n",
      "[[21.9        23.9        22.         34.59834685 38.03791015 -1.        ]]\n",
      "-72.44\n",
      "[[12.3        19.5        23.         30.29425952 26.05775161 -1.        ]]\n",
      "-21.24000000000001\n",
      "[[22.8        17.2         1.         20.70904797 58.43187963 -1.        ]]\n",
      "-100.0\n",
      "[[14.9        15.3         5.         10.03932848 41.72372877 -1.        ]]\n",
      "-52.360000000000014\n",
      "[[ 4.4        22.9         5.         32.57928234 28.93280412 -1.        ]]\n",
      "43.360000000000014\n",
      "[[ 9.9   20.     5.     5.422 19.098 -1.   ]]\n",
      "-3.319999999999993\n",
      "[[14.6        11.8         6.         13.51037806 42.10407796 -1.        ]]\n",
      "-61.51999999999998\n",
      "[[14.8   23.4    6.    25.719 50.031 -1.   ]]\n",
      "-25.75999999999999\n",
      "[[ 8.8         6.6         6.         31.18179912 42.28034276 -1.        ]]\n",
      "-38.72\n",
      "[[ 8.3   16.4    7.    50.346 49.318 -1.   ]]\n",
      "-3.9599999999999795\n",
      "[[ 6.6         8.6         9.         52.64666653 58.10795774 -1.        ]]\n",
      "-17.36\n",
      "driver reward  469.3200000000008\n",
      "[[ 9.4        34.6         5.         41.248075   47.12082522 40.        ]]\n",
      "46.80000000000001\n",
      "[[ 6.1         6.8         8.         49.14067055 48.43708847 39.        ]]\n",
      "-19.719999999999985\n",
      "[[ 1.7        21.3        17.         36.83404257 33.09100974 38.        ]]\n",
      "56.599999999999994\n",
      "[[ 5.2        10.         19.         37.68768502 42.25345591 37.        ]]\n",
      "-3.3599999999999994\n",
      "[[20.          5.8        19.         18.73726537 59.68165463 36.        ]]\n",
      "-117.44\n",
      "[[11.6        18.5        20.         32.33148825 55.9180513  35.        ]]\n",
      "-19.680000000000007\n",
      "[[14.5        14.4        20.         14.61036048 42.28603461 34.        ]]\n",
      "-52.51999999999998\n",
      "[[ 3.2         8.5        20.          8.87697149 50.95232723 33.        ]]\n",
      "5.440000000000012\n",
      "[[ 5.7         8.4        21.         22.32706604 51.27508677 32.        ]]\n",
      "-11.88000000000001\n",
      "[[12.4         9.5         6.         12.0139986  35.52085248 31.        ]]\n",
      "-53.91999999999999\n",
      "[[19.9   24.3    6.    23.585 43.453 30.   ]]\n",
      "-57.56\n",
      "[[11.    25.1    6.    15.64  55.155 29.   ]]\n",
      "5.52000000000001\n",
      "[[15.2        22.          7.          5.16707412 22.05597226 28.        ]]\n",
      "-32.96000000000001\n",
      "[[ 6.9   27.7    7.    28.247 12.105 27.   ]]\n",
      "41.72\n",
      "[[ 2.3         6.2        10.         30.90871582  7.40115464 26.        ]]\n",
      "4.200000000000003\n",
      "[[ 9.9   19.    13.     5.117 20.287 25.   ]]\n",
      "-6.519999999999982\n",
      "[[ 4.3    5.2   13.     4.292 20.879 24.   ]]\n",
      "-12.599999999999994\n",
      "[[ 7.1        13.4        13.         21.07901162 32.27178268 23.        ]]\n",
      "-5.400000000000006\n",
      "[[23.2         4.9        13.          8.03482152 50.50918848 22.        ]]\n",
      "-142.08\n",
      "[[ 6.          4.5        13.          7.30134762 48.63070597 21.        ]]\n",
      "-26.39999999999999\n",
      "[[ 4.4        15.1        13.         20.75484191 36.49662828 20.        ]]\n",
      "18.400000000000006\n",
      "[[19.3        13.7        14.         13.97148948 37.66741154 19.        ]]\n",
      "-87.4\n",
      "[[11.9        34.3        14.         38.86052144 40.24508908 18.        ]]\n",
      "28.840000000000032\n",
      "[[22.9        12.5        16.         31.57589665 49.04738437 17.        ]]\n",
      "-115.71999999999997\n",
      "[[10.1        18.6        16.         11.65184646 35.98998394 16.        ]]\n",
      "-9.160000000000025\n",
      "[[ 9.6         4.5        17.          1.30150952 37.92978707 15.        ]]\n",
      "-50.879999999999995\n",
      "[[12.8        12.         17.          0.97104493 39.02962532 14.        ]]\n",
      "-48.639999999999986\n",
      "[[ 9.9        15.         17.         12.98544236 36.78822302 13.        ]]\n",
      "-19.319999999999993\n",
      "[[ 8.1        26.         17.         19.41993889 20.08217035 12.        ]]\n",
      "28.120000000000005\n",
      "[[17.1        15.1        17.         11.1629556  36.92385525 11.        ]]\n",
      "-67.96000000000001\n",
      "[[ 1.         15.8        17.          5.91839007 52.24542541 10.        ]]\n",
      "43.760000000000005\n",
      "[[ 3.7        12.3        17.         19.98134217 44.79527918  9.        ]]\n",
      "14.200000000000003\n",
      "[[ 9.1         9.7        18.          6.68359592 33.96700833  8.        ]]\n",
      "-30.839999999999975\n",
      "[[ 8.2        20.         18.         24.67544169 44.39671959  7.        ]]\n",
      "8.240000000000009\n",
      "[[13.          6.3        18.          8.59792776 50.0561611   6.        ]]\n",
      "-68.24000000000001\n",
      "[[ 5.5        15.2        18.         18.61208831 49.67521513  5.        ]]\n",
      "11.240000000000009\n",
      "[[12.7        15.         18.         21.02843162 55.14221434  4.        ]]\n",
      "-38.359999999999985\n",
      "[[11.6    2.2   18.     8.199 58.162  3.   ]]\n",
      "-71.84\n",
      "[[ 7.4   14.9   18.     2.931 41.531  2.   ]]\n",
      "-2.640000000000015\n",
      "[[12.9        19.3        18.         20.16869686 38.90122701  1.        ]]\n",
      "1474.04\n",
      "[[ 9.4   14.9   18.    25.531 40.676  0.   ]]\n",
      "-16.24000000000001\n",
      "[[19.9         2.4        18.          9.93060782 56.30191007 -1.        ]]\n",
      "-127.63999999999999\n",
      "[[15.2        10.5        19.         15.59513436 31.25793803 -1.        ]]\n",
      "-69.75999999999999\n",
      "[[ 7.5         8.8        19.          9.7060447  42.71856364 -1.        ]]\n",
      "-22.840000000000003\n",
      "[[12.6        17.2        19.         10.48468389 47.39964993 -1.        ]]\n",
      "-30.639999999999986\n",
      "[[ 9.1        10.8        19.         18.01111858 29.09316814 -1.        ]]\n",
      "-27.319999999999993\n",
      "[[12.4        10.7        19.         16.72239209 20.68772323 -1.        ]]\n",
      "-50.08000000000001\n",
      "[[12.2        19.1        19.         15.04783792 41.78225744 -1.        ]]\n",
      "-21.840000000000003\n",
      "[[ 3.3   27.9   19.    35.644 58.475 -1.   ]]\n",
      "66.84\n",
      "[[24.     8.8    8.    11.444 48.067 -1.   ]]\n",
      "-135.03999999999996\n",
      "[[ 8.8        31.7         8.         32.82368928 36.29096944 -1.        ]]\n",
      "41.60000000000002\n",
      "[[ 8.5   19.4    8.    11.314 35.675 -1.   ]]\n",
      "4.280000000000001\n",
      "[[ 5.8   13.5    8.     6.842 46.853 -1.   ]]\n",
      "3.759999999999991\n",
      "[[12.1   13.5    8.     5.708 45.963 -1.   ]]\n",
      "-39.08000000000001\n",
      "[[ 1.5         4.          8.          3.00635598 49.57321287 -1.        ]]\n",
      "2.6000000000000014\n",
      "[[ 8.5         3.          8.          7.319571   59.08369061 -1.        ]]\n",
      "-48.2\n",
      "[[ 4.3        38.2         9.         37.18003521 28.87240247 -1.        ]]\n",
      "93.0\n",
      "[[11.         12.4         9.         30.87376373 34.79987155 -1.        ]]\n",
      "-35.119999999999976\n",
      "[[ 9.5        23.1         9.         27.41829884 59.24599659 -1.        ]]\n",
      "9.319999999999993\n",
      "[[ 2.         39.3         9.         57.58201791 36.73615711 -1.        ]]\n",
      "112.16000000000003\n",
      "[[ 2.7         8.6        12.         56.87569882 29.91197792 -1.        ]]\n",
      "9.159999999999997\n",
      "[[ 8.8   16.6   13.    38.447 46.213 -1.   ]]\n",
      "-6.719999999999999\n",
      "[[ 5.     8.7   13.    31.126 44.118 -1.   ]]\n",
      "-6.159999999999997\n",
      "[[13.4   21.    17.     4.008 44.026 -1.   ]]\n",
      "-23.919999999999987\n",
      "[[13.2        24.1        17.         10.34681978 33.45109586 -1.        ]]\n",
      "-12.639999999999986\n",
      "[[ 4.9        18.6        17.         26.55863685 46.11690753 -1.        ]]\n",
      "26.200000000000017\n",
      "[[18.9        18.3        18.         25.176221   43.46180637 -1.        ]]\n",
      "-69.96000000000001\n",
      "[[26.4        16.         18.          6.85298113 42.20117795 -1.        ]]\n",
      "-128.32\n",
      "[[ 2.5        23.1        18.         25.48136686 30.15808459 -1.        ]]\n",
      "56.91999999999999\n",
      "[[ 3.1        13.8        18.         40.1767611  32.16260255 -1.        ]]\n",
      "23.079999999999984\n",
      "[[ 7.4        39.9        10.         32.08157979  0.33185881 -1.        ]]\n",
      "77.36000000000001\n",
      "[[18.9         1.4        11.         20.21222665 13.41448355 -1.        ]]\n",
      "-124.03999999999996\n",
      "[[20.4        15.5        12.         16.5436643  33.56438371 -1.        ]]\n",
      "-89.11999999999998\n",
      "[[ 8.8        54.8        12.         58.20860968 13.70057444 -1.        ]]\n",
      "115.52000000000004\n",
      "[[ 2.4   14.2   12.    51.133 26.575 -1.   ]]\n",
      "29.12000000000002\n",
      "driver reward  200.32000000000033\n",
      "[[ 3.2         9.4         7.         36.87496537 16.87851324 40.        ]]\n",
      "8.319999999999993\n",
      "[[10.4        17.1         7.         40.33652606 24.18819465 39.        ]]\n",
      "-16.0\n",
      "[[ 4.8         6.2         8.         41.4055939  22.93309182 38.        ]]\n",
      "-12.799999999999997\n",
      "[[ 8.4   23.9   13.    30.274 42.665 37.   ]]\n",
      "19.360000000000014\n",
      "[[16.2        24.1        15.         22.7610435  28.58184419 36.        ]]\n",
      "-33.039999999999964\n",
      "[[ 7.4    9.7   16.     6.456 33.306 35.   ]]\n",
      "-19.28\n",
      "[[12.1   35.2   16.    16.523  9.147 34.   ]]\n",
      "30.359999999999957\n",
      "[[16.          8.1        17.          4.54450839 28.31060165 33.        ]]\n",
      "-82.88\n",
      "[[10.9        41.         18.         51.67309047 34.2104944  32.        ]]\n",
      "57.08000000000004\n",
      "[[ 5.7    7.2   15.    55.803 26.638 31.   ]]\n",
      "-15.719999999999999\n",
      "[[ 4.    10.1   19.    47.791 20.968 30.   ]]\n",
      "5.1200000000000045\n",
      "[[ 6.3        36.         18.         28.84201996 44.77532661 29.        ]]\n",
      "72.36000000000001\n",
      "[[ 4.    24.7   19.     5.166 55.208 28.   ]]\n",
      "51.84\n",
      "[[ 5.7         7.2        19.         13.34954766 45.94294949 27.        ]]\n",
      "-15.719999999999999\n",
      "[[10.6   11.6   19.     7.172 45.167 26.   ]]\n",
      "-34.95999999999998\n",
      "[[ 6.7    1.3   19.    12.826 44.378 25.   ]]\n",
      "-41.4\n",
      "[[ 5.3   19.6   19.     7.107 59.235 24.   ]]\n",
      "26.67999999999998\n",
      "[[ 4.8        15.5        19.         13.44529701 40.12031149 23.        ]]\n",
      "16.960000000000008\n",
      "[[13.5        11.3        19.          2.70552408 37.936561   22.        ]]\n",
      "-55.639999999999986\n",
      "[[10.1         3.4        19.          8.23521114 29.6703156  21.        ]]\n",
      "-57.8\n",
      "[[15.7        13.9        19.         21.86813999 16.77723439 20.        ]]\n",
      "-62.28\n",
      "[[26.5        24.4        20.         31.20559316 42.1306779  19.        ]]\n",
      "-102.12\n",
      "[[ 9.    31.1   20.     8.473  9.066 18.   ]]\n",
      "38.31999999999999\n",
      "[[13.2         5.1        20.         13.0297524  19.13522925 17.        ]]\n",
      "-73.43999999999998\n",
      "[[ 9.         16.1        20.         28.18350612 28.04781087 16.        ]]\n",
      "-9.680000000000007\n",
      "[[10.5   26.7   21.     2.48  42.506 15.   ]]\n",
      "14.039999999999992\n",
      "[[10.7        20.5        21.         11.47282167 33.74170016 14.        ]]\n",
      "-7.159999999999997\n",
      "[[ 8.8        27.8        21.         20.43756166 17.00546649 13.        ]]\n",
      "29.120000000000005\n",
      "[[20.8        11.2        22.          7.25918845 18.09709261 12.        ]]\n",
      "-105.6\n",
      "[[ 6.8         9.7        23.         11.56157241  2.41415883 11.        ]]\n",
      "-15.200000000000003\n",
      "[[19.5        40.9         5.         26.59621808 54.87116494 10.        ]]\n",
      "-1.7199999999999704\n",
      "[[27.3        39.8         5.         16.61490207  0.45494922  9.        ]]\n",
      "-58.27999999999997\n",
      "[[7.1   6.3   8.    5.641 6.245 8.   ]]\n",
      "-28.11999999999999\n",
      "[[17.6        24.5         8.         35.17512984 25.19570728  7.        ]]\n",
      "-41.28000000000003\n",
      "[[ 4.1        30.7         9.         29.63177788 58.13577635  6.        ]]\n",
      "70.36000000000001\n",
      "[[ 5.1   22.2    9.     4.112 57.354  5.   ]]\n",
      "36.360000000000014\n",
      "[[ 5.4        20.3         9.         26.66935071 50.26989597  4.        ]]\n",
      "28.23999999999998\n",
      "[[12.1    9.4    9.    22.817 41.362  3.   ]]\n",
      "-52.19999999999999\n",
      "[[12.6   27.5    9.     4.228 23.959  2.   ]]\n",
      "2.319999999999993\n",
      "[[ 7.3        17.7        10.         25.19184771 33.23986014  1.        ]]\n",
      "1507.0\n",
      "[[21.         26.8        10.         34.39041703 57.58406008  0.        ]]\n",
      "-57.039999999999964\n",
      "[[13.9        13.5        23.         15.22027901 38.16304297 -1.        ]]\n",
      "-51.31999999999999\n",
      "[[ 6.9        1.2        1.         7.6968896 40.4255663 -1.       ]]\n",
      "-43.08\n",
      "[[ 9.1        33.7         1.         35.53523343 36.9087237  -1.        ]]\n",
      "45.95999999999998\n",
      "[[11.6        22.5         7.         29.91251674 17.81080013 -1.        ]]\n",
      "-6.8799999999999955\n",
      "[[ 4.5        14.9         8.         39.01986303  5.36318522 -1.        ]]\n",
      "17.080000000000013\n",
      "[[ 5.5   48.5   12.     6.978 34.841 -1.   ]]\n",
      "117.80000000000001\n",
      "[[ 2.2         4.9        12.          2.51365272 30.24955478 -1.        ]]\n",
      "0.7199999999999989\n",
      "[[ 3.4         6.8        12.          4.48675878 35.25005716 -1.        ]]\n",
      "-1.3599999999999994\n",
      "[[ 7.9         5.9        12.          7.59218128 21.93709757 -1.        ]]\n",
      "-34.84\n",
      "[[ 4.9         4.6        13.          7.36287019 28.63276646 -1.        ]]\n",
      "-18.599999999999994\n",
      "[[ 3.4        12.7        13.          4.74803339 42.3260907  -1.        ]]\n",
      "17.520000000000024\n",
      "[[ 2.4         8.9        13.          3.64634624 35.52459822 -1.        ]]\n",
      "12.159999999999997\n",
      "[[ 2.9        10.8        13.          3.25780481 27.40790122 -1.        ]]\n",
      "14.83999999999999\n",
      "[[11.5        23.2        13.         28.78666877 42.56548564 -1.        ]]\n",
      "-3.960000000000008\n",
      "[[ 9.2   26.3   14.    11.812 47.039 -1.   ]]\n",
      "21.599999999999994\n",
      "[[10.5        20.9        15.         17.44512165 36.08048958 -1.        ]]\n",
      "-4.519999999999982\n",
      "[[ 8.6        14.3        15.         16.63743692 54.59991928 -1.        ]]\n",
      "-12.719999999999999\n",
      "[[10.2        15.8        15.         12.94872401 46.52112251 -1.        ]]\n",
      "-18.799999999999983\n",
      "[[ 8.5   19.9   15.    26.134 49.999 -1.   ]]\n",
      "5.880000000000024\n",
      "[[ 3.1        36.5        16.         34.59285126 13.27928791 -1.        ]]\n",
      "95.71999999999997\n",
      "[[ 6.8        10.5        17.         45.52096297  5.62047523 -1.        ]]\n",
      "-12.64\n",
      "[[ 5.6   20.1   19.    23.049  2.701 -1.   ]]\n",
      "26.23999999999998\n",
      "[[ 8.4        13.6        19.         17.3550547   3.27980612 -1.        ]]\n",
      "-13.599999999999994\n",
      "[[15.8   14.6   21.    13.511 20.081 -1.   ]]\n",
      "-60.72\n",
      "[[ 3.4         7.4        21.         22.65692866 14.49475742 -1.        ]]\n",
      "0.5600000000000023\n",
      "[[17.1    2.     7.    24.413 33.262 -1.   ]]\n",
      "-109.88\n",
      "[[15.7        21.1         7.         11.96832644 24.11192078 -1.        ]]\n",
      "-39.23999999999998\n",
      "[[14.2        25.          7.         31.08668619 31.017707   -1.        ]]\n",
      "-16.560000000000002\n",
      "[[15.1   11.6    8.    25.139 26.473 -1.   ]]\n",
      "-65.56\n",
      "[[13.4        32.6         8.         17.64645955  4.80681477 -1.        ]]\n",
      "13.199999999999989\n",
      "[[ 5.2         6.5         8.         18.94754656  0.30895127 -1.        ]]\n",
      "-14.559999999999988\n",
      "[[17.3         4.7        10.         13.00580423 12.85317897 -1.        ]]\n",
      "-102.6\n",
      "[[16.1        44.6        11.         49.06420315 25.85070489 -1.        ]]\n",
      "33.24000000000001\n",
      "[[ 4.1        18.8        13.         38.21943642  5.73464012 -1.        ]]\n",
      "32.28\n",
      "driver reward  837.8400000000003\n",
      "[[ 4.8         6.         16.         58.27100692 15.29639366 40.        ]]\n",
      "-13.439999999999998\n",
      "[[ 4.1   15.7   19.    58.155 32.214 39.   ]]\n",
      "22.360000000000014\n",
      "[[17.2   37.7    9.    15.225 22.858 38.   ]]\n",
      "3.67999999999995\n",
      "[[19.         10.2         9.         18.4214744  32.74905941 37.        ]]\n",
      "-96.56\n",
      "[[11.6        23.7         9.         30.84585716 47.5003861  36.        ]]\n",
      "-3.0399999999999636\n",
      "[[ 6.2        18.5         9.         17.30649342 41.6228554  35.        ]]\n",
      "17.04000000000002\n",
      "[[11.7        29.3        10.         36.32473038 45.45038756 34.        ]]\n",
      "14.199999999999989\n",
      "[[ 9.5        26.9        10.         50.56286481 34.70371222 33.        ]]\n",
      "21.480000000000018\n",
      "[[ 9.9   12.9   12.    46.778 47.462 32.   ]]\n",
      "-26.039999999999992\n",
      "[[ 6.7         5.1        18.         55.19313157 41.13139257 31.        ]]\n",
      "-29.24000000000001\n",
      "[[10.6   24.5   13.    20.467 44.344 30.   ]]\n",
      "6.319999999999993\n",
      "[[10.6        15.1        13.         26.17345382 34.96006168 29.        ]]\n",
      "-23.75999999999999\n",
      "[[24.2         4.7        13.          1.85834293 50.54687902 28.        ]]\n",
      "-149.51999999999998\n",
      "[[ 5.5        24.8        13.         14.29011698 27.28444377 27.        ]]\n",
      "41.96000000000001\n",
      "[[23.7         2.         13.          0.42280707 45.22880562 26.        ]]\n",
      "-154.76\n",
      "[[ 9.3         5.7        14.         10.37424472 54.41168401 25.        ]]\n",
      "-45.0\n",
      "[[ 5.7         3.8        14.         12.71429338 56.37764132 24.        ]]\n",
      "-26.599999999999994\n",
      "[[ 7.6        11.8        14.          8.88301411 40.02000141 23.        ]]\n",
      "-13.919999999999987\n",
      "[[ 3.5   33.3   14.    37.412 52.147 22.   ]]\n",
      "82.76000000000002\n",
      "[[12.3   23.9   17.     8.919 38.282 21.   ]]\n",
      "-7.160000000000025\n",
      "[[17.2        11.7        17.         10.07785179 31.66123168 20.        ]]\n",
      "-79.51999999999998\n",
      "[[ 9.3        24.6        17.         23.69564803 18.9162301  19.        ]]\n",
      "15.479999999999961\n",
      "[[ 3.         11.5        18.         35.00554754 11.74460889 18.        ]]\n",
      "16.400000000000006\n",
      "[[20.3   23.5   20.     7.053 44.978 17.   ]]\n",
      "-62.839999999999975\n",
      "[[ 9.5        11.1        20.          9.06034251 29.32765094 16.        ]]\n",
      "-29.080000000000013\n",
      "[[ 3.2        24.5        20.         34.84024713 38.75742604 15.        ]]\n",
      "56.640000000000015\n",
      "[[ 2.1    6.1   20.    30.551 31.769 14.   ]]\n",
      "5.240000000000009\n",
      "[[23.7         6.5        20.         14.34032628 45.67481292 13.        ]]\n",
      "-140.35999999999999\n",
      "[[18.5        42.1        20.         36.74439918 35.36247028 12.        ]]\n",
      "8.920000000000016\n",
      "[[ 5.3   16.5   21.    16.861 44.075 11.   ]]\n",
      "16.75999999999999\n",
      "[[15.9    3.4   21.     2.192 37.177 10.   ]]\n",
      "-97.24000000000001\n",
      "[[ 7.         15.         21.          5.41749903 53.85540111  9.        ]]\n",
      "0.4000000000000057\n",
      "[[ 9.         13.9        22.         21.93524775 43.78657123  8.        ]]\n",
      "-16.72\n",
      "[[20.3         8.5        22.          1.13202917 23.96424307  7.        ]]\n",
      "-110.84\n",
      "[[19.9        35.7         1.         45.58895018 34.20115457  6.        ]]\n",
      "-21.079999999999984\n",
      "[[ 4.6        16.4         7.         53.08819085 46.3218985   5.        ]]\n",
      "21.200000000000017\n",
      "[[16.5    5.3   11.    48.625 60.     4.   ]]\n",
      "-95.24000000000001\n",
      "[[ 6.2        10.7        13.         57.49160892 52.19449471  3.        ]]\n",
      "-7.9199999999999875\n",
      "driver reward  -899.0399999999998\n",
      "[[ 9.4        21.3         5.         20.79900516  4.4955353  40.        ]]\n",
      "4.239999999999981\n",
      "[[10.3        27.1         9.         40.76106336 12.01423723 39.        ]]\n",
      "16.67999999999998\n",
      "[[ 5.4   33.8   13.    11.971 37.294 38.   ]]\n",
      "71.44000000000005\n",
      "[[ 1.7        25.4        13.         28.11151331 59.01755155 37.        ]]\n",
      "69.72000000000003\n",
      "[[ 3.8         5.2        14.         21.35822847 54.57313947 36.        ]]\n",
      "-9.199999999999996\n",
      "[[ 4.2        16.         14.          3.43353056 57.54881884 35.        ]]\n",
      "22.640000000000015\n",
      "[[ 5.1        44.8        14.         29.27607044 20.65753661 34.        ]]\n",
      "108.68\n",
      "[[ 3.1         7.2        19.         27.06216592 24.81308422 33.        ]]\n",
      "1.9599999999999937\n",
      "[[15.         12.         20.         24.74668392  6.44716912 32.        ]]\n",
      "-63.599999999999994\n",
      "[[14.3        26.         20.          0.9335949  38.83810186 31.        ]]\n",
      "-14.039999999999964\n",
      "[[18.1        22.8        22.         17.4764908   1.46538213 30.        ]]\n",
      "-50.120000000000005\n",
      "[[ 2.1        14.6        12.          6.42491219 13.66646657 29.        ]]\n",
      "32.44000000000001\n",
      "[[ 8.3        31.         13.         22.72472677 30.30295807 28.        ]]\n",
      "42.76000000000005\n",
      "[[24.6        17.5        13.         16.39066504 34.38223289 27.        ]]\n",
      "-111.28000000000003\n",
      "[[ 7.3         2.4        13.         15.96448014 26.02381595 26.        ]]\n",
      "-41.959999999999994\n",
      "[[10.7         6.6        13.          9.17716444 23.72546749 25.        ]]\n",
      "-51.63999999999997\n",
      "[[ 6.8        31.3        13.         31.95899423 53.35928457 24.        ]]\n",
      "53.920000000000016\n",
      "[[ 9.4   28.4   14.     6.972 29.639 23.   ]]\n",
      "26.960000000000036\n",
      "[[ 7.1        15.9        14.         14.65519644 24.17572802 22.        ]]\n",
      "2.5999999999999943\n",
      "[[ 8.         26.5        15.          9.4254992  48.05681793 21.        ]]\n",
      "30.400000000000006\n",
      "[[ 1.6         3.6        15.          7.74092748 50.32636721 20.        ]]\n",
      "0.6400000000000006\n",
      "[[ 6.4         7.7        15.         12.44374318 38.5294993  19.        ]]\n",
      "-18.88000000000001\n",
      "[[ 7.8        12.3        16.          6.42029857 38.9734363  18.        ]]\n",
      "-13.680000000000007\n",
      "[[10.         52.6        16.         30.9455729   3.82910153 17.        ]]\n",
      "100.32\n",
      "[[25.         13.1        19.         19.21118168  8.55347859 16.        ]]\n",
      "-128.07999999999998\n",
      "[[22.9        10.3        19.          4.87118079 14.60556322 15.        ]]\n",
      "-122.76000000000002\n",
      "[[ 7.9         9.         20.          9.63377125 28.93711842 14.        ]]\n",
      "-24.919999999999987\n",
      "[[14.6        13.9        20.         15.8994795  41.92495867 13.        ]]\n",
      "-54.79999999999998\n",
      "[[ 3.         15.5        20.         23.01418947 54.9044871  12.        ]]\n",
      "29.200000000000003\n",
      "[[25.5        13.6        20.         16.77820099 37.86805979 11.        ]]\n",
      "-129.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.6        29.3        21.         24.93313131 54.74913979 10.        ]]\n",
      "1.2800000000000296\n",
      "[[15.3         9.         21.         21.3145612  48.77226337  9.        ]]\n",
      "-75.24000000000001\n",
      "[[ 5.1    6.4   21.    17.406 44.094  8.   ]]\n",
      "-14.200000000000003\n",
      "[[15.6        23.         21.         23.01707333 38.87531366  7.        ]]\n",
      "-32.48000000000002\n",
      "[[10.5   10.6   22.     5.216 28.622  6.   ]]\n",
      "-37.48000000000002\n",
      "[[ 2.2        29.3        22.         13.95536725 54.30207637  5.        ]]\n",
      "78.80000000000001\n",
      "[[19.4    4.7    0.     5.76  36.104  4.   ]]\n",
      "-116.87999999999997\n",
      "[[10.         16.1         1.         14.52557755 33.09602977  3.        ]]\n",
      "-16.480000000000018\n",
      "[[ 6.8        39.4         4.         38.87912585 11.41334824  2.        ]]\n",
      "79.84000000000003\n",
      "[[ 8.9   7.5   9.   25.73 10.2   1.  ]]\n",
      "1463.48\n",
      "[[ 9.    13.2    9.    26.144 32.332  0.   ]]\n",
      "-18.95999999999998\n",
      "[[25.9        51.         10.         35.38273742  7.47787005 -1.        ]]\n",
      "-12.920000000000073\n",
      "[[ 6.2   39.8   17.     8.09  43.709 -1.   ]]\n",
      "85.19999999999999\n",
      "[[ 3.5        23.2        18.          1.2889451  20.45783268 -1.        ]]\n",
      "50.44\n",
      "[[ 8.7        37.1        18.         11.62460716 49.16300612 -1.        ]]\n",
      "59.56\n",
      "[[ 7.4        26.2        18.          6.90677661 22.5473652  -1.        ]]\n",
      "33.52000000000001\n",
      "[[ 2.8        13.8        18.         15.12578836  8.33998062 -1.        ]]\n",
      "25.11999999999999\n",
      "[[ 5.8    8.    19.     4.046 15.382 -1.   ]]\n",
      "-13.840000000000003\n",
      "[[ 6.9    9.2   19.     4.584 29.104 -1.   ]]\n",
      "-17.480000000000004\n",
      "[[11.8         4.8        19.          6.23204121 14.65973932 -1.        ]]\n",
      "-64.88000000000001\n",
      "[[ 4.8        17.         19.         18.89072573  7.35208348 -1.        ]]\n",
      "21.75999999999999\n",
      "[[20.7        13.6        20.         13.74111441 12.33556985 -1.        ]]\n",
      "-97.23999999999998\n",
      "[[14.3        27.6        20.         12.08251622 52.48986832 -1.        ]]\n",
      "-8.920000000000016\n",
      "[[16.2        40.4        21.         22.38063099  4.28811491 -1.        ]]\n",
      "19.12000000000006\n",
      "[[19.          3.2        21.          2.49551938  0.66068255 -1.        ]]\n",
      "-118.95999999999998\n",
      "[[13.2        14.9        10.         13.67364673 26.38142262 -1.        ]]\n",
      "-42.08000000000001\n",
      "[[10.9         6.7        11.         10.40486157 34.31109724 -1.        ]]\n",
      "-52.68000000000001\n",
      "[[ 5.4         9.9        11.          0.47315654 45.77862596 -1.        ]]\n",
      "-5.040000000000006\n",
      "[[ 9.6        28.5        11.         33.41169245 59.58376383 -1.        ]]\n",
      "25.920000000000016\n",
      "[[ 3.3   14.    11.    37.496 43.528 -1.   ]]\n",
      "22.36\n",
      "[[ 6.5        22.2        12.         54.42074085 49.94125131 -1.        ]]\n",
      "26.840000000000003\n",
      "[[ 9.4   16.7   16.    42.24  27.415 -1.   ]]\n",
      "-10.480000000000018\n",
      "[[11.9        10.3        17.         48.00142513 44.30831474 -1.        ]]\n",
      "-47.96000000000001\n",
      "[[13.9        18.9        18.         51.43557208 58.44204465 -1.        ]]\n",
      "-34.039999999999964\n",
      "[[ 1.6        18.2         8.         41.46204369 44.43882672 -1.        ]]\n",
      "47.359999999999985\n",
      "[[23.8   18.8    9.     2.739 60.    -1.   ]]\n",
      "-101.68\n",
      "[[13.6        18.6        10.         20.2097489  50.05715009 -1.        ]]\n",
      "-32.96000000000001\n",
      "[[14.1         3.7        10.          6.54121266 55.0833482  -1.        ]]\n",
      "-84.04\n",
      "[[11.4        13.4        11.         16.44003831 39.87264897 -1.        ]]\n",
      "-34.639999999999986\n",
      "[[ 8.3        40.9        11.         54.34555392 24.14643958 -1.        ]]\n",
      "74.44\n",
      "[[ 5.4        36.8        13.         22.70551149 34.58800117 -1.        ]]\n",
      "81.04000000000002\n",
      "[[ 6.2        23.         13.         36.22628878 48.28162774 -1.        ]]\n",
      "31.439999999999998\n",
      "[[15.4        26.         14.         22.963737   57.27537261 -1.        ]]\n",
      "-21.519999999999982\n",
      "[[10.1        19.8        14.         36.1984784  51.44381343 -1.        ]]\n",
      "-5.319999999999993\n",
      "[[ 1.6   25.    14.    59.041 46.098 -1.   ]]\n",
      "69.12\n",
      "driver reward  958.0000000000003\n",
      "[[11.7        27.8         6.          2.6259339  46.03700771 40.        ]]\n",
      "9.400000000000034\n",
      "[[ 7.4        15.2         7.         15.31257671 37.33213476 39.        ]]\n",
      "-1.6800000000000068\n",
      "[[ 4.8   26.9    8.    46.788 36.342 38.   ]]\n",
      "53.44\n",
      "[[18.8   23.     7.     5.051 38.697 37.   ]]\n",
      "-54.23999999999995\n",
      "[[ 4.         33.1         7.         33.02741276 25.36579018 36.        ]]\n",
      "78.72\n",
      "[[ 4.7        25.          7.          6.54496265 17.83869375 35.        ]]\n",
      "48.04000000000002\n",
      "[[ 6.9        35.5         7.         33.61820722 28.80433755 34.        ]]\n",
      "66.68\n",
      "[[ 4.3   24.1    7.    45.634 47.017 33.   ]]\n",
      "47.879999999999995\n",
      "[[ 7.5        32.9        15.         44.92074392 17.67797698 32.        ]]\n",
      "54.28000000000003\n",
      "[[ 3.7        19.1        20.         58.99065376 27.66027517 31.        ]]\n",
      "35.96000000000001\n",
      "[[16.8        13.6        11.         59.57839078 12.08260202 30.        ]]\n",
      "-70.72\n",
      "[[ 5.         19.2        14.         43.75712195 30.3418571  29.        ]]\n",
      "27.439999999999998\n",
      "[[ 7.4         7.2        15.         44.21809066 43.53026196 28.        ]]\n",
      "-27.28\n",
      "[[ 5.9    6.8   16.    32.528 41.229 27.   ]]\n",
      "-18.36\n",
      "[[19.3        33.5        17.         42.04958888 30.61750699 26.        ]]\n",
      "-24.039999999999964\n",
      "[[ 4.9        44.1         5.          2.85048439 50.34350321 25.        ]]\n",
      "107.80000000000001\n",
      "[[14.2         7.2         6.         18.39356715 41.36867357 24.        ]]\n",
      "-73.51999999999998\n",
      "[[ 5.8        11.9         7.         28.88780706 51.87654715 23.        ]]\n",
      "-1.3599999999999852\n",
      "[[22.          8.          7.          5.06790225 40.07623195 22.        ]]\n",
      "-124.0\n",
      "[[ 7.2        19.1         7.         19.94689437 54.42328066 21.        ]]\n",
      "12.159999999999997\n",
      "[[14.2        35.7         7.         36.59518461 26.38574748 20.        ]]\n",
      "17.67999999999995\n",
      "[[23.          4.1         9.         16.93509653 44.25121228 19.        ]]\n",
      "-143.28\n",
      "[[ 9.2         6.6         9.          2.77365239 46.48289443 18.        ]]\n",
      "-41.43999999999998\n",
      "[[ 5.         17.4         9.         17.30178507 33.08382306 17.        ]]\n",
      "21.680000000000007\n",
      "[[15.9        32.7         9.         26.53362073 47.64307377 16.        ]]\n",
      "-3.480000000000018\n",
      "[[ 3.9        12.7         9.         28.25725487 37.57248425 15.        ]]\n",
      "14.120000000000019\n",
      "[[ 7.1        12.7        10.         12.79972718 46.88524486 14.        ]]\n",
      "-7.639999999999986\n",
      "[[ 6.5        37.2        10.         45.29117475 41.37001345 13.        ]]\n",
      "74.83999999999997\n",
      "[[ 6.4        11.6        12.         39.99883178 35.88691211 12.        ]]\n",
      "-6.3999999999999915\n",
      "[[ 3.4   10.1   15.    51.622 29.306 11.   ]]\n",
      "9.200000000000003\n",
      "[[ 3.9        18.9        21.         29.57438835 34.70017446 10.        ]]\n",
      "33.960000000000036\n",
      "[[ 6.5        15.9        21.         30.35669491 24.35556912  9.        ]]\n",
      "6.680000000000007\n",
      "[[ 7.2        13.1        22.         15.74230957 26.88340386  8.        ]]\n",
      "-7.039999999999992\n",
      "[[21.         19.2         0.          2.79460379 24.4482283   7.        ]]\n",
      "-81.36000000000001\n",
      "[[ 3.3        29.4         3.         33.31051275 14.59722801  6.        ]]\n",
      "71.64000000000004\n",
      "[[ 7.8   46.4    8.    22.352 56.874  5.   ]]\n",
      "95.44000000000005\n",
      "[[ 7.6        16.4         8.         18.04337359 36.58571338  4.        ]]\n",
      "0.8000000000000114\n",
      "[[14.6         7.7         8.          4.78664111 28.7628494   3.        ]]\n",
      "-74.64000000000001\n",
      "[[ 2.3         8.9         8.          0.82930487 20.27884421  2.        ]]\n",
      "12.840000000000003\n",
      "[[14.5         9.          9.          9.49465826 19.09030108  1.        ]]\n",
      "1430.2\n",
      "[[11.7         5.8         9.          8.83814241 31.52882952  0.        ]]\n",
      "-61.0\n",
      "[[ 5.4         2.          9.          4.64659307 25.42187925 -1.        ]]\n",
      "-30.32\n",
      "[[ 3.1        13.4         9.         19.78222743 20.32272765 -1.        ]]\n",
      "21.799999999999997\n",
      "[[ 8.8        17.9         9.         11.51604903 45.1316712  -1.        ]]\n",
      "-2.5600000000000023\n",
      "[[10.1        11.2         9.          6.28435573 29.92746447 -1.        ]]\n",
      "-32.839999999999975\n",
      "[[ 4.4        15.8         9.         18.85436229 23.78231818 -1.        ]]\n",
      "20.639999999999986\n",
      "[[16.         33.9        10.         34.03106873 13.04908952 -1.        ]]\n",
      "-0.3199999999999932\n",
      "[[10.3        24.8        13.          9.53465684 38.07272347 -1.        ]]\n",
      "9.319999999999993\n",
      "[[ 7.          9.5        13.          0.49637989 35.59395734 -1.        ]]\n",
      "-17.200000000000003\n",
      "[[ 9.5        13.2        13.          7.51354778 14.73500825 -1.        ]]\n",
      "-22.359999999999985\n",
      "[[ 3.1        24.4        13.         25.99645707 25.46245085 -1.        ]]\n",
      "57.0\n",
      "[[18.1    6.2   13.     3.978 34.774 -1.   ]]\n",
      "-103.24000000000001\n",
      "[[ 4.7        20.2        13.          7.61599263 10.39986823 -1.        ]]\n",
      "32.68000000000001\n",
      "[[ 8.5        16.9        13.         19.62018585 29.48792989 -1.        ]]\n",
      "-3.719999999999999\n",
      "[[13.3   22.5   14.     3.542 24.479 -1.   ]]\n",
      "-18.43999999999997\n",
      "[[ 5.4        29.9        14.         33.7435948  42.77358426 -1.        ]]\n",
      "58.960000000000036\n",
      "[[18.3        11.         14.          9.20537199 56.59156803 -1.        ]]\n",
      "-89.24000000000001\n",
      "[[18.3        12.3        14.          2.70578637 27.12997822 -1.        ]]\n",
      "-85.08000000000001\n",
      "[[26.8        12.9        14.          2.23705923 49.3130671  -1.        ]]\n",
      "-140.96000000000004\n",
      "[[16.2        20.2        14.          8.12421526 40.75125292 -1.        ]]\n",
      "-45.51999999999998\n",
      "[[ 8.8   16.8   15.     3.496 57.635 -1.   ]]\n",
      "-6.0800000000000125\n",
      "[[15.3        13.3        15.          5.60542015 56.85088265 -1.        ]]\n",
      "-61.48000000000002\n",
      "[[10.7        29.4        15.         15.37402636 18.75702423 -1.        ]]\n",
      "21.32000000000005\n",
      "[[12.3        14.5        15.         19.24071291 12.99960621 -1.        ]]\n",
      "-37.24000000000001\n",
      "[[14.         18.3        15.         23.61473604 11.62602742 -1.        ]]\n",
      "-36.639999999999986\n",
      "[[17.4        45.4        15.         46.22525968 30.96723255 -1.        ]]\n",
      "26.960000000000036\n",
      "driver reward  1024.8400000000006\n",
      "[[ 4.2        20.2         7.         13.2054256   3.13802804 40.        ]]\n",
      "36.08000000000001\n",
      "[[ 8.9   16.2    9.    13.513 26.325 39.   ]]\n",
      "-8.680000000000007\n",
      "[[10.6         8.1         9.         19.49715652 10.38729145 38.        ]]\n",
      "-46.16\n",
      "[[10.          6.2         9.         17.3429296  14.17076989 37.        ]]\n",
      "-48.16\n",
      "[[ 3.6   11.2    9.     9.295 18.257 36.   ]]\n",
      "11.360000000000014\n",
      "[[ 4.8         2.9        10.         10.04333958 20.28163005 35.        ]]\n",
      "-23.359999999999992\n",
      "[[11.1        16.6        11.         28.56133868 27.46208233 34.        ]]\n",
      "-22.360000000000014\n",
      "[[ 5.2   30.6   13.    13.716 48.896 33.   ]]\n",
      "62.559999999999974\n",
      "[[ 3.9        27.8        13.         28.73449261 21.29952597 32.        ]]\n",
      "62.44\n",
      "[[13.1         5.         16.         21.43948025 24.88132309 31.        ]]\n",
      "-73.08000000000001\n",
      "[[ 4.9        25.2        17.         40.16808913 42.6739857  30.        ]]\n",
      "47.31999999999999\n",
      "[[ 9.3        15.6        20.         39.36393082 58.65327186 29.        ]]\n",
      "-13.319999999999993\n",
      "[[11.9        17.1        21.         21.12669379 36.38147357 28.        ]]\n",
      "-26.19999999999999\n",
      "[[ 4.7        24.7        21.         37.84255965 20.18041346 27.        ]]\n",
      "47.08000000000001\n",
      "[[20.         16.1        10.         19.29602509 16.58780432 26.        ]]\n",
      "-84.47999999999999\n",
      "[[12.7    2.9   11.    18.561 28.995 25.   ]]\n",
      "-77.08\n",
      "[[ 6.3        28.5        12.         13.42668841  4.82154484 24.        ]]\n",
      "48.360000000000014\n",
      "[[ 9.8   16.9   14.     3.652 29.461 23.   ]]\n",
      "-12.560000000000002\n",
      "[[ 1.9    5.2   14.     7.847 31.996 22.   ]]\n",
      "3.720000000000006\n",
      "[[ 5.4        14.2        14.         19.12314282 33.11091093 21.        ]]\n",
      "8.719999999999999\n",
      "[[13.2         9.6        14.         15.01451193 34.69667439 20.        ]]\n",
      "-59.039999999999964\n",
      "[[ 3.7        35.3        14.         35.42230336  4.3771728  19.        ]]\n",
      "87.80000000000001\n",
      "[[15.9        21.         23.         32.68345612 37.61073862 18.        ]]\n",
      "-40.91999999999999\n",
      "[[23.7         5.5         6.          3.61681047 38.6933706  17.        ]]\n",
      "-143.56\n",
      "[[ 6.5        31.          6.         27.23833128 52.85943519 16.        ]]\n",
      "55.0\n",
      "[[ 3.6   27.9    7.     8.228 34.911 15.   ]]\n",
      "64.80000000000001\n",
      "[[ 6.          7.7         7.          8.76362199 48.31157458 14.        ]]\n",
      "-16.159999999999997\n",
      "[[ 4.9        11.6         7.          9.06136295 36.37713047 13.        ]]\n",
      "3.799999999999997\n",
      "[[10.3        29.2         7.         12.96099258 17.09013371 12.        ]]\n",
      "23.400000000000034\n",
      "[[14.2        11.8         8.          0.12361164 18.26347297 11.        ]]\n",
      "-58.79999999999998\n",
      "[[16.9         8.5         8.          0.86548644 37.77315033 10.        ]]\n",
      "-87.72\n",
      "[[ 4.4        20.9         8.          3.61177312 17.9831518   9.        ]]\n",
      "36.960000000000036\n",
      "[[11.1         4.1         9.          5.45373709 32.87658931  8.        ]]\n",
      "-62.36\n",
      "[[ 8.         22.6         9.         18.30849528 46.22739142  7.        ]]\n",
      "17.919999999999987\n",
      "[[12.6         3.6         9.          9.77762863 50.61073454  6.        ]]\n",
      "-74.16\n",
      "[[ 9.8        14.4         9.         12.5994121  32.58713056  5.        ]]\n",
      "-20.560000000000002\n",
      "[[ 4.5        29.6         9.         17.15281266  2.82333386  4.        ]]\n",
      "64.12\n",
      "[[19.4        12.5        11.          5.08325214  7.49035879  3.        ]]\n",
      "-91.91999999999999\n",
      "[[ 3.8    3.4   12.    11.08  11.502  2.   ]]\n",
      "-14.959999999999994\n",
      "[[ 6.5        14.5        12.          9.41071072 30.42539832  1.        ]]\n",
      "1502.2\n",
      "[[ 5.1        12.3        12.          3.72049468 46.87904781  0.        ]]\n",
      "4.680000000000007\n",
      "[[ 6.9        29.5        12.         20.9402229  21.88480005 -1.        ]]\n",
      "47.48000000000002\n",
      "[[23.9         1.9        12.          3.04687635 36.05469332 -1.        ]]\n",
      "-156.43999999999997\n",
      "[[ 6.3         6.3        12.          9.74941536 26.7803121  -1.        ]]\n",
      "-22.679999999999993\n",
      "[[17.9        23.8        12.         15.66269077 22.40796595 -1.        ]]\n",
      "-45.56\n",
      "[[13.          9.7        12.         14.04152713 33.10378737 -1.        ]]\n",
      "-57.359999999999985\n",
      "[[ 7.9         5.7        12.         11.29990391 43.02599205 -1.        ]]\n",
      "-35.480000000000004\n",
      "[[ 3.9        17.4        12.         16.83741571 57.35787696 -1.        ]]\n",
      "29.160000000000025\n",
      "[[16.1        41.2        13.         44.01133265 27.79551495 -1.        ]]\n",
      "22.359999999999957\n",
      "[[ 4.2   36.4   13.     6.405 26.516 -1.   ]]\n",
      "87.92000000000002\n",
      "[[ 3.1       18.5       13.         7.986373  48.0652438 -1.       ]]\n",
      "38.120000000000005\n",
      "[[ 7.2        11.5        13.          7.23078505 59.51799135 -1.        ]]\n",
      "-12.159999999999997\n",
      "[[ 5.9         5.         13.          8.66952852 52.26285085 -1.        ]]\n",
      "-24.120000000000005\n",
      "[[ 6.2         7.4        13.          7.78017143 55.22494643 -1.        ]]\n",
      "-18.480000000000004\n",
      "[[ 6.4         5.         14.          5.51816925 55.89882214 -1.        ]]\n",
      "-27.519999999999996\n",
      "[[17.5         9.8        14.          3.35928462 48.52763376 -1.        ]]\n",
      "-87.63999999999999\n",
      "[[ 4.4        13.7        14.         11.31882928 37.07659836 -1.        ]]\n",
      "13.919999999999987\n",
      "[[10.1        10.9        15.          1.15813659 39.64026664 -1.        ]]\n",
      "-33.79999999999998\n",
      "[[ 9.2        43.2        15.         31.2049037   9.02050246 -1.        ]]\n",
      "75.67999999999995\n",
      "[[16.2        12.8        16.          8.55088674  3.3756512  -1.        ]]\n",
      "-69.19999999999999\n",
      "[[13.8         9.8        17.          0.92904921 25.2608306  -1.        ]]\n",
      "-62.48000000000002\n",
      "[[ 5.1         4.5        17.          5.88727097 31.97211895 -1.        ]]\n",
      "-20.28\n",
      "[[14.2         6.1        17.          1.21005356 42.80973511 -1.        ]]\n",
      "-77.03999999999996\n",
      "[[ 5.5        18.6        17.         12.28509375 22.70444026 -1.        ]]\n",
      "22.120000000000005\n",
      "[[11.3        25.3        17.         21.86533616 44.42600696 -1.        ]]\n",
      "4.1200000000000045\n",
      "[[ 7.9        18.1        17.         44.04745671 41.97971348 -1.        ]]\n",
      "4.200000000000017\n",
      "[[25.7        24.9        11.         51.02521602 25.83693058 -1.        ]]\n",
      "-95.07999999999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.    24.2    7.    56.992 38.13  -1.   ]]\n",
      "-17.75999999999999\n",
      "[[13.1        24.3        12.         45.55839478  5.26736456 -1.        ]]\n",
      "-11.319999999999993\n",
      "[[ 6.6   32.4   13.    19.446 26.11  -1.   ]]\n",
      "58.80000000000001\n",
      "[[ 6.5         8.9        13.         20.87080638 17.06551972 -1.        ]]\n",
      "-15.719999999999999\n",
      "[[15.6   12.2   14.     5.559 40.224 -1.   ]]\n",
      "-67.03999999999996\n",
      "[[ 2.         13.         14.         17.60369695 37.63954206 -1.        ]]\n",
      "28.0\n",
      "[[ 5.1        14.4        15.         16.16256746 28.10534584 -1.        ]]\n",
      "11.400000000000006\n",
      "[[11.8         6.7        16.          4.80694734 23.38686778 -1.        ]]\n",
      "-58.8\n",
      "[[ 1.2        27.         17.         16.32341857 46.52772534 -1.        ]]\n",
      "78.24000000000001\n",
      "[[11.9        29.         17.          6.35247654 12.10190869 -1.        ]]\n",
      "11.879999999999995\n",
      "[[ 3.3        14.4        17.         20.71089772  7.96803917 -1.        ]]\n",
      "23.640000000000015\n",
      "[[12.7         9.2        17.         11.39879613  4.51428055 -1.        ]]\n",
      "-56.91999999999999\n",
      "[[15.7        22.4        18.         24.35355251 25.28580113 -1.        ]]\n",
      "-35.07999999999993\n",
      "[[25.3        14.5        18.         18.01204661 47.56813498 -1.        ]]\n",
      "-125.63999999999999\n",
      "[[24.2   27.7   19.     5.04  57.985 -1.   ]]\n",
      "-75.91999999999996\n",
      "[[ 8.2        24.1        19.         19.32959726 29.88826073 -1.        ]]\n",
      "21.360000000000014\n",
      "[[23.2        17.8        19.          5.26707812 28.38125923 -1.        ]]\n",
      "-100.80000000000001\n",
      "[[ 1.1        25.         19.         27.715318   41.33937079 -1.        ]]\n",
      "72.51999999999998\n",
      "[[ 5.8         9.7        19.         29.51153097 44.90388967 -1.        ]]\n",
      "-8.399999999999991\n",
      "[[ 6.2        12.3        19.         25.35100798 38.26162336 -1.        ]]\n",
      "-2.799999999999997\n",
      "[[13.1         7.8        19.          4.86061206 36.56313126 -1.        ]]\n",
      "-64.11999999999998\n",
      "[[ 8.3    9.1   19.    11.168 27.818 -1.   ]]\n",
      "-27.319999999999993\n",
      "[[ 5.5        46.6        19.         53.46145356 42.78179706 -1.        ]]\n",
      "111.72000000000003\n",
      "[[ 2.7        30.         11.         46.44349516 14.4931565  -1.        ]]\n",
      "77.63999999999999\n",
      "[[11.8   15.5    6.    23.75  19.099 -1.   ]]\n",
      "-30.639999999999986\n",
      "[[15.          4.6         6.          8.02752254 30.78563142 -1.        ]]\n",
      "-87.28\n",
      "[[11.5         5.          7.         16.92844452 32.79911604 -1.        ]]\n",
      "-62.2\n",
      "[[10.        11.         7.         6.7480646 50.195211  -1.       ]]\n",
      "-32.79999999999998\n",
      "[[ 7.6    5.3    7.    10.161 53.634 -1.   ]]\n",
      "-34.719999999999985\n",
      "[[10.         16.2         7.         32.53035203 46.43062269 -1.        ]]\n",
      "-16.159999999999997\n",
      "[[18.5         6.8         7.         19.9352807  49.26415603 -1.        ]]\n",
      "-104.03999999999999\n",
      "[[15.7        15.8         7.         19.3939779  46.69730779 -1.        ]]\n",
      "-56.19999999999999\n",
      "[[18.2        15.1         8.         16.06560349 52.63978784 -1.        ]]\n",
      "-75.43999999999997\n",
      "[[ 6.1         3.3         8.         17.31426115 47.89924407 -1.        ]]\n",
      "-30.919999999999987\n",
      "[[ 1.    12.4    8.     5.536 45.558 -1.   ]]\n",
      "32.879999999999995\n",
      "[[ 4.1         6.7         8.          2.21308373 41.50864315 -1.        ]]\n",
      "-6.439999999999998\n",
      "[[ 6.8        12.9         8.         10.78913087 37.41255019 -1.        ]]\n",
      "-4.9599999999999795\n",
      "[[ 4.6        35.1         8.         37.87768335  9.43359324 -1.        ]]\n",
      "81.03999999999996\n",
      "[[17.         25.9        12.         47.83921897 12.30859285 -1.        ]]\n",
      "-32.71999999999997\n",
      "[[ 6.5   13.8   17.    34.577 27.654 -1.   ]]\n",
      "-0.03999999999999204\n",
      "[[ 5.2        16.2        18.         33.40154373  6.30307042 -1.        ]]\n",
      "16.480000000000018\n",
      "driver reward  -34.11999999999918\n",
      "[[ 2.3        15.2         1.         24.93903956 31.75784035 40.        ]]\n",
      "33.0\n",
      "[[11.4        24.9         9.          5.53389523  1.21414998 39.        ]]\n",
      "2.160000000000025\n",
      "[[10.1        12.4        11.         26.63730074  8.17680632 38.        ]]\n",
      "-29.0\n",
      "[[ 9.2   12.3   12.    12.929 19.077 37.   ]]\n",
      "-23.19999999999999\n",
      "[[ 6.9   20.9   12.    12.637 33.9   36.   ]]\n",
      "19.960000000000036\n",
      "[[ 3.5        43.9        12.         54.62687566 32.67496865 35.        ]]\n",
      "116.68\n",
      "[[14.6   39.1   19.    12.053 27.627 34.   ]]\n",
      "25.839999999999975\n",
      "[[ 9.3        11.9        19.          7.72174509 23.67336309 33.        ]]\n",
      "-25.160000000000025\n",
      "[[ 1.8    5.8   19.     9.458 17.431 32.   ]]\n",
      "6.32\n",
      "[[17.9        11.5        19.         15.02745179 27.87327373 31.        ]]\n",
      "-84.91999999999999\n",
      "[[ 7.4        28.3        19.         25.30457287 50.06462159 30.        ]]\n",
      "40.23999999999998\n",
      "[[22.8        29.7        19.         26.82644786 58.98241215 29.        ]]\n",
      "-60.0\n",
      "[[17.6        15.5        20.          0.58636765 47.76906272 28.        ]]\n",
      "-70.08000000000001\n",
      "[[12.1   23.9   21.    17.926 40.207 27.   ]]\n",
      "-5.799999999999983\n",
      "[[ 3.7        19.5        21.         36.13902289 37.16938482 26.        ]]\n",
      "37.24000000000001\n",
      "[[ 9.2        18.5         1.         41.39348997 54.81086631 25.        ]]\n",
      "-3.359999999999985\n",
      "[[ 2.8        12.          3.         46.20008683 46.38487765 24.        ]]\n",
      "19.36\n",
      "[[ 5.4   46.3    9.    11.345 18.448 23.   ]]\n",
      "111.44000000000005\n",
      "[[ 2.2   23.2   11.     4.959 43.    22.   ]]\n",
      "59.28\n",
      "[[11.4        36.7        11.         50.54228694 46.16620187 21.        ]]\n",
      "39.920000000000016\n",
      "[[ 9.9         4.1        16.         44.50996931 53.9347686  20.        ]]\n",
      "-54.2\n",
      "[[12.3        23.         20.         56.66641629 22.1243296  19.        ]]\n",
      "-10.039999999999964\n",
      "[[ 2.6   22.7    9.    41.319 35.606 18.   ]]\n",
      "54.96000000000001\n",
      "[[10.9   26.2   11.    42.095 18.17  17.   ]]\n",
      "9.719999999999999\n",
      "[[ 4.8        15.1         6.         22.95002911 23.03372716 16.        ]]\n",
      "15.680000000000007\n",
      "[[23.7        32.6         7.         30.48863601 17.90656105 15.        ]]\n",
      "-56.839999999999975\n",
      "[[26.4        27.8         8.         40.5220701  37.57883793 14.        ]]\n",
      "-90.56\n",
      "[[ 4.    28.2   13.    12.96  31.568 13.   ]]\n",
      "63.03999999999999\n",
      "[[ 5.8        19.1        13.         14.9664308  15.58149264 12.        ]]\n",
      "21.67999999999998\n",
      "[[14.1         5.5        14.          6.99500563 18.85532075 11.        ]]\n",
      "-78.28\n",
      "[[ 4.2         2.8        14.          5.00614254 20.61021498 10.        ]]\n",
      "-19.6\n",
      "[[ 4.7         9.         14.         17.04497259 15.56766711  9.        ]]\n",
      "-3.1599999999999966\n",
      "[[ 5.9   21.    15.     6.578 28.973  8.   ]]\n",
      "27.080000000000013\n",
      "[[11.         20.2        15.         21.14403402 56.55212425  7.        ]]\n",
      "-10.159999999999997\n",
      "[[15.     7.7   17.     2.804 51.521  6.   ]]\n",
      "-77.35999999999999\n",
      "[[10.7    7.1   17.     3.723 47.164  5.   ]]\n",
      "-50.03999999999998\n",
      "[[ 2.5         5.7        17.          2.46108124 55.30479972  4.        ]]\n",
      "1.240000000000009\n",
      "[[ 0.4         6.6        17.          7.31314957 50.21172802  3.        ]]\n",
      "18.4\n",
      "[[ 7.         12.1        17.         21.22472115 49.05112651  2.        ]]\n",
      "-8.879999999999995\n",
      "[[11.3        13.2        17.         11.56284876 30.22364295  1.        ]]\n",
      "1465.4\n",
      "[[ 9.4   20.8   17.     1.948  4.621  0.   ]]\n",
      "2.6399999999999864\n",
      "[[10.2        18.3        18.          7.68853529 32.27630873 -1.        ]]\n",
      "-10.799999999999983\n",
      "[[ 4.5        23.5        18.         25.14145756 40.63946196 -1.        ]]\n",
      "44.599999999999994\n",
      "[[ 9.2        25.1        18.         36.06240025 28.67543388 -1.        ]]\n",
      "17.76000000000002\n",
      "[[ 6.8   26.5   19.     4.011 32.357 -1.   ]]\n",
      "38.56000000000003\n",
      "[[ 1.6        19.3        19.         15.49169136 49.5125283  -1.        ]]\n",
      "50.879999999999995\n",
      "[[ 3.6        13.9        19.         26.20980474 49.79256318 -1.        ]]\n",
      "20.0\n",
      "[[19.3        27.9        19.         26.68400719 15.97273455 -1.        ]]\n",
      "-41.960000000000036\n",
      "[[ 6.8         4.2        20.         21.60846672 20.66063954 -1.        ]]\n",
      "-32.8\n",
      "[[24.2        16.8        20.          1.8177692  54.69909118 -1.        ]]\n",
      "-110.80000000000001\n",
      "[[13.5        15.8        21.         20.01202951 45.66322804 -1.        ]]\n",
      "-41.24000000000001\n",
      "[[ 8.         13.6        21.         25.71851709 44.91085228 -1.        ]]\n",
      "-10.879999999999995\n",
      "[[20.2        18.5        21.         23.74031311 44.63248338 -1.        ]]\n",
      "-78.16000000000003\n",
      "[[10.9        18.6        21.         27.19445546 34.69884163 -1.        ]]\n",
      "-14.599999999999994\n",
      "[[ 5.3        23.1        22.         45.76673648 42.19046339 -1.        ]]\n",
      "37.879999999999995\n",
      "[[ 7.    30.3    9.    10.824 30.088 -1.   ]]\n",
      "49.360000000000014\n",
      "[[14.6        30.         10.         25.08760946 18.27410512 -1.        ]]\n",
      "-3.2800000000000296\n",
      "[[ 8.1         7.1        12.         25.01239249 20.89607199 -1.        ]]\n",
      "-32.36\n",
      "[[16.8        19.6        12.         16.5036237  29.87727795 -1.        ]]\n",
      "-51.52000000000004\n",
      "[[ 6.2        26.5        13.         42.45840725 23.4956081  -1.        ]]\n",
      "42.639999999999986\n",
      "[[ 6.         23.5        19.         29.23920091  5.71172368 -1.        ]]\n",
      "34.400000000000006\n",
      "[[ 8.9         2.5         7.         29.66853988 15.09967052 -1.        ]]\n",
      "-52.519999999999996\n",
      "[[17.3         3.5         8.         17.77555674 27.26188351 -1.        ]]\n",
      "-106.44\n",
      "[[25.8        31.1         8.         33.48972368 48.26164388 -1.        ]]\n",
      "-75.92000000000002\n",
      "[[18.4   12.7    9.    10.277 54.803 -1.   ]]\n",
      "-84.47999999999999\n",
      "[[ 5.6         7.5         9.          9.90832055 49.63000603 -1.        ]]\n",
      "-14.079999999999998\n",
      "[[13.9         7.         10.         10.39373151 29.78081273 -1.        ]]\n",
      "-72.11999999999998\n",
      "[[ 3.7    4.4   10.     3.381 33.637 -1.   ]]\n",
      "-11.080000000000005\n",
      "[[12.9        28.1        10.         26.83103146 31.57055175 -1.        ]]\n",
      "2.1999999999999886\n",
      "[[ 5.2         2.7        11.         20.80485645 30.71782708 -1.        ]]\n",
      "-26.72\n",
      "[[18.5        15.9        11.         24.52821935 47.23729    -1.        ]]\n",
      "-74.91999999999999\n",
      "[[ 5.5        12.3        11.         23.53106943 57.1568157  -1.        ]]\n",
      "1.9599999999999937\n",
      "[[19.8        12.6        12.          1.85936277 44.23527467 -1.        ]]\n",
      "-94.32\n",
      "[[ 7.9    9.4   12.     8.805 28.305 -1.   ]]\n",
      "-23.64\n",
      "[[ 4.2        32.         12.         14.04589115 56.86199793 -1.        ]]\n",
      "73.83999999999997\n",
      "[[12.8        16.4        13.          3.91300415 33.99464157 -1.        ]]\n",
      "-34.56\n",
      "[[ 9.4        22.3        13.         19.13397522 21.89640098 -1.        ]]\n",
      "7.439999999999998\n",
      "[[13.6        18.2        13.         14.45020626 31.36889261 -1.        ]]\n",
      "-34.23999999999998\n",
      "[[ 3.6        42.3        13.         48.29279416  2.4023074  -1.        ]]\n",
      "110.88\n",
      "[[ 4.5         3.3        18.         46.57532135  4.57786399 -1.        ]]\n",
      "-20.04\n",
      "driver reward  809.5600000000005\n",
      "[[ 3.3        17.6         6.         13.02072449 20.97807819 40.        ]]\n",
      "33.879999999999995\n",
      "[[ 1.6        16.1         6.         24.57229713  9.30405995 39.        ]]\n",
      "40.639999999999986\n",
      "[[ 9.1   32.7    9.    16.71  42.942 38.   ]]\n",
      "42.75999999999999\n",
      "[[ 2.6        16.2         9.         32.27985763 37.83962416 37.        ]]\n",
      "34.16\n",
      "[[18.2         4.5        12.         17.45668681 40.40237875 36.        ]]\n",
      "-109.35999999999999\n",
      "[[ 5.7        17.8        12.         15.40101974 21.35843877 35.        ]]\n",
      "18.200000000000017\n",
      "[[ 1.3   23.8   12.    12.576 44.286 34.   ]]\n",
      "67.32\n",
      "[[15.7         7.9        12.          5.59099719 31.14073118 33.        ]]\n",
      "-81.48000000000002\n",
      "[[ 9.6        10.9        12.         18.91198842 18.65875463 32.        ]]\n",
      "-30.400000000000006\n",
      "[[ 3.3   19.2   13.    14.516 33.928 31.   ]]\n",
      "39.0\n",
      "[[ 2.1         2.2        13.         13.13586323 35.39595447 30.        ]]\n",
      "-7.2400000000000055\n",
      "[[ 4.1        19.8        13.         14.23401888 11.92964954 29.        ]]\n",
      "35.48000000000002\n",
      "[[12.5         7.2        14.          5.29460617 25.91191159 28.        ]]\n",
      "-61.95999999999998\n",
      "[[ 2.5        25.5        14.         12.46149016 52.45101981 27.        ]]\n",
      "64.6\n",
      "[[ 9.3        19.2        14.         15.04377205 31.71533294 26.        ]]\n",
      "-1.799999999999983\n",
      "[[ 2.4        13.5        14.         14.02564814 44.83595902 25.        ]]\n",
      "26.879999999999995\n",
      "[[ 6.3         6.2        14.         15.51036692 33.47545304 24.        ]]\n",
      "-23.0\n",
      "[[11.2        23.9        15.          0.49423329 15.93705432 23.        ]]\n",
      "0.32000000000005\n",
      "[[ 9.1   13.1   16.    14.045 32.049 22.   ]]\n",
      "-19.95999999999998\n",
      "[[ 0.2   11.4   16.    16.078 20.594 21.   ]]\n",
      "35.120000000000005\n",
      "[[17.5        19.1        17.         26.57426824 33.3106683  20.        ]]\n",
      "-57.879999999999995\n",
      "[[13.2   13.    18.     7.392 44.442 19.   ]]\n",
      "-48.16\n",
      "[[ 4.7    8.4   18.     7.354 49.254 18.   ]]\n",
      "-5.0800000000000125\n",
      "[[ 8.6        28.7        18.         22.40218833 16.53478924 17.        ]]\n",
      "33.360000000000014\n",
      "[[14.9       27.        18.        30.7977762 29.092833  16.       ]]\n",
      "-14.919999999999959\n",
      "[[16.3        29.4        20.         46.29799953 19.98939071 15.        ]]\n",
      "-16.75999999999999\n",
      "[[22.9   49.4   10.    20.989 32.248 14.   ]]\n",
      "2.3600000000000136\n",
      "[[21.8        18.1        11.          4.4910156  57.03340639 13.        ]]\n",
      "-90.32000000000005\n",
      "[[24.         19.5        12.          6.86109879 50.71408329 12.        ]]\n",
      "-100.80000000000001\n",
      "[[ 7.5   40.6   12.    45.848 48.107 11.   ]]\n",
      "78.92000000000002\n",
      "[[12.4   28.    19.     7.567 46.686 10.   ]]\n",
      "5.28000000000003\n",
      "[[ 0.6        16.8        19.         14.98826555 31.96179082  9.        ]]\n",
      "49.67999999999999\n",
      "[[ 8.5    5.3   19.     5.346 24.929  8.   ]]\n",
      "-40.84\n",
      "[[14.2        13.1        19.         20.13441824 42.10158873  7.        ]]\n",
      "-54.639999999999986\n",
      "[[19.6        10.8        19.         17.19719325 24.96873199  6.        ]]\n",
      "-98.72\n",
      "[[ 4.4   17.3   20.     7.258 14.959  5.   ]]\n",
      "25.439999999999998\n",
      "[[ 4.6        13.7        21.          8.62662719  4.91981334  4.        ]]\n",
      "12.560000000000016\n",
      "[[20.5        29.         22.         34.51997015 12.12020136  3.        ]]\n",
      "-46.599999999999966\n",
      "[[ 8.2   28.4   17.     5.846 28.209  2.   ]]\n",
      "35.12000000000003\n",
      "[[ 3.4        11.5        17.         18.53488333 36.14598852  1.        ]]\n",
      "1513.68\n",
      "[[ 2.2        31.9        17.         48.57835351 47.59196667  0.        ]]\n",
      "87.12\n",
      "[[ 6.8    6.5    7.    53.556 55.262 -1.   ]]\n",
      "-25.439999999999998\n",
      "[[ 7.4   11.7   10.    59.809 55.813 -1.   ]]\n",
      "-12.879999999999995\n",
      "[[ 4.1   43.3    8.    17.315 44.141 -1.   ]]\n",
      "110.68\n",
      "[[ 7.3   22.3    8.     3.272 18.818 -1.   ]]\n",
      "21.72\n",
      "[[ 2.2         2.8         8.          7.25563688 18.03688914 -1.        ]]\n",
      "-6.0\n",
      "[[ 8.9         6.4         8.         13.80778406 20.48363135 -1.        ]]\n",
      "-40.040000000000006\n",
      "[[17.3   15.7    9.     7.666 50.137 -1.   ]]\n",
      "-67.4\n",
      "[[11.6    7.6    9.     5.322 38.171 -1.   ]]\n",
      "-54.56\n",
      "[[ 8.5        10.5         9.         11.87978434 26.95860287 -1.        ]]\n",
      "-24.19999999999999\n",
      "[[13.1    3.5    9.    16.564 42.446 -1.   ]]\n",
      "-77.88000000000001\n",
      "[[11.         10.2         9.         14.63019956 56.28601151 -1.        ]]\n",
      "-42.16\n",
      "[[ 9.         16.6         9.         27.91551293 39.32094574 -1.        ]]\n",
      "-8.080000000000013\n",
      "[[21.1         8.7         9.          0.40140643 47.42542069 -1.        ]]\n",
      "-115.63999999999999\n",
      "[[11.6        19.7         9.         23.34239947 36.42004745 -1.        ]]\n",
      "-15.839999999999975\n",
      "[[17.1        16.3        10.         22.19573898 37.29926003 -1.        ]]\n",
      "-64.12000000000003\n",
      "[[14.4   22.    10.     1.574 48.14  -1.   ]]\n",
      "-27.519999999999982\n",
      "[[ 0.9         4.8        11.          0.94162864 53.83151274 -1.        ]]\n",
      "9.240000000000002\n",
      "[[10.         20.9        11.          3.12982592 23.48049047 -1.        ]]\n",
      "-1.1199999999999761\n",
      "[[ 8.9         7.9        12.         17.50006491 22.56535246 -1.        ]]\n",
      "-35.239999999999995\n",
      "[[ 6.         30.6        12.         31.71495835  0.08873327 -1.        ]]\n",
      "57.120000000000005\n",
      "[[16.4         5.4        13.         13.81485591 11.84420722 -1.        ]]\n",
      "-94.23999999999998\n",
      "[[21.         17.1        13.         19.11180233 40.69954002 -1.        ]]\n",
      "-88.07999999999998\n",
      "[[17.8        34.7        13.         19.67833456  2.73392176 -1.        ]]\n",
      "-10.0\n",
      "[[ 6.3         6.9        14.         20.54357367  3.70580223 -1.        ]]\n",
      "-20.75999999999999\n",
      "[[19.3        23.5        14.         18.80069706 35.99623729 -1.        ]]\n",
      "-56.039999999999964\n",
      "[[ 2.6        10.8        14.          5.77943955 33.81687814 -1.        ]]\n",
      "16.879999999999995\n",
      "[[ 9.         17.         14.         19.87797395 31.10414664 -1.        ]]\n",
      "-6.799999999999983\n",
      "[[11.9        20.4        14.         13.39402027 10.89419335 -1.        ]]\n",
      "-15.639999999999986\n",
      "[[19.1        12.4        14.          3.75602942 18.91057311 -1.        ]]\n",
      "-90.19999999999999\n",
      "[[14.1   25.8   14.     2.851 49.31  -1.   ]]\n",
      "-13.319999999999993\n",
      "[[ 6.7    8.3   15.     8.276 42.729 -1.   ]]\n",
      "-19.0\n",
      "[[11.9         8.7        15.         10.82335994 23.06709974 -1.        ]]\n",
      "-53.08000000000001\n",
      "[[ 8.         11.2        15.         14.90952927 39.89843174 -1.        ]]\n",
      "-18.560000000000002\n",
      "[[20.1        12.2        15.         16.27811026 53.83964887 -1.        ]]\n",
      "-97.63999999999999\n",
      "[[ 8.4   20.6   15.     5.529 39.359 -1.   ]]\n",
      "8.800000000000011\n",
      "[[ 4.7        12.7        15.         15.58292199 43.87945917 -1.        ]]\n",
      "8.680000000000007\n",
      "[[ 6.8         7.4        15.         19.62854574 52.98128416 -1.        ]]\n",
      "-22.559999999999988\n",
      "[[24.4         2.         15.          3.56357783 37.20996146 -1.        ]]\n",
      "-159.51999999999998\n",
      "[[13.3        26.9        16.         12.88357197 50.27136118 -1.        ]]\n",
      "-4.360000000000014\n",
      "[[20.9        15.4        16.          0.14043522 16.59331937 -1.        ]]\n",
      "-92.83999999999997\n",
      "[[ 9.7        15.9        16.         18.532935   33.46301642 -1.        ]]\n",
      "-15.080000000000013\n",
      "[[19.3         6.4        16.          9.52314923 49.40596628 -1.        ]]\n",
      "-110.76000000000002\n",
      "[[ 6.3        21.6        16.         20.18660791 35.96579888 -1.        ]]\n",
      "26.28\n",
      "[[18.5        16.8        16.         17.15783678 19.17169399 -1.        ]]\n",
      "-72.03999999999996\n",
      "[[10.9        25.8        16.          2.23691562 47.64069083 -1.        ]]\n",
      "8.439999999999998\n",
      "[[13.1         6.7        16.          5.53790469 56.29705497 -1.        ]]\n",
      "-67.64000000000001\n",
      "[[ 3.8        16.7        17.         20.04474242 52.90254896 -1.        ]]\n",
      "27.599999999999994\n",
      "[[10.1         3.4        17.         26.86085316 41.81369529 -1.        ]]\n",
      "-57.8\n",
      "[[ 2.4        10.3        17.         15.32473277 42.03617088 -1.        ]]\n",
      "16.64\n",
      "[[13.1        11.7        17.         14.22485989 40.82930959 -1.        ]]\n",
      "-51.639999999999986\n",
      "[[ 5.7        13.9        17.         18.4238164  48.02508174 -1.        ]]\n",
      "5.719999999999999\n",
      "[[14.2        13.8        18.         18.77234635 50.72516216 -1.        ]]\n",
      "-52.400000000000006\n",
      "[[21.1        19.5        18.         22.41002534 36.92406652 -1.        ]]\n",
      "-81.07999999999998\n",
      "[[17.1        37.5        19.         22.19446878 12.1908013  -1.        ]]\n",
      "3.7200000000000273\n",
      "[[15.9        22.6        19.         28.79101489  4.35288314 -1.        ]]\n",
      "-35.80000000000001\n",
      "[[ 3.5         4.1        19.         30.22744108  7.95398686 -1.        ]]\n",
      "-10.68\n",
      "[[23.1        24.9        11.          1.10483012 42.32441095 -1.        ]]\n",
      "-77.39999999999998\n",
      "[[ 5.7        15.7        12.         21.08102581 34.92395461 -1.        ]]\n",
      "11.480000000000018\n",
      "[[19.2         5.7        12.          7.37916793 39.46928286 -1.        ]]\n",
      "-112.32\n",
      "[[ 8.5        13.4        12.          4.3837208  59.41042947 -1.        ]]\n",
      "-14.919999999999987\n",
      "[[ 9.9         5.6        12.         12.01239799 51.04937991 -1.        ]]\n",
      "-49.39999999999999\n",
      "[[ 6.8        35.1        12.         40.37402115 49.68754047 -1.        ]]\n",
      "66.08000000000004\n",
      "[[15.8    9.5   12.    21.372 52.343 -1.   ]]\n",
      "-77.03999999999999\n",
      "[[18.2         2.5        12.          4.71677179 57.58447467 -1.        ]]\n",
      "-115.75999999999999\n",
      "[[ 2.6        22.         12.         16.25494087 41.55693889 -1.        ]]\n",
      "52.72\n",
      "[[ 4.6        20.3        12.          7.98344213 57.24501825 -1.        ]]\n",
      "33.68000000000001\n",
      "[[14.2        17.9        12.         10.79907081 25.26553245 -1.        ]]\n",
      "-39.279999999999944\n",
      "[[ 3.7         6.1        12.          1.61130869 21.95722811 -1.        ]]\n",
      "-5.640000000000001\n",
      "[[12.1        18.1        12.         17.36764947 45.2671605  -1.        ]]\n",
      "-24.360000000000014\n",
      "[[15.9        28.5        12.         14.54647248 24.98012073 -1.        ]]\n",
      "-16.91999999999996\n",
      "[[12.9        21.7        12.          5.83177817 37.45566237 -1.        ]]\n",
      "-18.28\n",
      "[[ 2.9         8.         13.         13.16810513 33.2154033  -1.        ]]\n",
      "5.8799999999999955\n",
      "[[ 1.         15.4        13.          8.70281957 48.74971975 -1.        ]]\n",
      "42.48000000000002\n",
      "[[ 8.4         7.         13.         19.26328997 52.34848862 -1.        ]]\n",
      "-34.72\n",
      "[[14.5        29.3        13.         33.64374391 46.50557438 -1.        ]]\n",
      "-4.839999999999975\n",
      "[[ 6.3    7.    13.    46.479 48.628 -1.   ]]\n",
      "-20.439999999999998\n",
      "[[16.7        26.6        18.         45.4231907  31.76854607 -1.        ]]\n",
      "-28.439999999999998\n",
      "driver reward  -769.6399999999992\n",
      "[[ 6.8   20.8    5.    33.317 30.845 40.   ]]\n",
      "20.319999999999993\n",
      "[[ 6.1        28.8         7.         14.26178117 17.76475486 39.        ]]\n",
      "50.68000000000001\n",
      "[[ 4.8        17.5         8.         27.06357775 17.76876455 38.        ]]\n",
      "23.359999999999985\n",
      "[[ 2.2   28.7    8.     4.005 37.927 37.   ]]\n",
      "76.88000000000002\n",
      "[[14.2        25.6         9.         27.92185188  6.58040926 36.        ]]\n",
      "-14.639999999999986\n",
      "[[21.7    7.2   12.    13.95  14.644 35.   ]]\n",
      "-124.51999999999998\n",
      "[[ 1.4        16.8        12.         14.33953136 30.35260072 34.        ]]\n",
      "44.24000000000001\n",
      "[[13.5        42.6        12.         36.10266978 53.27281638 33.        ]]\n",
      "44.51999999999998\n",
      "[[ 1.5         5.4        16.         30.85030311 51.617617   32.        ]]\n",
      "7.079999999999998\n",
      "[[14.6        24.4        17.         43.36346733 42.83809719 31.        ]]\n",
      "-21.19999999999999\n",
      "[[ 0.5        23.4        20.         36.74916609 19.98997783 30.        ]]\n",
      "71.48000000000002\n",
      "[[26.8        21.8         7.         36.25079085 28.3313546  29.        ]]\n",
      "-112.48000000000002\n",
      "[[15.8        23.5         7.         13.52165897 16.39948476 28.        ]]\n",
      "-32.23999999999995\n",
      "[[ 5.3        12.          8.         19.51986921 11.0875369  27.        ]]\n",
      "2.3599999999999994\n",
      "[[ 4.6        37.7         9.         35.31978527 43.27970687 26.        ]]\n",
      "89.35999999999996\n",
      "[[18.7        12.8         9.         14.6186145  56.45868447 25.        ]]\n",
      "-86.19999999999999\n",
      "[[10.7        25.9         9.         33.10140411 55.12565608 24.        ]]\n",
      "10.120000000000033\n",
      "[[12.         14.1        10.         42.27009304 48.56990441 23.        ]]\n",
      "-36.48000000000002\n",
      "[[ 4.         30.8        11.         11.97715085 31.83405176 22.        ]]\n",
      "71.36000000000001\n",
      "[[ 8.8         4.1        12.          6.03901629 37.65019126 21.        ]]\n",
      "-46.72\n",
      "[[ 7.6        16.7        12.         27.97127746 37.66827888 20.        ]]\n",
      "1.7600000000000193\n",
      "[[15.4        27.1        13.         44.94540891 44.67213189 19.        ]]\n",
      "-18.0\n",
      "[[ 4.5        25.1        15.         16.42565609 44.07315388 18.        ]]\n",
      "49.72\n",
      "[[ 5.8   23.6   15.     2.078 29.824 17.   ]]\n",
      "36.079999999999984\n",
      "[[ 1.6        33.5        16.         29.56250335 51.17623051 16.        ]]\n",
      "96.32\n",
      "[[ 7.    18.9   16.    48.654 57.852 15.   ]]\n",
      "12.880000000000024\n",
      "[[22.2         9.          9.         31.34687615 37.37524082 14.        ]]\n",
      "-122.16\n",
      "[[12.9        21.2        10.         28.04959806 23.95176778 13.        ]]\n",
      "-19.879999999999995\n",
      "[[ 5.5        20.7        11.         48.84993747 37.48083765 12.        ]]\n",
      "28.840000000000003\n",
      "[[ 2.1        20.8        13.         27.02595797 44.15744678 11.        ]]\n",
      "52.28\n",
      "[[ 2.9   22.2   14.     5.785 52.229 10.   ]]\n",
      "51.32000000000002\n",
      "[[12.8    5.2   14.     6.991 39.803  9.   ]]\n",
      "-70.39999999999999\n",
      "[[ 3.8         6.7        14.         10.63869192 38.24081212  8.        ]]\n",
      "-4.3999999999999915\n",
      "[[ 5.9        19.1        14.         35.27340037 42.0253702   7.        ]]\n",
      "21.0\n",
      "[[ 2.2        12.2        17.         31.55389946 31.19267885  6.        ]]\n",
      "24.080000000000013\n",
      "[[25.7        27.1        17.         34.84457469 32.90927915  5.        ]]\n",
      "-88.03999999999996\n",
      "[[19.9   17.2   17.     5.335 26.705  4.   ]]\n",
      "-80.27999999999994\n",
      "[[ 4.4         3.5        17.          8.28620002 24.99223349  3.        ]]\n",
      "-18.72\n",
      "[[ 3.2         7.5        17.         10.78056582 16.57002907  2.        ]]\n",
      "2.240000000000009\n",
      "[[ 7.1        30.3        17.         38.30645804 25.62225682  1.        ]]\n",
      "1548.68\n",
      "[[ 8.8   27.2   18.     8.866 46.16   0.   ]]\n",
      "27.200000000000017\n",
      "[[ 1.7        17.3        18.         16.91173571 30.57900994 -1.        ]]\n",
      "43.80000000000001\n",
      "[[17.8   10.6   18.     4.038 29.396 -1.   ]]\n",
      "-87.11999999999998\n",
      "[[ 3.3         4.6        18.          5.99945987 36.30343069 -1.        ]]\n",
      "-7.719999999999992\n",
      "[[ 9.7        18.2        18.         22.20462098 37.74131516 -1.        ]]\n",
      "-7.719999999999999\n",
      "[[16.8         8.4        18.         11.30629653 53.94380189 -1.        ]]\n",
      "-87.36000000000001\n",
      "[[15.1        19.9        19.          8.61364267 25.59925947 -1.        ]]\n",
      "-39.0\n",
      "[[ 6.6        27.6        19.         20.40122621  2.13824839 -1.        ]]\n",
      "43.44\n",
      "[[15.3   14.9   19.     5.436 28.261 -1.   ]]\n",
      "-56.360000000000014\n",
      "[[ 3.1   19.6   19.     3.32  46.688 -1.   ]]\n",
      "41.639999999999986\n",
      "[[ 9.3    8.7   19.     2.289 45.283 -1.   ]]\n",
      "-35.39999999999999\n",
      "[[11.6        24.         19.         10.4518068  33.23010186 -1.        ]]\n",
      "-2.0800000000000125\n",
      "[[18.         18.9        19.         15.76773832 29.54473467 -1.        ]]\n",
      "-61.91999999999999\n",
      "[[10.7         4.3        20.          7.22973811 26.35185183 -1.        ]]\n",
      "-59.0\n",
      "[[ 6.8        20.5        20.         23.28684872  5.96078149 -1.        ]]\n",
      "19.360000000000014\n",
      "[[17.2    6.1   21.     7.757 14.668 -1.   ]]\n",
      "-97.43999999999997\n",
      "[[ 5.9        11.6        21.         17.17480982 19.47128928 -1.        ]]\n",
      "-3.0\n",
      "[[ 2.4         8.3        22.         23.66501577 26.94812727 -1.        ]]\n",
      "10.239999999999995\n",
      "[[17.1         9.9         0.         18.52368108 21.90391326 -1.        ]]\n",
      "-84.6\n",
      "[[21.2    7.4    4.     5.119 46.76  -1.   ]]\n",
      "-120.48000000000002\n",
      "[[ 2.9         5.3         4.          7.14183998 50.03506931 -1.        ]]\n",
      "-2.759999999999991\n",
      "[[16.2        37.6         5.         50.58650525 36.85815172 -1.        ]]\n",
      "10.160000000000025\n",
      "[[12.8   22.2   13.    60.    39.815 -1.   ]]\n",
      "-16.0\n",
      "[[ 4.1        18.4        18.         42.10658762 48.09243092 -1.        ]]\n",
      "31.0\n",
      "[[ 7.4        33.4        19.         38.01275492  8.6671361  -1.        ]]\n",
      "56.56\n",
      "driver reward  1056.0400000000002\n",
      "[[ 8.1   20.3    5.    11.661 48.691 40.   ]]\n",
      "9.880000000000024\n",
      "[[ 3.9         6.          7.          9.7265236  44.40997662 39.        ]]\n",
      "-7.320000000000007\n",
      "[[13.8         8.1         7.         15.38574258 45.28047053 38.        ]]\n",
      "-67.91999999999999\n",
      "[[ 9.3        27.3         8.          2.76888332 31.70400845 37.        ]]\n",
      "24.120000000000005\n",
      "[[14.         20.4        11.         21.79851934 24.47470969 36.        ]]\n",
      "-29.919999999999987\n",
      "[[ 8.5         9.         11.         22.84760646 26.16646675 35.        ]]\n",
      "-29.0\n",
      "[[ 9.1        17.7        12.         17.84894996 45.78444276 34.        ]]\n",
      "-5.239999999999981\n",
      "[[ 6.          4.1        12.         13.49790336 38.63646276 33.        ]]\n",
      "-27.679999999999993\n",
      "[[11.3        22.4        12.         34.08022084 38.00895752 32.        ]]\n",
      "-5.160000000000025\n",
      "[[ 8.6        30.8        18.          2.61885194 23.79041712 31.        ]]\n",
      "40.08000000000004\n",
      "[[ 3.5        17.5        18.         22.80307247 17.91257076 30.        ]]\n",
      "32.20000000000002\n",
      "[[25.         24.2        18.         30.17808157 41.66208478 29.        ]]\n",
      "-92.56\n",
      "[[ 8.9        17.8        19.         13.57791839 45.82681365 28.        ]]\n",
      "-3.5600000000000023\n",
      "[[ 7.5    9.3   19.     7.38  52.941 27.   ]]\n",
      "-21.239999999999995\n",
      "[[ 4.8   11.4   19.     6.967 42.268 26.   ]]\n",
      "3.8400000000000034\n",
      "[[ 1.         12.2        19.         16.88589302 48.00400876 25.        ]]\n",
      "32.24000000000001\n",
      "[[ 8.2        13.8        19.         23.38135533 42.20836274 24.        ]]\n",
      "-11.599999999999994\n",
      "[[ 8.1        32.4        19.         59.61273179 50.64983072 23.        ]]\n",
      "48.60000000000002\n",
      "[[ 7.7        21.4         9.         42.60192746 27.04575098 22.        ]]\n",
      "16.120000000000033\n",
      "[[ 5.    10.7    9.    51.337 26.052 21.   ]]\n",
      "0.2400000000000091\n",
      "[[17.4   19.7   13.    30.278 21.68  20.   ]]\n",
      "-55.279999999999944\n",
      "[[ 7.8        20.1        14.         42.25464236 26.36864042 19.        ]]\n",
      "11.280000000000001\n",
      "[[21.7         2.1        20.         24.09771844 36.5530231  18.        ]]\n",
      "-140.84\n",
      "[[19.6         8.3        21.          4.12781087 36.34750239 17.        ]]\n",
      "-106.72\n",
      "[[ 3.9         6.5        21.          8.74263917 27.91616135 16.        ]]\n",
      "-5.719999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.         51.9        21.         55.58517366 15.7784101  15.        ]]\n",
      "125.28000000000003\n",
      "[[ 1.9        13.         14.         44.43378213 22.05373861 14.        ]]\n",
      "28.680000000000007\n",
      "[[21.8        13.3         7.         38.21656221 47.58880197 13.        ]]\n",
      "-105.68\n",
      "[[16.8   33.    10.     3.542 33.579 12.   ]]\n",
      "-8.639999999999986\n",
      "[[ 2.8        13.1        10.          5.28562754 48.91377433 11.        ]]\n",
      "22.88000000000001\n",
      "[[ 6.8        24.6        10.         27.27114023 40.64308007 10.        ]]\n",
      "32.47999999999999\n",
      "[[17.9        18.2        10.         28.75794206 41.34162328  9.        ]]\n",
      "-63.47999999999996\n",
      "[[14.3   13.4   11.     5.864 27.143  8.   ]]\n",
      "-54.360000000000014\n",
      "[[20.2        11.8        11.         16.85412072 57.04560771  7.        ]]\n",
      "-99.6\n",
      "[[13.          4.7        11.          8.01392397 53.93037996  6.        ]]\n",
      "-73.35999999999999\n",
      "[[ 3.1        10.2        12.         10.11943659 43.17489471  5.        ]]\n",
      "11.560000000000016\n",
      "[[ 8.5        14.3        12.          0.37442967 22.64538641  4.        ]]\n",
      "-12.039999999999992\n",
      "[[11.9        25.6        12.          1.10155598 56.73590535  3.        ]]\n",
      "1.0\n",
      "[[13.6        46.6        12.         46.31159771 21.28714719  2.        ]]\n",
      "56.639999999999986\n",
      "[[ 5.7   40.    14.    20.792 51.781  1.   ]]\n",
      "1589.24\n",
      "[[17.         20.         15.         15.37237163 42.26374505  0.        ]]\n",
      "-51.599999999999994\n",
      "[[ 9.9         2.         16.         11.44711487 35.25209484 -1.        ]]\n",
      "-60.92\n",
      "[[ 5.6         5.2        16.         11.45242318 34.58195291 -1.        ]]\n",
      "-21.439999999999998\n",
      "[[12.7        10.7        16.         17.75916173 23.10996032 -1.        ]]\n",
      "-52.119999999999976\n",
      "[[13.1        29.9        16.         26.79325349 37.29710709 -1.        ]]\n",
      "6.600000000000023\n",
      "[[11.7    4.1   16.    20.791 32.603 -1.   ]]\n",
      "-66.43999999999998\n",
      "[[14.3        25.2        16.         33.11453902 22.42550949 -1.        ]]\n",
      "-16.599999999999966\n",
      "[[ 5.9        25.7        17.         46.78407233 50.71780874 -1.        ]]\n",
      "42.120000000000005\n",
      "[[ 8.4        18.9         7.         24.61222912 44.01751707 -1.        ]]\n",
      "3.3600000000000136\n",
      "[[10.4    4.4    7.    16.852 43.737 -1.   ]]\n",
      "-56.64\n",
      "[[15.5   17.1    7.     3.455 35.263 -1.   ]]\n",
      "-50.68000000000001\n",
      "[[12.2   15.     7.     1.523 32.822 -1.   ]]\n",
      "-34.95999999999998\n",
      "[[ 8.3        23.5         7.          0.76733694 15.32990176 -1.        ]]\n",
      "18.75999999999999\n",
      "[[ 1.8   18.5    9.     9.098 33.697 -1.   ]]\n",
      "46.96000000000001\n",
      "[[11.5         5.8         9.         10.19588088 46.24876427 -1.        ]]\n",
      "-59.64\n",
      "[[ 6.8         8.6         9.         10.60117264 50.74079917 -1.        ]]\n",
      "-18.719999999999985\n",
      "[[ 7.9        25.1         9.         27.94131836 55.21091138 -1.        ]]\n",
      "26.599999999999994\n",
      "[[12.    11.3   10.     5.313 58.501 -1.   ]]\n",
      "-45.44\n",
      "[[ 2.5    0.7   10.     6.815 56.898 -1.   ]]\n",
      "-14.760000000000002\n",
      "[[17.9         7.6        10.          2.76329192 31.7441477  -1.        ]]\n",
      "-97.4\n",
      "[[13.6         5.4        10.          8.32996468 42.02272806 -1.        ]]\n",
      "-75.19999999999999\n",
      "[[14.9        32.3        11.         40.61602099 49.2003528  -1.        ]]\n",
      "2.0400000000000205\n",
      "[[ 6.8   45.    12.     2.921 56.405 -1.   ]]\n",
      "97.76000000000005\n",
      "[[ 9.2         4.1        12.         10.96892959 46.18603608 -1.        ]]\n",
      "-49.43999999999998\n",
      "[[ 7.5        26.6        13.         19.65376436 22.3475868  -1.        ]]\n",
      "34.120000000000005\n",
      "[[ 2.6        13.4        13.          9.17686735 10.68577395 -1.        ]]\n",
      "25.200000000000003\n",
      "[[ 4.1   17.4   14.     3.991 31.606 -1.   ]]\n",
      "27.80000000000001\n",
      "[[ 6.5        19.3        14.          7.09663155 14.72651555 -1.        ]]\n",
      "17.560000000000002\n",
      "[[ 2.4    3.2   15.     4.403 16.79  -1.   ]]\n",
      "-6.079999999999998\n",
      "[[ 5.6        20.2        15.         30.07657783 18.20103341 -1.        ]]\n",
      "26.56000000000003\n",
      "[[ 3.3        22.6        16.          5.93215261 26.67664973 -1.        ]]\n",
      "49.879999999999995\n",
      "[[ 0.7        10.8        16.         16.18086267 26.43673271 -1.        ]]\n",
      "29.799999999999997\n",
      "[[ 5.3        17.         17.         34.92798304 15.31029446 -1.        ]]\n",
      "18.359999999999985\n",
      "[[19.9   17.4   19.     1.955 22.662 -1.   ]]\n",
      "-79.63999999999999\n",
      "[[ 7.5        16.2        19.         20.43968576 36.88478988 -1.        ]]\n",
      "0.8400000000000034\n",
      "[[12.2         9.         19.          3.60469711 49.29817304 -1.        ]]\n",
      "-54.16\n",
      "[[ 7.5         8.         19.         16.01803648 51.5806247  -1.        ]]\n",
      "-25.39999999999999\n",
      "[[13.9        12.2        19.          9.27871997 43.59816904 -1.        ]]\n",
      "-55.48000000000002\n",
      "[[ 8.8        17.7        20.          7.83120519 53.10155086 -1.        ]]\n",
      "-3.1999999999999886\n",
      "[[ 5.2        11.1        20.         14.92239928 50.89871955 -1.        ]]\n",
      "0.1599999999999966\n",
      "[[ 6.4        31.6        20.          0.8965574  15.93361556 -1.        ]]\n",
      "57.60000000000002\n",
      "[[10.9         7.4        20.          5.73990842 17.39446119 -1.        ]]\n",
      "-50.44\n",
      "[[ 4.6        28.2        20.         35.27829923  5.85179855 -1.        ]]\n",
      "58.960000000000036\n",
      "[[ 5.4   30.     6.    11.982 13.672 -1.   ]]\n",
      "59.28000000000003\n",
      "[[11.7        29.6         7.         36.23382706 32.55211732 -1.        ]]\n",
      "15.160000000000025\n",
      "[[22.3         8.2         7.         24.01936171 50.59239316 -1.        ]]\n",
      "-125.4\n",
      "[[16.         24.4         7.         31.89562124 47.82807923 -1.        ]]\n",
      "-30.71999999999997\n",
      "[[21.9        14.          8.          2.20708263 56.52146894 -1.        ]]\n",
      "-104.11999999999998\n",
      "[[16.         18.1         8.         30.9136598  41.77425382 -1.        ]]\n",
      "-50.879999999999995\n",
      "[[ 9.9        10.9         9.         35.22458499 43.94638908 -1.        ]]\n",
      "-32.44\n",
      "[[ 7.5        17.2         9.         27.56950338 27.38498566 -1.        ]]\n",
      "4.0400000000000205\n",
      "[[21.1        20.7        11.         14.42774782 30.31919953 -1.        ]]\n",
      "-77.23999999999995\n",
      "[[ 6.9         8.9        11.          0.82953657 23.44465773 -1.        ]]\n",
      "-18.439999999999998\n",
      "[[15.9        31.         12.         29.35002647  8.14450874 -1.        ]]\n",
      "-8.919999999999959\n",
      "[[24.4        25.         14.         34.9484098  26.73912716 -1.        ]]\n",
      "-85.91999999999996\n",
      "[[18.3        33.6        15.         15.74515352 51.14480872 -1.        ]]\n",
      "-16.920000000000016\n",
      "[[15.8        30.1        15.          1.24329195 27.63139451 -1.        ]]\n",
      "-11.120000000000005\n",
      "[[12.6    7.8   15.     6.641 28.304 -1.   ]]\n",
      "-60.72\n",
      "[[ 6.7         9.9        15.         14.08467834 41.25915505 -1.        ]]\n",
      "-13.88000000000001\n",
      "[[ 4.5        20.3        15.         35.84772532 52.88552991 -1.        ]]\n",
      "34.360000000000014\n",
      "[[ 3.8        16.8        16.         28.536327   34.08557687 -1.        ]]\n",
      "27.919999999999987\n",
      "[[15.5        35.8        16.         57.37787609 41.67919509 -1.        ]]\n",
      "9.160000000000025\n",
      "[[10.9         3.4        20.         46.81166508 42.10689925 -1.        ]]\n",
      "-63.24000000000001\n",
      "driver reward  54.040000000000916\n",
      "[[13.1   17.6   10.    26.461 46.321 40.   ]]\n",
      "-32.76000000000002\n",
      "[[19.2         9.8        10.         20.39785704 54.33351586 39.        ]]\n",
      "-99.19999999999999\n",
      "[[12.6        13.1        10.         21.88301618 53.04860196 38.        ]]\n",
      "-43.75999999999999\n",
      "[[ 9.2         9.4        10.         13.18796045 44.75750714 37.        ]]\n",
      "-32.480000000000004\n",
      "[[ 4.7        31.9        11.         45.86291636 52.07700384 36.        ]]\n",
      "70.12\n",
      "[[ 6.1        15.1        18.         50.31389378 43.91380973 35.        ]]\n",
      "6.840000000000003\n",
      "[[ 0.6         8.6         8.         57.65545603 40.03064111 34.        ]]\n",
      "23.440000000000005\n",
      "[[ 9.1        37.3        13.         49.93996076 12.2568824  33.        ]]\n",
      "57.48000000000002\n",
      "[[ 7.3        24.6        20.         28.19034148 22.61053757 32.        ]]\n",
      "29.079999999999984\n",
      "[[21.7        21.9         7.         32.28684391 47.94382701 31.        ]]\n",
      "-77.47999999999996\n",
      "[[21.          4.3         8.          7.27357221 48.47886237 30.        ]]\n",
      "-129.04\n",
      "[[13.9   24.6    8.     4.371 58.775 29.   ]]\n",
      "-15.800000000000011\n",
      "[[10.6   16.7    8.     5.314 31.833 28.   ]]\n",
      "-18.639999999999986\n",
      "[[15.1        26.8         9.         16.44911342 48.08419403 27.        ]]\n",
      "-16.91999999999996\n",
      "[[16.3        11.5         9.          6.1036212  49.79717427 26.        ]]\n",
      "-74.03999999999999\n",
      "[[ 7.3        6.5        9.         9.0552019 41.524927  25.       ]]\n",
      "-28.840000000000003\n",
      "[[ 1.7        15.3         9.         11.17213581 57.31790549 24.        ]]\n",
      "37.400000000000006\n",
      "[[ 4.8         3.5         9.          3.01726956 55.51201586 23.        ]]\n",
      "-21.440000000000005\n",
      "[[17.3        32.7         9.         17.68595766  7.90673008 22.        ]]\n",
      "-13.0\n",
      "[[ 5.9        21.6        12.         28.4119775  28.75796186 21.        ]]\n",
      "29.0\n",
      "[[ 2.4   24.7   12.     7.041 18.279 20.   ]]\n",
      "62.72000000000003\n",
      "[[13.2        13.8        12.          2.04079831 17.23260862 19.        ]]\n",
      "-45.599999999999994\n",
      "[[17.6        13.         12.          4.00401678 46.97825434 18.        ]]\n",
      "-78.08000000000001\n",
      "[[ 3.4         7.7        12.          7.69519056 56.22556816 17.        ]]\n",
      "1.5200000000000102\n",
      "[[ 5.5        25.2        12.          2.02217438 26.85234891 16.        ]]\n",
      "43.24000000000001\n",
      "[[12.4         6.2        12.          7.90552153 33.06138539 15.        ]]\n",
      "-64.48\n",
      "[[ 3.5        19.1        12.         18.58345879 21.31373974 14.        ]]\n",
      "37.31999999999999\n",
      "[[13.2        17.1        12.         20.79786237 16.67808238 13.        ]]\n",
      "-35.03999999999999\n",
      "[[ 9.3   20.1   12.     4.824 32.1   12.   ]]\n",
      "1.079999999999984\n",
      "[[16.1        12.9        13.         16.0240254  38.32880038 11.        ]]\n",
      "-68.19999999999999\n",
      "[[20.9        18.1        13.         12.78015301 39.80079804 10.        ]]\n",
      "-84.19999999999999\n",
      "[[ 8.1        13.2        13.          2.23417611 38.19380156  9.        ]]\n",
      "-12.839999999999975\n",
      "[[ 4.9         8.7        13.         13.15157327 45.3427371   8.        ]]\n",
      "-5.47999999999999\n",
      "[[ 6.4        30.6        13.         37.4208398  42.06600589  7.        ]]\n",
      "54.400000000000006\n",
      "[[ 8.3        10.7        14.         38.33452669 39.86332366  6.        ]]\n",
      "-22.19999999999999\n",
      "[[ 2.1        12.8        17.         40.94557998 50.30016886  5.        ]]\n",
      "26.680000000000007\n",
      "[[ 1.4   45.6   17.     6.159 22.634  4.   ]]\n",
      "136.40000000000003\n",
      "[[ 8.9        19.1        17.         15.9223639  47.02886815  3.        ]]\n",
      "0.5999999999999943\n",
      "[[11.6         5.7        17.          9.91272995 50.95167388  2.        ]]\n",
      "-60.64\n",
      "[[10.5    4.6   17.    11.047 43.66   1.   ]]\n",
      "1443.32\n",
      "[[ 8.9        10.4        17.         29.50155383 46.21557899  0.        ]]\n",
      "-27.24000000000001\n",
      "[[ 5.4        12.         17.         35.23055686 42.87136609 -1.        ]]\n",
      "1.6800000000000068\n",
      "[[ 4.3        22.6        19.         42.52835854 25.9810165  -1.        ]]\n",
      "43.079999999999984\n",
      "[[19.2   11.7    7.    17.896 28.096 -1.   ]]\n",
      "-93.11999999999998\n",
      "[[ 5.2   19.4    8.     1.972 31.06  -1.   ]]\n",
      "26.720000000000027\n",
      "[[15.2         4.6         8.         15.38191659 45.41527049 -1.        ]]\n",
      "-88.63999999999999\n",
      "[[10.3        17.6         8.          5.58530698 26.6656848  -1.        ]]\n",
      "-13.719999999999999\n",
      "[[ 7.4        17.8         8.         19.48934534 36.04594283 -1.        ]]\n",
      "6.639999999999986\n",
      "[[16.5        39.2         8.         38.95130786 24.23645629 -1.        ]]\n",
      "13.240000000000009\n",
      "[[ 7.         13.9        11.         48.27050847 17.93651407 -1.        ]]\n",
      "-3.119999999999976\n",
      "[[ 7.          0.9        17.         52.05438774 24.03939343 -1.        ]]\n",
      "-44.72\n",
      "[[ 3.1         4.8        23.         50.1169835  23.20676648 -1.        ]]\n",
      "-5.719999999999999\n",
      "[[10.9   11.4   12.    50.531 20.131 -1.   ]]\n",
      "-37.640000000000015\n",
      "[[25.          7.8        10.         28.50456847 42.85188001 -1.        ]]\n",
      "-145.03999999999996\n",
      "[[10.1        31.3        10.         49.91112704 46.82655586 -1.        ]]\n",
      "31.480000000000018\n",
      "[[11.7   12.5   12.    25.774 46.362 -1.   ]]\n",
      "-39.56\n",
      "[[26.4         8.1        12.          9.28765775 21.30161654 -1.        ]]\n",
      "-153.6\n",
      "[[ 6.8        16.8        12.         23.95548592 19.53486696 -1.        ]]\n",
      "7.519999999999982\n",
      "[[11.4   16.1   13.     4.356 12.927 -1.   ]]\n",
      "-26.0\n",
      "[[ 3.8         0.6        13.          8.42033567 11.28037708 -1.        ]]\n",
      "-23.919999999999995\n",
      "[[10.4        22.6        13.         27.23466293 24.14168739 -1.        ]]\n",
      "1.5999999999999943\n",
      "[[19.1        39.         13.         48.44983928 51.69386562 -1.        ]]\n",
      "-5.079999999999984\n",
      "[[18.9         5.8        17.         31.52706051 49.77375239 -1.        ]]\n",
      "-109.95999999999998\n",
      "[[ 6.2        18.6        17.          7.11154504 48.94278529 -1.        ]]\n",
      "17.360000000000014\n",
      "[[10.9        11.1        17.         11.34555927 30.52046694 -1.        ]]\n",
      "-38.599999999999994\n",
      "[[ 6.1        23.3        17.         21.34861454 16.39897232 -1.        ]]\n",
      "33.08000000000001\n",
      "[[13.3        10.9        17.          4.56242795 21.9107359  -1.        ]]\n",
      "-55.56\n",
      "[[ 1.1        47.8        17.         53.27996214 20.90872871 -1.        ]]\n",
      "145.48000000000002\n",
      "[[ 6.7   29.8   17.    31.63  29.056 -1.   ]]\n",
      "49.80000000000001\n",
      "[[12.3    8.7   18.    12.889 38.414 -1.   ]]\n",
      "-55.79999999999998\n",
      "[[ 3.1         7.9        18.          1.97806386 37.39400665 -1.        ]]\n",
      "4.200000000000003\n",
      "[[ 7.         11.4        18.          5.57648498 55.14444348 -1.        ]]\n",
      "-11.11999999999999\n",
      "[[ 5.4         0.6        18.          5.34274359 49.34500478 -1.        ]]\n",
      "-34.8\n",
      "[[ 5.8        12.9        18.         20.54974187 41.58432379 -1.        ]]\n",
      "1.8400000000000034\n",
      "[[11.    13.5   19.    12.249 42.395 -1.   ]]\n",
      "-31.599999999999994\n",
      "[[ 7.          6.3        19.          5.04864467 41.31630687 -1.        ]]\n",
      "-27.439999999999998\n",
      "[[15.5        45.3        19.         43.51749436 28.52308327 -1.        ]]\n",
      "39.56\n",
      "[[20.6         6.8        19.         19.18538083 32.51128125 -1.        ]]\n",
      "-118.32000000000002\n",
      "[[21.1        22.8        19.         29.46546811 56.63177721 -1.        ]]\n",
      "-70.52000000000004\n",
      "[[ 4.3         2.         19.         25.17931075 52.66809203 -1.        ]]\n",
      "-22.839999999999996\n",
      "[[21.         19.8        19.          4.04516502 36.02562357 -1.        ]]\n",
      "-79.44\n",
      "[[12.8        12.2        19.          4.60648481 36.76968567 -1.        ]]\n",
      "-48.0\n",
      "[[ 6.2        22.         20.          6.24288937 20.96871778 -1.        ]]\n",
      "28.24000000000001\n",
      "[[ 3.7         8.9        20.         12.2522013  25.04339921 -1.        ]]\n",
      "3.319999999999993\n",
      "[[ 7.2        32.         20.         31.10989473  1.24106328 -1.        ]]\n",
      "53.44\n",
      "driver reward  77.63999999999993\n",
      "[[ 1.9   27.7    8.    14.492 60.    40.   ]]\n",
      "75.72000000000003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1         8.6         8.         16.90961739 50.48410157 39.        ]]\n",
      "13.240000000000009\n",
      "[[ 4.7         6.9         9.         20.81523127 49.279411   38.        ]]\n",
      "-9.88000000000001\n",
      "[[ 7.1        17.3         9.         22.34903085 33.55102187 37.        ]]\n",
      "7.0800000000000125\n",
      "[[ 5.2        11.1        10.         21.42004184 47.00305473 36.        ]]\n",
      "0.1599999999999966\n",
      "[[12.3        27.5        10.         32.49181953 36.4790786  35.        ]]\n",
      "4.360000000000014\n",
      "[[19.3    5.2   13.    12.656 50.284 34.   ]]\n",
      "-114.6\n",
      "[[ 6.7        18.7        13.         24.1567011  27.78216967 33.        ]]\n",
      "14.280000000000001\n",
      "[[ 2.9   22.8   13.    13.996 51.293 32.   ]]\n",
      "53.24000000000001\n",
      "[[ 8.          9.8        14.          3.25525608 40.79350939 31.        ]]\n",
      "-23.040000000000006\n",
      "[[ 3.6        33.9        14.         27.55205046 19.8657774  30.        ]]\n",
      "84.0\n",
      "[[ 8.2         2.5        15.         20.62546281 11.78918702 29.        ]]\n",
      "-47.75999999999999\n",
      "[[ 5.6   33.1   17.     7.165 43.351 28.   ]]\n",
      "67.83999999999997\n",
      "[[ 1.         13.6        17.          6.79511711 30.17604333 27.        ]]\n",
      "36.72\n",
      "[[ 7.9        15.9        17.         21.29821772 23.97109856 26.        ]]\n",
      "-2.8400000000000034\n",
      "[[23.5        25.5        17.         31.73029484 34.54333492 25.        ]]\n",
      "-78.19999999999999\n",
      "[[17.    10.4   18.     5.467 41.739 24.   ]]\n",
      "-82.32\n",
      "[[ 3.7        16.         18.         22.17723111 44.89507415 23.        ]]\n",
      "26.04000000000002\n",
      "[[22.3        12.1        18.         15.61342048 20.6022851  22.        ]]\n",
      "-112.91999999999999\n",
      "[[19.8         3.5        19.          5.94234656  5.69848639 21.        ]]\n",
      "-123.44\n",
      "[[17.4         4.2        20.         21.05660124 20.73224681 20.        ]]\n",
      "-104.88\n",
      "[[22.8        18.2        20.         23.66183459 31.61443107 19.        ]]\n",
      "-96.80000000000001\n",
      "[[ 9.9         5.5        20.         16.58194465 32.38633042 18.        ]]\n",
      "-49.72\n",
      "[[13.7        13.         20.          6.32567021 26.12622776 17.        ]]\n",
      "-51.56\n",
      "[[ 7.9         9.2        20.         14.25071555 14.23938997 16.        ]]\n",
      "-24.28\n",
      "[[18.4        19.9        23.         15.60714743 49.79447287 15.        ]]\n",
      "-61.44\n",
      "[[16.1        13.7         5.         12.44611742 21.71125736 14.        ]]\n",
      "-65.63999999999999\n",
      "[[11.4        17.9         5.          2.97971792 11.65753629 13.        ]]\n",
      "-20.23999999999998\n",
      "[[ 4.9        41.5         7.         39.03195744 22.9096902  12.        ]]\n",
      "99.48000000000002\n",
      "[[10.2        17.9         8.         24.67832184 20.37100971 11.        ]]\n",
      "-12.079999999999984\n",
      "[[ 5.4        38.6        11.         40.96984863 54.8777167  10.        ]]\n",
      "86.80000000000001\n",
      "[[ 8.5    0.8   12.    34.616 49.625  9.   ]]\n",
      "-55.24\n",
      "[[15.4         1.8        15.         23.30979484 57.77704525  8.        ]]\n",
      "-98.96\n",
      "[[ 6.3        14.1        15.         19.88296987 38.00564758  7.        ]]\n",
      "2.280000000000001\n",
      "[[13.3        28.1        16.         16.28476121 21.10816342  6.        ]]\n",
      "-0.5200000000000387\n",
      "[[ 7.     4.3   16.    15.938 28.938  5.   ]]\n",
      "-33.84\n",
      "[[ 2.6         5.         16.         18.06693496 30.21349623  4.        ]]\n",
      "-1.6799999999999997\n",
      "[[ 5.6         8.2        17.         25.55570183 31.64917424  3.        ]]\n",
      "-11.83999999999999\n",
      "[[18.2        26.4        18.         20.10843134  6.35268413  2.        ]]\n",
      "-39.27999999999997\n",
      "[[18.9        23.8        18.         20.75605287  0.75734965  1.        ]]\n",
      "1447.6399999999999\n",
      "[[16.3         6.9         7.          3.88136623 14.82096706  0.        ]]\n",
      "-88.76000000000002\n",
      "[[11.7         5.1         8.          7.31842107 27.49887093 -1.        ]]\n",
      "-63.23999999999998\n",
      "[[ 6.1        17.6         8.         19.21570013 29.72815017 -1.        ]]\n",
      "14.839999999999975\n",
      "[[ 3.9   11.1    8.     8.519 38.516 -1.   ]]\n",
      "9.0\n",
      "[[ 6.8        40.3         8.         47.17024328 51.96935179 -1.        ]]\n",
      "82.72000000000003\n",
      "[[20.8   13.1   19.    32.241 45.534 -1.   ]]\n",
      "-99.51999999999998\n",
      "[[ 9.6         2.3        19.         24.34885935 52.35462861 -1.        ]]\n",
      "-57.91999999999999\n",
      "[[19.3        23.4        20.         14.3311424  21.52306129 -1.        ]]\n",
      "-56.360000000000014\n",
      "[[ 2.5        33.9        20.         39.01225051 42.0816239  -1.        ]]\n",
      "91.48000000000002\n",
      "[[ 4.         15.4        12.         54.13190759 38.8893723  -1.        ]]\n",
      "22.080000000000013\n",
      "[[ 2.2         9.4        17.         48.3046387  43.25573848 -1.        ]]\n",
      "15.11999999999999\n",
      "[[12.9        23.4         8.         32.28413591 25.06139321 -1.        ]]\n",
      "-12.839999999999975\n",
      "[[ 6.5   12.4    9.    27.508 36.692 -1.   ]]\n",
      "-4.519999999999982\n",
      "[[ 7.5    5.6    9.    25.267 46.689 -1.   ]]\n",
      "-33.08\n",
      "[[16.9   28.2    9.     3.926 26.456 -1.   ]]\n",
      "-24.67999999999995\n",
      "[[ 3.2   11.8    9.     4.819 34.997 -1.   ]]\n",
      "16.0\n",
      "[[ 3.6        17.6         9.         18.77302576 28.63472557 -1.        ]]\n",
      "31.839999999999975\n",
      "[[20.6        25.9        10.         29.11668473 33.55813856 -1.        ]]\n",
      "-57.19999999999999\n",
      "[[ 4.3       29.4       12.        11.8541165 55.6914082 -1.       ]]\n",
      "64.84000000000003\n",
      "[[ 8.9        19.7        13.         16.87033663 41.2616185  -1.        ]]\n",
      "2.519999999999982\n",
      "[[13.3        19.8        13.         23.36394702 42.26571219 -1.        ]]\n",
      "-27.080000000000013\n",
      "[[15.6        29.8        13.         26.31264844 17.59157033 -1.        ]]\n",
      "-10.71999999999997\n",
      "[[ 8.3   19.8   14.    20.567 39.044 -1.   ]]\n",
      "6.9199999999999875\n",
      "[[ 3.2        19.7        14.          6.31978268 56.73121664 -1.        ]]\n",
      "41.28\n",
      "[[ 3.4         1.8        14.          3.80596915 59.4392028  -1.        ]]\n",
      "-17.36\n",
      "[[ 9.4        12.9        17.          1.19707164 41.47529155 -1.        ]]\n",
      "-22.640000000000015\n",
      "[[ 4.3         2.1        17.          5.00690672 43.74825295 -1.        ]]\n",
      "-22.520000000000003\n",
      "[[ 7.8        35.2        17.         26.51257915  8.94647988 -1.        ]]\n",
      "59.60000000000002\n",
      "[[22.6        29.5        18.         33.86874093  7.35031869 -1.        ]]\n",
      "-59.27999999999997\n",
      "[[23.5        20.6        21.         35.31134676 33.80458456 -1.        ]]\n",
      "-93.88\n",
      "[[24.3        23.9        23.         42.22221299 42.52408333 -1.        ]]\n",
      "-88.75999999999999\n",
      "[[ 5.9        13.8         8.         46.81986089 50.45460943 -1.        ]]\n",
      "4.039999999999992\n",
      "[[17.8   13.6   16.    24.717 28.896 -1.   ]]\n",
      "-77.51999999999998\n",
      "[[18.8         5.7        17.          9.84963814 45.88415179 -1.        ]]\n",
      "-109.6\n",
      "[[ 3.5        14.2        17.          5.73286122 35.60901998 -1.        ]]\n",
      "21.640000000000015\n",
      "[[ 6.5        14.4        17.         15.29288569 46.67940263 -1.        ]]\n",
      "1.8800000000000239\n",
      "[[11.6        16.3        17.         13.40449466 30.41595858 -1.        ]]\n",
      "-26.72\n",
      "[[21.3        15.7        17.         16.35333812  0.98299042 -1.        ]]\n",
      "-94.6\n",
      "[[21.3        24.9        17.         13.60426494 41.78233276 -1.        ]]\n",
      "-65.16000000000003\n",
      "[[11.9        7.2       17.         6.9019978 41.1698973 -1.       ]]\n",
      "-57.879999999999995\n",
      "[[ 5.5        27.         18.          9.14231136 18.78427787 -1.        ]]\n",
      "49.0\n",
      "[[ 6.6        22.3        18.         26.27068991 30.29221961 -1.        ]]\n",
      "26.480000000000018\n",
      "[[18.3   10.3   18.     1.611 18.319 -1.   ]]\n",
      "-91.48000000000002\n",
      "[[13.2   13.    18.    18.293 31.581 -1.   ]]\n",
      "-48.16\n",
      "[[19.1        18.4        18.         17.71598099  5.97030667 -1.        ]]\n",
      "-71.0\n",
      "[[ 9.1         9.9        18.          4.47431991 19.70156135 -1.        ]]\n",
      "-30.19999999999999\n",
      "[[ 1.1       25.7       18.        30.2521175 21.7638176 -1.       ]]\n",
      "74.75999999999999\n",
      "[[ 2.2        11.8        18.         26.47253359  9.6413702  -1.        ]]\n",
      "22.799999999999997\n",
      "[[ 9.5        7.6       18.         9.3600121  9.4801672 -1.       ]]\n",
      "-40.28\n",
      "[[14.         42.6        18.         50.43247007 32.48660683 -1.        ]]\n",
      "41.120000000000005\n",
      "[[16.4        30.9        20.          6.55805482 50.23382051 -1.        ]]\n",
      "-12.639999999999986\n",
      "[[ 5.          3.4        20.          6.58652012 45.55840416 -1.        ]]\n",
      "-23.119999999999997\n",
      "[[ 5.5         8.8        20.         11.57527501 39.36913454 -1.        ]]\n",
      "-9.240000000000009\n",
      "[[12.8         5.8        20.          8.28438842 31.35000087 -1.        ]]\n",
      "-68.48\n",
      "[[15.         27.         21.         38.02933115 58.80825538 -1.        ]]\n",
      "-15.599999999999966\n",
      "[[12.4        19.6         8.         34.15497044 28.4326665  -1.        ]]\n",
      "-21.599999999999994\n",
      "[[14.7        17.5         9.         27.31684034 43.03199133 -1.        ]]\n",
      "-43.96000000000001\n",
      "[[11.4   15.2    9.     5.574 51.895 -1.   ]]\n",
      "-28.879999999999995\n",
      "[[18.8        10.7         9.          6.0270124  43.57168563 -1.        ]]\n",
      "-93.6\n",
      "[[15.6        13.          9.         22.95862369 27.14219975 -1.        ]]\n",
      "-64.48000000000002\n",
      "[[11.5        29.4        10.         49.49310894 29.24019998 -1.        ]]\n",
      "15.879999999999995\n",
      "[[ 6.1         7.3        16.         40.26618489 38.74379602 -1.        ]]\n",
      "-18.11999999999999\n",
      "[[11.8    8.7   17.    25.792 31.697 -1.   ]]\n",
      "-52.400000000000006\n",
      "[[21.2         9.6        17.         14.06649318 30.01413141 -1.        ]]\n",
      "-113.43999999999997\n",
      "[[ 4.8   12.2   17.     6.447 43.316 -1.   ]]\n",
      "6.400000000000006\n",
      "[[13.2   10.7   17.     8.643 45.665 -1.   ]]\n",
      "-55.51999999999998\n",
      "[[ 3.4         8.9        17.         15.83368964 50.73191244 -1.        ]]\n",
      "5.359999999999999\n",
      "[[13.8    6.5   18.    10.254 35.597 -1.   ]]\n",
      "-73.03999999999999\n",
      "[[ 9.          5.8        18.          7.31383158 34.42506423 -1.        ]]\n",
      "-42.64\n",
      "[[15.5        18.6        18.          2.37873438 31.63463385 -1.        ]]\n",
      "-45.879999999999995\n",
      "[[14.1         4.4        18.          5.02860547 41.32158658 -1.        ]]\n",
      "-81.8\n",
      "[[ 8.1        13.         18.          6.82256745 36.60002926 -1.        ]]\n",
      "-13.480000000000018\n",
      "[[ 7.          6.1        18.          7.43630904 35.63570618 -1.        ]]\n",
      "-28.08\n",
      "[[11.         20.6        18.          2.06835655  4.89573121 -1.        ]]\n",
      "-8.879999999999995\n",
      "[[15.3         9.2        18.          2.30528591 28.80691203 -1.        ]]\n",
      "-74.6\n",
      "[[ 8.1        23.4        18.         10.66636    13.87538693 -1.        ]]\n",
      "19.80000000000001\n",
      "[[10.    16.7   19.     7.216 26.372 -1.   ]]\n",
      "-14.560000000000002\n",
      "[[ 6.         20.9        19.         19.75880494 15.85576125 -1.        ]]\n",
      "26.080000000000013\n",
      "[[ 8.2        21.         20.          4.94519289 11.87822231 -1.        ]]\n",
      "11.439999999999998\n",
      "[[11.7         9.8        20.          9.05151733 13.55731501 -1.        ]]\n",
      "-48.19999999999999\n",
      "[[ 8.9        23.9        20.         23.65770678  6.59425757 -1.        ]]\n",
      "15.960000000000036\n",
      "driver reward  -1108.4399999999996\n",
      "[[ 4.8        20.3         9.         32.30992297 54.26509248 40.        ]]\n",
      "32.31999999999999\n",
      "[[20.4        16.6        11.         25.19512649 48.35272262 39.        ]]\n",
      "-85.6\n",
      "[[10.8        18.4        11.         32.16302333 42.72319452 38.        ]]\n",
      "-14.560000000000002\n",
      "[[26.6         2.8        13.         12.47882528 57.26856966 37.        ]]\n",
      "-171.92000000000002\n",
      "[[ 4.    15.3   13.     4.41  39.854 36.   ]]\n",
      "21.75999999999999\n",
      "[[18.9   11.    13.    11.906 60.    35.   ]]\n",
      "-93.32\n",
      "[[13.5        10.1        14.         11.22938799 40.68218878 34.        ]]\n",
      "-59.48000000000002\n",
      "[[ 3.    10.4   14.    13.864 51.747 33.   ]]\n",
      "12.879999999999995\n",
      "[[ 7.1        16.         14.         11.43143741 34.35208904 32.        ]]\n",
      "2.9199999999999875\n",
      "[[ 8.3        14.1        14.         27.07137257 50.18565803 31.        ]]\n",
      "-11.319999999999993\n",
      "[[12.3         5.         15.         22.59998137 57.7463727  30.        ]]\n",
      "-67.64\n",
      "[[ 4.2        27.8        15.         45.45695948 51.74676998 29.        ]]\n",
      "60.400000000000006\n",
      "[[ 8.         21.          9.         45.27991012 28.64975588 28.        ]]\n",
      "12.800000000000011\n",
      "[[ 1.8        15.9         9.         32.67743136 35.77009546 27.        ]]\n",
      "38.640000000000015\n",
      "[[19.5        12.9         9.         13.85055165 37.43819866 26.        ]]\n",
      "-91.32\n",
      "[[16.9   30.5   10.    24.112 28.315 25.   ]]\n",
      "-17.319999999999993\n",
      "[[11.3        11.8        10.         16.77563035 48.73413438 24.        ]]\n",
      "-39.08000000000001\n",
      "[[ 9.5         1.         10.         21.2121773  39.27001178 23.        ]]\n",
      "-61.39999999999999\n",
      "[[ 8.2    7.5   10.    20.017 53.672 22.   ]]\n",
      "-31.75999999999999\n",
      "[[14.8        15.8        11.         21.28106577 53.53767558 21.        ]]\n",
      "-50.08000000000001\n",
      "[[14.2         9.         11.         34.2163198  50.53500759 20.        ]]\n",
      "-67.75999999999999\n",
      "[[ 7.1         8.1        11.         42.83995184 59.29999229 19.        ]]\n",
      "-22.36\n",
      "[[21.7         7.1        18.         27.93056281 57.96539002 18.        ]]\n",
      "-124.83999999999997\n",
      "[[11.8        21.8        18.         40.43406961 31.7562389  17.        ]]\n",
      "-10.47999999999999\n",
      "[[13.8   18.9    7.    10.737 25.784 16.   ]]\n",
      "-33.360000000000014\n",
      "[[10.         30.1         7.         29.67736689 10.5486055  15.        ]]\n",
      "28.319999999999993\n",
      "[[16.    12.7    8.     6.799 19.003 14.   ]]\n",
      "-68.16\n",
      "[[ 2.8         5.6         8.          9.55041497 19.90515619 13.        ]]\n",
      "-1.1199999999999903\n",
      "[[ 3.1        36.8         9.         10.44786095 59.22723281 12.        ]]\n",
      "96.68\n",
      "[[ 9.3        32.5         9.         34.84066364 28.25557846 11.        ]]\n",
      "40.76000000000005\n",
      "[[22.2   24.6   11.     6.453 18.258 10.   ]]\n",
      "-72.23999999999995\n",
      "[[ 6.         32.6        11.         28.38029199 39.32056244  9.        ]]\n",
      "63.51999999999998\n",
      "[[ 1.7        13.1        11.         32.69197366 51.57009833  8.        ]]\n",
      "30.360000000000014\n",
      "[[23.4        19.4        12.         30.72000365 46.02035105  7.        ]]\n",
      "-97.03999999999996\n",
      "[[ 6.7        22.         14.         10.23703998 26.09864785  6.        ]]\n",
      "24.840000000000003\n",
      "[[13.2         9.7        14.          4.19235614 24.16718543  5.        ]]\n",
      "-58.72\n",
      "[[10.4        14.6        14.          4.58866586 19.9284964   4.        ]]\n",
      "-24.0\n",
      "[[17.5   10.5   14.     8.347 13.663  3.   ]]\n",
      "-85.4\n",
      "[[ 3.6        10.5        14.          8.02218108 22.49195539  2.        ]]\n",
      "9.120000000000005\n",
      "[[ 7.5        19.6        14.         25.1074995  22.56144684  1.        ]]\n",
      "1511.72\n",
      "[[16.7        25.6        15.         20.38638073  4.58428148  0.        ]]\n",
      "-31.639999999999986\n",
      "[[27.          6.2        15.          9.59458641 22.85864347 -1.        ]]\n",
      "-163.76000000000002\n",
      "[[17.          7.4        15.          4.58704729 12.51735051 -1.        ]]\n",
      "-91.91999999999999\n",
      "[[ 5.2        35.3        15.         42.56738584 19.35270657 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 6.5   40.2    8.     9.019 30.436 -1.   ]]\n",
      "84.44\n",
      "[[11.2    1.7    9.     3.426 41.98  -1.   ]]\n",
      "-70.71999999999998\n",
      "[[ 5.1        31.7         9.         36.91912382 57.01097464 -1.        ]]\n",
      "66.76000000000002\n",
      "[[11.5    6.5   18.    24.97  49.809 -1.   ]]\n",
      "-57.39999999999999\n",
      "[[14.8    2.4   18.    12.729 51.493 -1.   ]]\n",
      "-92.96\n",
      "[[ 4.7        16.4        18.         24.62874688 47.64730065 -1.        ]]\n",
      "20.52000000000001\n",
      "[[26.8        31.3        18.         26.864213    7.71643909 -1.        ]]\n",
      "-82.07999999999998\n",
      "[[21.9         8.8        19.         16.3660459  15.63835443 -1.        ]]\n",
      "-120.75999999999999\n",
      "[[18.6         8.5        19.          0.74768174 22.47101355 -1.        ]]\n",
      "-99.28\n",
      "[[ 7.         26.4        19.         20.12121533  5.66670173 -1.        ]]\n",
      "36.880000000000024\n",
      "[[11.6   26.8   20.    11.095 39.292 -1.   ]]\n",
      "6.8799999999999955\n",
      "[[ 7.3   10.    20.     2.193 42.612 -1.   ]]\n",
      "-17.64\n",
      "[[ 3.9        32.4        20.         33.68697164 31.40708516 -1.        ]]\n",
      "77.16000000000003\n",
      "[[11.5    9.7   22.    19.13  25.187 -1.   ]]\n",
      "-47.16\n",
      "[[15.8        18.9         0.         24.53346908 34.47901185 -1.        ]]\n",
      "-46.96000000000001\n",
      "[[16.4        12.3         5.         31.72139293 43.97673334 -1.        ]]\n",
      "-72.16\n",
      "[[ 1.4    5.9    6.    26.613 39.061 -1.   ]]\n",
      "9.36\n",
      "[[29.7        18.1         6.          8.06704889 39.95344571 -1.        ]]\n",
      "-144.03999999999996\n",
      "[[ 3.5        13.7         7.         15.67530809 50.13580015 -1.        ]]\n",
      "20.040000000000006\n",
      "[[12.7        29.3         7.         36.95323225 59.17856342 -1.        ]]\n",
      "7.400000000000034\n",
      "[[17.1        11.8         8.         17.38132457 37.81783255 -1.        ]]\n",
      "-78.52000000000001\n",
      "[[ 6.6   17.3    8.     4.221 43.586 -1.   ]]\n",
      "10.480000000000018\n",
      "[[ 0.6        18.          8.          9.84592984 25.87197691 -1.        ]]\n",
      "53.519999999999996\n",
      "[[ 5.3   23.     8.     4.678 47.865 -1.   ]]\n",
      "37.56\n",
      "[[ 3.5        24.7         8.          5.42978686 23.96164719 -1.        ]]\n",
      "55.24000000000001\n",
      "[[14.4         8.9         9.          9.93040375 28.0355616  -1.        ]]\n",
      "-69.44\n",
      "[[ 3.         30.1         9.         32.4369614  43.84742933 -1.        ]]\n",
      "75.91999999999999\n",
      "[[15.8        12.         11.         28.57188098 46.67001474 -1.        ]]\n",
      "-69.03999999999999\n",
      "[[ 4.9         6.1        11.         31.00516224 52.55525491 -1.        ]]\n",
      "-13.799999999999997\n",
      "[[15.2   22.    11.     4.84  38.824 -1.   ]]\n",
      "-32.96000000000001\n",
      "[[ 2.2        18.         11.         10.6595841  53.59718523 -1.        ]]\n",
      "42.640000000000015\n",
      "[[ 5.2        12.2        12.         10.87301342 45.00342405 -1.        ]]\n",
      "3.680000000000007\n",
      "[[ 5.8         2.2        12.          7.58200795 39.34311099 -1.        ]]\n",
      "-32.4\n",
      "[[17.         21.5        12.         23.13317867 21.37720057 -1.        ]]\n",
      "-46.80000000000001\n",
      "[[ 9.9        15.7        12.          4.26971595  6.56644854 -1.        ]]\n",
      "-17.080000000000013\n",
      "[[ 7.3         6.3        14.          5.07448041  8.61598926 -1.        ]]\n",
      "-29.47999999999999\n",
      "[[ 6.5         9.9        16.         15.69415927  6.38195321 -1.        ]]\n",
      "-12.519999999999982\n",
      "[[ 6.2         2.6        16.         10.78809345 11.06423109 -1.        ]]\n",
      "-33.84\n",
      "[[ 6.9        16.         16.         23.01096654 12.82058837 -1.        ]]\n",
      "4.280000000000001\n",
      "[[23.6   11.1   18.     1.383 21.82  -1.   ]]\n",
      "-124.96000000000001\n",
      "[[ 7.2        13.5        18.         13.90866528 38.3001829  -1.        ]]\n",
      "-5.759999999999991\n",
      "[[ 8.5        15.6        18.         24.01715186 46.13743384 -1.        ]]\n",
      "-7.8799999999999955\n",
      "[[20.5   16.    18.     9.602 13.276 -1.   ]]\n",
      "-88.19999999999999\n",
      "[[18.8         7.         18.         11.07934838 38.14579003 -1.        ]]\n",
      "-105.44\n",
      "[[ 8.5        14.6        18.         10.58523244 21.55108555 -1.        ]]\n",
      "-11.080000000000013\n",
      "[[ 7.3       15.4       18.        30.0639995 32.2795987 -1.       ]]\n",
      "-0.3599999999999852\n",
      "[[ 6.4   34.    18.    31.689  4.699 -1.   ]]\n",
      "65.28000000000003\n",
      "[[22.         12.5        21.         27.23151363 13.75848411 -1.        ]]\n",
      "-109.6\n",
      "[[ 6.8    6.1    1.    17.725 21.939 -1.   ]]\n",
      "-26.719999999999985\n",
      "[[18.5        19.7         5.          5.93681893 29.871514   -1.        ]]\n",
      "-62.75999999999999\n",
      "[[ 2.9    8.7    5.     5.522 41.079 -1.   ]]\n",
      "8.120000000000005\n",
      "[[12.5         8.5         5.          5.50994188 45.45312484 -1.        ]]\n",
      "-57.79999999999998\n",
      "[[ 5.8        10.6         5.          1.85429649 37.21325159 -1.        ]]\n",
      "-5.519999999999982\n",
      "[[ 3.7        19.6         6.          5.31070741 56.2310067  -1.        ]]\n",
      "37.56\n",
      "[[ 6.4        13.3         6.         18.8281707  41.99662404 -1.        ]]\n",
      "-0.960000000000008\n",
      "[[10.         26.8         7.         52.93656265 50.66680839 -1.        ]]\n",
      "17.76000000000002\n",
      "[[12.2        21.3        15.         46.32849791 19.06776801 -1.        ]]\n",
      "-14.799999999999983\n",
      "[[16.4        14.         20.         20.85393063 25.70199323 -1.        ]]\n",
      "-66.72\n",
      "[[ 7.6        15.1        20.         23.78060949  8.07194107 -1.        ]]\n",
      "-3.359999999999985\n",
      "driver reward  -920.4399999999997\n",
      "[[ 6.8   21.3    9.     1.126 23.789 40.   ]]\n",
      "21.919999999999987\n",
      "[[13.8         3.6        11.         14.78364241 25.79535419 39.        ]]\n",
      "-82.32000000000001\n",
      "[[ 4.8         7.5        11.         21.14615271 34.54318084 38.        ]]\n",
      "-8.64\n",
      "[[ 9.5   14.4   11.    12.644 56.793 37.   ]]\n",
      "-18.519999999999982\n",
      "[[ 1.6        20.1        12.         33.16158278 53.34811225 36.        ]]\n",
      "53.44\n",
      "[[ 3.3   22.1   13.    13.867 36.995 35.   ]]\n",
      "48.28\n",
      "[[ 5.5         5.9        13.          9.68971461 47.52726159 34.        ]]\n",
      "-18.519999999999996\n",
      "[[ 4.9        32.3        13.         41.7433306  30.04729416 33.        ]]\n",
      "70.04000000000005\n",
      "[[ 7.7        28.8        20.         53.22145488  8.64956664 32.        ]]\n",
      "39.80000000000001\n",
      "[[ 2.9         6.1         8.         48.27135911 15.75757141 31.        ]]\n",
      "-0.19999999999999574\n",
      "[[14.1   42.6   11.    22.036 30.844 30.   ]]\n",
      "40.44\n",
      "[[ 1.5        18.7        12.         16.27869454 47.85361327 29.        ]]\n",
      "49.640000000000015\n",
      "[[ 8.3         7.8        12.         22.59007153 49.77598184 28.        ]]\n",
      "-31.480000000000004\n",
      "[[11.         14.6        12.         19.15214625 55.69217982 27.        ]]\n",
      "-28.080000000000013\n",
      "[[10.1        24.7        12.         21.61443451 36.60901182 26.        ]]\n",
      "10.360000000000014\n",
      "[[11.2        13.4        12.          3.12380997 22.03191833 25.        ]]\n",
      "-33.28\n",
      "[[ 8.2        13.         12.         14.37308493 29.16278626 24.        ]]\n",
      "-14.159999999999997\n",
      "[[18.1         4.5        12.          5.22902946 41.16643921 23.        ]]\n",
      "-108.68\n",
      "[[ 6.3        15.9        12.         26.1461981  47.51552312 22.        ]]\n",
      "8.04000000000002\n",
      "[[19.8        25.6        12.          8.96903197 23.11418275 21.        ]]\n",
      "-52.72000000000003\n",
      "[[ 7.2        13.9        13.         10.86089333 30.79029203 20.        ]]\n",
      "-4.480000000000018\n",
      "[[19.4        18.         13.         13.96608847 34.8553894  19.        ]]\n",
      "-74.32\n",
      "[[16.6        19.3        13.          8.34318118 29.4916591  18.        ]]\n",
      "-51.12000000000003\n",
      "[[ 8.         17.3        13.         25.27311976 31.90668563 17.        ]]\n",
      "0.960000000000008\n",
      "[[23.          4.2        13.         12.65977818 47.1214096  16.        ]]\n",
      "-142.95999999999998\n",
      "[[ 4.8        10.5        13.         11.44283077 37.6390886  15.        ]]\n",
      "0.9599999999999937\n",
      "[[11.4        34.1        13.         37.73893491 33.12362746 14.        ]]\n",
      "31.600000000000023\n",
      "[[ 1.6    7.7   14.    43.951 38.607 13.   ]]\n",
      "13.759999999999998\n",
      "[[15.5        26.5        19.          7.03255661 58.61991353 12.        ]]\n",
      "-20.599999999999966\n",
      "[[ 5.5        20.1        19.         22.34247298 49.7049571  11.        ]]\n",
      "26.919999999999987\n",
      "[[19.7        19.         20.         15.88018179 58.34948176 10.        ]]\n",
      "-73.16000000000003\n",
      "[[21.4         7.5        20.          3.59019178 48.83586562  9.        ]]\n",
      "-121.51999999999998\n",
      "[[12.9        16.9        20.         19.46750896 49.54186021  8.        ]]\n",
      "-33.639999999999986\n",
      "[[13.4        10.8        20.          8.66780096 44.64004973  7.        ]]\n",
      "-56.56\n",
      "[[ 9.2        12.         20.          2.47979469 49.29496521  6.        ]]\n",
      "-24.159999999999997\n",
      "[[11.3         8.2        20.         14.65910384 59.84012288  5.        ]]\n",
      "-50.599999999999994\n",
      "[[14.1        49.3        21.         35.56274631 10.77262667  4.        ]]\n",
      "61.879999999999995\n",
      "[[10.8        19.3        12.         41.81051164 22.77650908  3.        ]]\n",
      "-11.680000000000007\n",
      "[[ 7.     7.8   12.    39.094 21.868  2.   ]]\n",
      "-22.64\n",
      "[[ 3.6        17.7        17.         24.95137896 24.56335119  1.        ]]\n",
      "1532.16\n",
      "[[20.1        19.6        17.         17.25224282 16.84343245  0.        ]]\n",
      "-73.96000000000004\n",
      "[[24.5        23.3        18.         23.21939617 22.96328577 -1.        ]]\n",
      "-92.03999999999996\n",
      "[[12.7         0.7        19.         20.34716829 34.74731531 -1.        ]]\n",
      "-84.11999999999999\n",
      "[[21.2        22.1        19.          5.36443925 48.17876052 -1.        ]]\n",
      "-73.44\n",
      "[[ 3.3        18.4        19.         21.47491045 41.83325292 -1.        ]]\n",
      "36.44\n",
      "[[23.         18.5        20.         24.33505237 57.08607607 -1.        ]]\n",
      "-97.19999999999999\n",
      "[[22.9         8.7        20.          9.70190721 53.5460454  -1.        ]]\n",
      "-127.87999999999997\n",
      "[[10.2        35.1        20.         24.50378597 15.70292933 -1.        ]]\n",
      "42.960000000000036\n",
      "[[27.3         5.1        20.          3.3512532  39.12874935 -1.        ]]\n",
      "-169.32\n",
      "[[23.6        21.5        21.         26.39749613 39.20172615 -1.        ]]\n",
      "-91.68\n",
      "[[19.9        10.3        21.          1.82648444 54.9238975  -1.        ]]\n",
      "-102.35999999999999\n",
      "[[ 5.1        16.8        21.         16.51257317 39.53162301 -1.        ]]\n",
      "19.080000000000013\n",
      "[[22.7         4.8        21.         10.4668921  57.12417604 -1.        ]]\n",
      "-139.0\n",
      "[[10.4         7.8        22.          0.25864886 43.49369647 -1.        ]]\n",
      "-45.75999999999999\n",
      "[[ 6.4        15.7        22.         18.902406   48.89878296 -1.        ]]\n",
      "6.719999999999999\n",
      "[[17.1        32.9         1.         23.73020425 10.05966442 -1.        ]]\n",
      "-11.0\n",
      "[[18.8        13.2         6.          1.15694116 16.74542938 -1.        ]]\n",
      "-85.6\n",
      "[[ 8.6   25.9    7.     9.643 32.642 -1.   ]]\n",
      "24.400000000000006\n",
      "[[ 0.4         9.9         7.         18.8815343  34.94135315 -1.        ]]\n",
      "28.959999999999994\n",
      "[[18.2        10.6         7.          4.89998453 57.61750428 -1.        ]]\n",
      "-89.83999999999997\n",
      "[[ 8.1         2.9         7.          4.22884801 48.19972672 -1.        ]]\n",
      "-45.8\n",
      "[[11.         31.8         7.         36.79112015 20.46832595 -1.        ]]\n",
      "26.960000000000036\n",
      "[[13.9   20.7    9.     7.709 16.69  -1.   ]]\n",
      "-28.28\n",
      "[[ 5.          4.2         9.          3.95906561 14.95269176 -1.        ]]\n",
      "-20.559999999999995\n",
      "[[ 3.8        14.3         9.         18.53128096  5.11875981 -1.        ]]\n",
      "19.919999999999987\n",
      "[[18.6        18.8        12.         22.18875497 28.91251534 -1.        ]]\n",
      "-66.32000000000002\n",
      "[[ 3.2   17.8   13.     3.244 19.73  -1.   ]]\n",
      "35.20000000000002\n",
      "[[ 7.2        17.3        13.          0.53410642 32.10193098 -1.        ]]\n",
      "6.400000000000006\n",
      "[[ 9.7         2.         14.          8.67168804 36.37432159 -1.        ]]\n",
      "-59.55999999999999\n",
      "[[19.          6.         14.          3.16485597 53.89189251 -1.        ]]\n",
      "-110.0\n",
      "[[ 6.5         7.7        14.          2.3362883  52.95608609 -1.        ]]\n",
      "-19.559999999999988\n",
      "[[ 7.6        26.6        14.         26.97549094 29.23582542 -1.        ]]\n",
      "33.44\n",
      "[[ 3.3  13.1  14.   13.66 36.26 -1.  ]]\n",
      "19.480000000000018\n",
      "[[14.5        18.2        14.         16.7233936  59.05080428 -1.        ]]\n",
      "-40.360000000000014\n",
      "[[10.4         1.4        15.          6.78199975 57.82229632 -1.        ]]\n",
      "-66.24000000000001\n",
      "[[ 7.1        16.7        17.          3.64532307 34.2854898  -1.        ]]\n",
      "5.160000000000025\n",
      "[[12.5         4.3        17.         14.14738446 47.1967225  -1.        ]]\n",
      "-71.24\n",
      "[[12.1        42.4        17.         38.94424905 18.64295261 -1.        ]]\n",
      "53.400000000000034\n",
      "[[21.         17.9        19.         10.74259549 13.04087416 -1.        ]]\n",
      "-85.51999999999998\n",
      "[[13.5   12.5   19.     6.895 33.974 -1.   ]]\n",
      "-51.79999999999998\n",
      "[[15.8        15.6        19.          9.90501082 35.06803713 -1.        ]]\n",
      "-57.51999999999998\n",
      "[[ 9.         26.6        20.         30.13072857 29.42071058 -1.        ]]\n",
      "23.919999999999987\n",
      "[[ 6.2         1.9        20.         33.20491113 33.38934222 -1.        ]]\n",
      "-36.08\n",
      "[[ 7.2        12.1        22.         49.55693472 36.21583069 -1.        ]]\n",
      "-10.240000000000009\n",
      "[[ 3.4        11.4         7.         38.01267262 35.58598613 -1.        ]]\n",
      "13.36\n",
      "[[ 7.5        43.          7.          1.66712865  0.64328555 -1.        ]]\n",
      "86.60000000000002\n",
      "[[17.9        16.9        12.          0.49395362 24.13876948 -1.        ]]\n",
      "-67.63999999999999\n",
      "[[ 8.7         6.         12.          5.56922477 35.94310134 -1.        ]]\n",
      "-39.959999999999994\n",
      "[[ 5.5        31.7        12.         22.88190127 12.99812741 -1.        ]]\n",
      "64.03999999999999\n",
      "[[ 4.2   13.4   14.     5.374 13.397 -1.   ]]\n",
      "14.319999999999993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.2        14.3        14.          3.64513722  1.48997072 -1.        ]]\n",
      "24.0\n",
      "[[ 1.6        32.2        18.         32.00255721 19.90878154 -1.        ]]\n",
      "92.15999999999997\n",
      "[[21.6   14.3   19.     4.415 42.828 -1.   ]]\n",
      "-101.12000000000003\n",
      "[[ 5.         10.2        19.         19.26526735 39.83272157 -1.        ]]\n",
      "-1.3599999999999994\n",
      "[[19.9        15.5        19.          4.66466757 38.2829598  -1.        ]]\n",
      "-85.71999999999997\n",
      "[[ 7.9        19.5        19.         13.08951646 26.50840711 -1.        ]]\n",
      "8.680000000000007\n",
      "[[14.5         5.4        19.          6.96968355  8.80327198 -1.        ]]\n",
      "-81.32\n",
      "[[ 6.         11.8        19.         20.73871048 10.71191705 -1.        ]]\n",
      "-3.0400000000000063\n",
      "[[ 9.4        31.5        19.         32.82506277 45.30042964 -1.        ]]\n",
      "36.879999999999995\n",
      "[[12.8        10.1        20.         18.42053292 48.94191218 -1.        ]]\n",
      "-54.72\n",
      "[[ 9.4   23.5   20.     9.988 21.047 -1.   ]]\n",
      "11.28000000000003\n",
      "[[ 7.3   26.7   20.    26.138  1.812 -1.   ]]\n",
      "35.80000000000001\n",
      "[[20.9        14.6        21.         11.18619875 17.34078544 -1.        ]]\n",
      "-95.4\n",
      "[[13.6        31.8        21.         31.71925571  8.8805083  -1.        ]]\n",
      "9.28000000000003\n",
      "[[13.         24.5         8.         28.44253731 37.06350008 -1.        ]]\n",
      "-10.0\n",
      "[[ 1.7        12.5         8.         23.03567673 24.23116427 -1.        ]]\n",
      "28.440000000000012\n",
      "[[ 4.3   15.4    8.     5.29  28.918 -1.   ]]\n",
      "20.04000000000002\n",
      "[[ 5.9         8.          8.         12.7098953  39.13605681 -1.        ]]\n",
      "-14.519999999999996\n",
      "[[ 3.3        26.9         8.         32.75312083 23.74094657 -1.        ]]\n",
      "63.640000000000015\n",
      "[[ 7.6        29.6         9.         44.11811247  3.60712727 -1.        ]]\n",
      "43.03999999999999\n",
      "[[29.7        14.7        15.         30.80689778 11.51708724 -1.        ]]\n",
      "-154.91999999999996\n",
      "[[16.2        17.2        16.         29.43859795 24.47700355 -1.        ]]\n",
      "-55.119999999999976\n",
      "[[ 8.7   26.2   17.     0.134 37.586 -1.   ]]\n",
      "24.680000000000007\n",
      "[[13.8        16.5        17.         14.57556478 32.91919845 -1.        ]]\n",
      "-41.03999999999999\n",
      "[[10.7        10.4        17.         13.41895233 34.41374374 -1.        ]]\n",
      "-39.48000000000002\n",
      "[[17.1         5.2        17.          1.79923679 43.8778617  -1.        ]]\n",
      "-99.64000000000001\n",
      "[[15.5        21.5        17.          7.30175088  7.43449465 -1.        ]]\n",
      "-36.599999999999994\n",
      "[[10.9        15.4        18.          8.65261221  3.23225697 -1.        ]]\n",
      "-24.840000000000003\n",
      "[[13.8   17.1   18.    23.006 30.602 -1.   ]]\n",
      "-39.120000000000005\n",
      "[[ 2.7        15.3        18.         35.00429627 26.47183565 -1.        ]]\n",
      "30.60000000000001\n",
      "[[ 5.2   15.9   18.    22.143 17.407 -1.   ]]\n",
      "15.519999999999982\n",
      "[[18.7        20.7        19.         29.14535144 15.37754878 -1.        ]]\n",
      "-60.91999999999996\n",
      "[[ 6.8        17.6        21.         17.96224165 12.77064964 -1.        ]]\n",
      "10.079999999999984\n",
      "[[24.         16.1        21.         13.65548988 43.41406586 -1.        ]]\n",
      "-111.68\n",
      "[[12.7        27.4        21.         22.25814524 30.98182109 -1.        ]]\n",
      "1.32000000000005\n",
      "[[24.8        10.1        21.          8.06781954 38.56756187 -1.        ]]\n",
      "-136.32\n",
      "[[10.9        14.1        22.         19.21664555 20.5093956  -1.        ]]\n",
      "-29.0\n",
      "[[ 4.          6.1        22.         10.90357297 16.09331662 -1.        ]]\n",
      "-7.679999999999993\n",
      "driver reward  -1501.0799999999995\n",
      "[[ 4.3        35.6         6.         16.45340871 28.87283074 40.        ]]\n",
      "84.68\n",
      "[[ 5.          2.4         7.         17.60899296 26.472182   39.        ]]\n",
      "-26.32\n",
      "[[ 5.2        21.4         8.         24.94559287 40.91664577 38.        ]]\n",
      "33.12000000000003\n",
      "[[ 1.7         5.7         9.         30.88540023 45.30227992 37.        ]]\n",
      "6.68\n",
      "[[21.5        14.1        12.         19.41065876 45.33486769 36.        ]]\n",
      "-101.08000000000001\n",
      "[[ 6.7         5.6        12.          9.35010326 40.58613623 35.        ]]\n",
      "-27.64\n",
      "[[ 7.4         9.4        13.          6.45898145 40.24724    34.        ]]\n",
      "-20.239999999999995\n",
      "[[ 6.         26.5        13.         11.91317199 16.18787663 33.        ]]\n",
      "44.0\n",
      "[[14.6        11.2        14.         11.32092175 41.63051772 32.        ]]\n",
      "-63.43999999999997\n",
      "[[ 5.1        13.8        14.         13.61454627 29.98145518 31.        ]]\n",
      "9.480000000000018\n",
      "[[ 6.5        17.4        14.          8.61625287 52.21444173 30.        ]]\n",
      "11.480000000000018\n",
      "[[11.9        18.5        15.         25.04044924 40.70497158 29.        ]]\n",
      "-21.72\n",
      "[[ 9.7        23.6        15.          1.90367551 30.8054939  28.        ]]\n",
      "9.56000000000003\n",
      "[[11.6        18.3        16.          8.84397943 13.47743974 27.        ]]\n",
      "-20.319999999999993\n",
      "[[ 8.8   24.2   17.    19.971 31.36  26.   ]]\n",
      "17.599999999999994\n",
      "[[ 0.6        43.5        17.         51.2321607   0.83942539 25.        ]]\n",
      "135.12\n",
      "[[ 8.5        39.3         0.          6.07808237 16.48721313 24.        ]]\n",
      "67.96000000000004\n",
      "[[ 3.6        13.4         7.         11.55727385  8.00453779 23.        ]]\n",
      "18.400000000000006\n",
      "[[12.4         2.9         9.          4.56880182 21.2406216  22.        ]]\n",
      "-75.04\n",
      "[[ 9.6         7.4        11.          3.7637914  23.35088174 21.        ]]\n",
      "-41.599999999999994\n",
      "[[ 8.1        22.4        12.         27.86553095 25.97758058 20.        ]]\n",
      "16.599999999999994\n",
      "[[ 5.3    5.6   12.    28.602 26.366 19.   ]]\n",
      "-18.11999999999999\n",
      "[[ 4.5        10.5        14.         32.64940019 35.29895291 18.        ]]\n",
      "3.0\n",
      "[[11.5        24.5        15.         10.96655801 23.00744904 17.        ]]\n",
      "0.20000000000001705\n",
      "[[10.7    6.1   16.    12.663 28.769 16.   ]]\n",
      "-53.23999999999998\n",
      "[[ 7.         12.6        16.         11.40505853 39.10302143 15.        ]]\n",
      "-7.280000000000001\n",
      "[[ 7.    12.8   17.    26.467 32.431 14.   ]]\n",
      "-6.640000000000015\n",
      "[[ 4.3   23.5   17.    26.225 13.209 13.   ]]\n",
      "45.96000000000001\n",
      "[[ 6.6        20.         18.         22.08300399 39.44628893 12.        ]]\n",
      "19.120000000000005\n",
      "[[18.9        39.5        18.         24.96664547 15.56553275 11.        ]]\n",
      "-2.1200000000000045\n",
      "[[14.2        32.7        19.         44.8788339  18.24727028 10.        ]]\n",
      "8.079999999999984\n",
      "[[ 9.    48.8   13.     5.251 60.     9.   ]]\n",
      "94.96000000000004\n",
      "[[12.9         3.6        13.          2.54258266 44.65998289  8.        ]]\n",
      "-76.2\n",
      "[[ 6.         25.8        13.         30.51068821 31.71417081  7.        ]]\n",
      "41.75999999999999\n",
      "[[16.7        11.9        14.         30.04922414 23.17944693  6.        ]]\n",
      "-75.48000000000002\n",
      "[[ 5.2   36.1   15.     5.695 42.959  5.   ]]\n",
      "80.15999999999997\n",
      "[[ 6.4        12.5        16.          6.69093244 36.77567758  4.        ]]\n",
      "-3.519999999999982\n",
      "[[ 9.7        19.5        16.         14.30514506 30.17357739  3.        ]]\n",
      "-3.5600000000000023\n",
      "[[ 5.1         7.5        16.          7.23107923 27.75705403  2.        ]]\n",
      "-10.679999999999993\n",
      "[[ 1.6        16.4        16.         18.89090985 38.71394482  1.        ]]\n",
      "1541.6\n",
      "[[ 4.         26.7        16.         41.15427928 44.55867395  0.        ]]\n",
      "58.24000000000001\n",
      "[[ 3.4   20.1   20.    51.334 58.185 -1.   ]]\n",
      "41.20000000000002\n",
      "[[ 1.6        25.8        12.         38.83210238 35.69157478 -1.        ]]\n",
      "71.67999999999998\n",
      "[[18.3        20.         13.         39.86136326 51.40586198 -1.        ]]\n",
      "-60.44\n",
      "[[11.4        26.2        17.          4.2562781  59.15272949 -1.        ]]\n",
      "6.319999999999993\n",
      "[[ 0.1        27.6        17.         22.47826111 38.57886051 -1.        ]]\n",
      "87.63999999999999\n",
      "[[19.5        21.5        17.         22.61197285 40.57641236 -1.        ]]\n",
      "-63.80000000000001\n",
      "[[24.2   14.    19.     2.553 43.255 -1.   ]]\n",
      "-119.75999999999999\n",
      "[[ 4.9         1.3        19.          3.22547065 39.5327714  -1.        ]]\n",
      "-29.159999999999997\n",
      "[[11.     9.    19.     4.777 29.897 -1.   ]]\n",
      "-46.0\n",
      "[[ 0.8        16.8        19.         18.93398421 20.42893839 -1.        ]]\n",
      "48.31999999999999\n",
      "[[24.6        12.9        19.          2.48005498 49.72737677 -1.        ]]\n",
      "-126.0\n",
      "[[ 6.1        21.8        19.         21.25453623 29.70856972 -1.        ]]\n",
      "28.28\n",
      "[[ 7.3        11.9        19.         25.69008431 28.17941713 -1.        ]]\n",
      "-11.560000000000002\n",
      "[[21.7         4.8        19.          4.98757678 40.87134323 -1.        ]]\n",
      "-132.2\n",
      "[[ 2.4         2.9        20.          8.21884751 42.56154816 -1.        ]]\n",
      "-7.039999999999999\n",
      "[[19.4        30.2        20.         53.06080248 52.88227574 -1.        ]]\n",
      "-35.27999999999997\n",
      "[[ 3.8   11.1   18.    47.64  46.962 -1.   ]]\n",
      "9.680000000000007\n",
      "[[ 1.8        37.5        19.         22.25667285 19.15666121 -1.        ]]\n",
      "107.76000000000005\n",
      "[[21.         11.5        20.         15.4246664  33.56029569 -1.        ]]\n",
      "-106.0\n",
      "[[ 9.9         7.7        20.          2.99664442 35.18570814 -1.        ]]\n",
      "-42.68000000000001\n",
      "[[11.6        21.8        20.         13.82547566 26.98321807 -1.        ]]\n",
      "-9.119999999999976\n",
      "[[14.9   22.4   20.     3.991 42.468 -1.   ]]\n",
      "-29.639999999999986\n",
      "[[ 3.4        19.         21.         16.30894315 52.21742718 -1.        ]]\n",
      "37.68000000000001\n",
      "[[ 4.3        20.5        21.         34.61663513 44.94175346 -1.        ]]\n",
      "36.360000000000014\n",
      "[[ 4.9         8.8        21.         29.98723794 49.93827629 -1.        ]]\n",
      "-5.160000000000011\n",
      "[[21.         41.8        22.         46.69311802 36.15387726 -1.        ]]\n",
      "-9.039999999999964\n",
      "driver reward  1335.5600000000002\n",
      "[[13.1        11.8         1.         35.85631912 26.71715054 40.        ]]\n",
      "-51.31999999999999\n",
      "[[10.2   24.1    7.    13.137 24.582 39.   ]]\n",
      "7.760000000000019\n",
      "[[ 5.2        17.7         8.         10.86263877 44.86478411 38.        ]]\n",
      "21.28\n",
      "[[ 5.6        26.2         8.          3.74133453 17.97640896 37.        ]]\n",
      "45.76000000000002\n",
      "[[ 4.1    9.1    9.    11.656 12.199 36.   ]]\n",
      "1.240000000000009\n",
      "[[ 7.         31.4         9.         33.77318952 40.93843746 35.        ]]\n",
      "52.879999999999995\n",
      "[[ 2.2   20.3   14.    15.088 44.082 34.   ]]\n",
      "50.0\n",
      "[[ 9.2         4.7        14.          6.92525551 35.22915822 33.        ]]\n",
      "-47.51999999999998\n",
      "[[17.7         7.4        15.         19.80450188 43.01110657 32.        ]]\n",
      "-96.68\n",
      "[[10.4         9.8        15.         17.26940993 46.50909291 31.        ]]\n",
      "-39.360000000000014\n",
      "[[ 4.3   29.1   15.    42.764 54.087 30.   ]]\n",
      "63.880000000000024\n",
      "[[ 6.4   35.8   18.     5.361 41.839 29.   ]]\n",
      "71.04000000000002\n",
      "[[18.2        25.4        18.         27.82565541 45.49284639 28.        ]]\n",
      "-42.47999999999996\n",
      "[[ 8.1   32.    18.     5.875 55.38  27.   ]]\n",
      "47.31999999999999\n",
      "[[ 4.3        11.2        18.         11.32727035 41.5330002  26.        ]]\n",
      "6.6000000000000085\n",
      "[[12.1    6.9   19.     6.711 23.607 25.   ]]\n",
      "-60.19999999999999\n",
      "[[ 1.5   11.2   19.     6.679 12.248 24.   ]]\n",
      "25.64\n",
      "[[19.8        42.4        20.         48.47303232 32.35691802 23.        ]]\n",
      "1.0399999999999636\n",
      "[[ 6.3    5.7    7.    45.602 35.787 22.   ]]\n",
      "-24.599999999999994\n",
      "[[ 6.2         9.6         9.         36.80205196 22.80547467 21.        ]]\n",
      "-11.439999999999998\n",
      "[[ 3.8         7.3        11.         42.23763605 29.6914953  20.        ]]\n",
      "-2.4799999999999898\n",
      "[[ 5.8         5.         14.         41.00318929 26.42955439 19.        ]]\n",
      "-23.439999999999998\n",
      "[[ 4.          6.3         6.         36.43639735 18.90396507 18.        ]]\n",
      "-7.040000000000006\n",
      "[[17.6         3.3        13.         22.11464111 27.91672332 17.        ]]\n",
      "-109.12\n",
      "[[22.9        41.7        13.         42.61901078 33.66988241 16.        ]]\n",
      "-22.279999999999973\n",
      "[[ 0.1        13.6        14.         36.13994334 45.60280766 15.        ]]\n",
      "42.84\n",
      "[[ 8.6        14.         14.         30.31185995 31.96373942 14.        ]]\n",
      "-13.680000000000007\n",
      "[[19.1        22.2        16.         15.55866911 55.32808959 13.        ]]\n",
      "-58.839999999999975\n",
      "[[ 3.9        31.1        16.         19.36032489 21.98371349 12.        ]]\n",
      "73.0\n",
      "[[16.6        25.5        16.         31.15078364 23.90037834 11.        ]]\n",
      "-31.28000000000003\n",
      "[[11.2   21.8   17.     3.934 25.559 10.   ]]\n",
      "-6.400000000000006\n",
      "[[ 9.         15.1        17.         24.91048861 14.36084862  9.        ]]\n",
      "-12.879999999999995\n",
      "[[ 1.4         9.2        18.         34.69893732 13.35546741  8.        ]]\n",
      "19.92\n",
      "[[24.5         8.7         9.         10.89935435 29.42460071  7.        ]]\n",
      "-138.76000000000002\n",
      "[[ 6.6        38.9         9.         31.82195924  4.53450822  6.        ]]\n",
      "79.60000000000002\n",
      "[[27.3   28.    12.     8.14  53.255  5.   ]]\n",
      "-96.03999999999996\n",
      "[[15.7        33.1        12.         16.65576226 10.31311149  4.        ]]\n",
      "-0.839999999999975\n",
      "[[ 8.6   17.2   13.     7.77  25.305  3.   ]]\n",
      "-3.4399999999999693\n",
      "[[ 7.         10.9        13.         11.39872977 37.31648574  2.        ]]\n",
      "-12.719999999999985\n",
      "[[ 7.2         6.4        13.         10.77572474 50.08522707  1.        ]]\n",
      "1471.52\n",
      "[[ 6.9        21.3        13.         20.5881804  29.65111333  0.        ]]\n",
      "21.23999999999998\n",
      "[[15.4        17.7        14.          2.92973519 44.71958154 -1.        ]]\n",
      "-48.08000000000001\n",
      "[[12.7         4.3        14.          9.65799785 33.29561557 -1.        ]]\n",
      "-72.6\n",
      "[[ 1.2        10.         14.         16.53626197 27.66844682 -1.        ]]\n",
      "23.840000000000003\n",
      "[[19.6        23.7        14.         22.82739487 27.55037741 -1.        ]]\n",
      "-57.44\n",
      "[[13.9         0.9        15.         14.60024948 37.84966758 -1.        ]]\n",
      "-91.64\n",
      "[[ 2.7        21.3        15.         28.32304888 50.56921269 -1.        ]]\n",
      "49.80000000000001\n",
      "[[ 7.1        11.6        17.         37.10390996 56.14990431 -1.        ]]\n",
      "-11.159999999999997\n",
      "[[10.7        21.3         5.          5.67253816 50.28822672 -1.        ]]\n",
      "-4.599999999999994\n",
      "[[ 6.5        15.8         5.          7.22768811 29.70819211 -1.        ]]\n",
      "6.359999999999985\n",
      "[[14.8        25.8         5.         25.05282703 20.68057659 -1.        ]]\n",
      "-18.079999999999984\n",
      "[[22.1        23.9         6.          4.77967425 55.71839185 -1.        ]]\n",
      "-73.80000000000001\n",
      "[[20.4         8.7         6.         10.6507884  41.24068213 -1.        ]]\n",
      "-110.87999999999997\n",
      "[[20.6         9.          6.          4.26333076 12.32869975 -1.        ]]\n",
      "-111.28\n",
      "[[ 8.7         8.6         7.          2.88589495 18.93168969 -1.        ]]\n",
      "-31.639999999999972\n",
      "[[ 7.1        11.7         8.          5.90361972 23.4892689  -1.        ]]\n",
      "-10.839999999999975\n",
      "[[17.1        15.9         9.          7.24983169 56.2864762  -1.        ]]\n",
      "-65.4\n",
      "[[14.8         5.8         9.          4.00013514 46.78586253 -1.        ]]\n",
      "-82.08000000000001\n",
      "[[15.8         4.7         9.         15.83487143 38.44149301 -1.        ]]\n",
      "-92.4\n",
      "[[ 2.8         3.3         9.         12.81019138 34.28474099 -1.        ]]\n",
      "-8.479999999999997\n",
      "[[ 8.7    6.1    9.    11.033 31.312 -1.   ]]\n",
      "-39.639999999999986\n",
      "[[10.7        19.6         9.         22.25355818 34.11171011 -1.        ]]\n",
      "-10.039999999999992\n",
      "[[ 4.7         5.9        10.         26.95301184 24.58235527 -1.        ]]\n",
      "-13.080000000000013\n",
      "[[ 7.7         6.9        11.         22.8233085  35.57091432 -1.        ]]\n",
      "-30.28\n",
      "[[ 0.6        34.9        12.         47.0297968  11.20136108 -1.        ]]\n",
      "107.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.1         9.         18.         44.1865085  22.92084511 -1.        ]]\n",
      "-19.480000000000004\n",
      "[[20.3        26.9        11.         17.07496837 36.36605137 -1.        ]]\n",
      "-51.960000000000036\n",
      "[[21.4        33.1        11.         54.22914689 43.38050446 -1.        ]]\n",
      "-39.599999999999966\n",
      "[[ 5.5        16.2        13.         38.77687194 47.43724363 -1.        ]]\n",
      "14.439999999999998\n",
      "[[ 5.5   17.8   14.    21.072 45.358 -1.   ]]\n",
      "19.560000000000002\n",
      "[[15.5         8.6        14.          5.95394969 35.86082567 -1.        ]]\n",
      "-77.88\n",
      "[[ 0.8        22.9        14.         27.06759082 42.79017461 -1.        ]]\n",
      "67.84\n",
      "[[ 8.8        13.1        14.          9.48474228 40.3091546  -1.        ]]\n",
      "-17.919999999999987\n",
      "[[ 9.9        18.         14.         26.10656604 29.57202892 -1.        ]]\n",
      "-9.719999999999999\n",
      "[[12.6        30.2        14.         37.73860562  1.78885425 -1.        ]]\n",
      "10.960000000000036\n",
      "[[23.1        10.         16.         12.06343695 16.5177209  -1.        ]]\n",
      "-125.08000000000001\n",
      "[[16.4        23.5        17.         18.74337624 48.1325802  -1.        ]]\n",
      "-36.31999999999999\n",
      "[[ 4.2         2.1        17.         16.51520136 44.41827368 -1.        ]]\n",
      "-21.840000000000003\n",
      "[[20.6        14.9        17.         17.31225057 30.58759573 -1.        ]]\n",
      "-92.4\n",
      "[[14.7         4.7        17.          5.77996444 33.5956674  -1.        ]]\n",
      "-84.91999999999999\n",
      "[[ 7.1        19.8        17.         25.10116203 50.56049228 -1.        ]]\n",
      "15.080000000000013\n",
      "[[26.3        15.         18.         20.69757904 21.77653676 -1.        ]]\n",
      "-130.83999999999997\n",
      "[[25.6        36.2        18.         13.30085974  4.33937223 -1.        ]]\n",
      "-58.24000000000001\n",
      "[[ 9.8        11.         19.         10.48057801 11.87789581 -1.        ]]\n",
      "-31.439999999999998\n",
      "[[19.6   18.7   19.     2.589 49.125 -1.   ]]\n",
      "-73.44\n",
      "[[ 4.2         4.4        20.          7.74447275 51.09089448 -1.        ]]\n",
      "-14.480000000000011\n",
      "[[16.8        13.5        20.          1.06149278 21.59705364 -1.        ]]\n",
      "-71.03999999999999\n",
      "[[ 4.1    3.2   20.     4.455 17.242 -1.   ]]\n",
      "-17.64\n",
      "[[ 6.2        33.2        20.         40.67617972  4.28808661 -1.        ]]\n",
      "64.07999999999998\n",
      "[[ 7.5   34.7   18.     9.076 19.772 -1.   ]]\n",
      "60.039999999999964\n",
      "[[ 8.1         0.8        18.          8.05962807 11.56166958 -1.        ]]\n",
      "-52.52\n",
      "[[ 7.6         9.9        18.          9.41812883 15.82977265 -1.        ]]\n",
      "-20.0\n",
      "[[13.1    5.5   19.     1.534 27.551 -1.   ]]\n",
      "-71.48\n",
      "[[ 5.8    7.4   19.     7.338 31.636 -1.   ]]\n",
      "-15.759999999999991\n",
      "[[14.2        27.2        19.          6.43935815 18.46584303 -1.        ]]\n",
      "-9.519999999999982\n",
      "[[ 5.2        20.2        19.         21.13793929 30.18053202 -1.        ]]\n",
      "29.28\n",
      "[[11.5        21.         20.         21.34099688 46.68324041 -1.        ]]\n",
      "-11.0\n",
      "[[15.3        18.6        20.         22.81115527 24.56067634 -1.        ]]\n",
      "-44.52000000000004\n",
      "[[25.         11.2        22.         12.52319379 46.38369605 -1.        ]]\n",
      "-134.16000000000003\n",
      "[[12.8        18.4        23.         32.00591629 38.82073723 -1.        ]]\n",
      "-28.159999999999997\n",
      "driver reward  -686.1999999999995\n",
      "[[ 5.3    3.3    8.    45.31  17.631 40.   ]]\n",
      "-25.479999999999997\n",
      "[[12.    12.9    8.    44.74  18.843 39.   ]]\n",
      "-40.31999999999999\n",
      "[[ 9.7   26.5    9.    13.792 30.765 38.   ]]\n",
      "18.839999999999975\n",
      "[[ 6.5        11.7         9.          5.12097303 33.59808169 37.        ]]\n",
      "-6.759999999999991\n",
      "[[ 9.5    4.8    9.     8.128 27.453 36.   ]]\n",
      "-49.24000000000001\n",
      "[[11.8        13.9        11.          2.02850098 43.35154998 35.        ]]\n",
      "-35.76000000000002\n",
      "[[11.6        11.2        11.         16.76550723 58.48946623 34.        ]]\n",
      "-43.039999999999964\n",
      "[[11.6    7.1   11.    13.772 55.034 33.   ]]\n",
      "-56.16\n",
      "[[10.2         4.8        11.         14.31453705 40.02888267 32.        ]]\n",
      "-54.0\n",
      "[[ 6.8        28.2        11.         19.61763798 19.26199528 31.        ]]\n",
      "44.0\n",
      "[[ 8.6        14.2        12.         25.604538   19.01838323 30.        ]]\n",
      "-13.039999999999964\n",
      "[[14.9         6.3        12.         19.14197984 29.00319295 29.        ]]\n",
      "-81.16\n",
      "[[ 8.1         7.4        12.         18.67735354 29.48536474 28.        ]]\n",
      "-31.39999999999999\n",
      "[[ 5.8        13.2        12.         12.43513198 41.45402496 27.        ]]\n",
      "2.8000000000000114\n",
      "[[ 0.4        22.8        12.         31.71866027 29.30992968 26.        ]]\n",
      "70.24000000000001\n",
      "[[ 7.9         4.2        15.         29.37115257 33.82589077 25.        ]]\n",
      "-40.28\n",
      "[[14.5        14.1        16.         28.94803589 51.35465911 24.        ]]\n",
      "-53.48000000000002\n",
      "[[ 4.         13.3        17.         22.79641933 58.52092288 23.        ]]\n",
      "15.36\n",
      "[[15.8        17.5        18.         26.37496808 50.89992923 22.        ]]\n",
      "-51.43999999999997\n",
      "[[ 3.2   40.5   18.     5.586 17.648 21.   ]]\n",
      "107.83999999999997\n",
      "[[ 8.8        19.6        18.          8.22646142 45.94317559 20.        ]]\n",
      "2.8799999999999955\n",
      "[[12.2        16.8        18.         22.09959352 50.40418191 19.        ]]\n",
      "-29.19999999999999\n",
      "[[ 4.5        20.6        18.         40.30429437 50.51111691 18.        ]]\n",
      "35.31999999999999\n",
      "[[10.3         3.4        21.         26.63817399 51.36928384 17.        ]]\n",
      "-59.16000000000001\n",
      "[[ 6.6        25.5        21.          0.06092643 47.65194015 16.        ]]\n",
      "36.72\n",
      "[[ 4.3    6.3    5.     8.074 49.296 15.   ]]\n",
      "-9.079999999999998\n",
      "[[ 4.4        22.1         5.         26.08084736 45.959791   14.        ]]\n",
      "40.80000000000001\n",
      "[[ 2.4        20.9         6.          5.95385586 57.67922999 13.        ]]\n",
      "50.56000000000003\n",
      "[[11.4        11.2         7.         12.28952603 49.25906354 12.        ]]\n",
      "-41.68000000000001\n",
      "[[10.9        20.1         7.         15.69034439 36.48135564 11.        ]]\n",
      "-9.799999999999983\n",
      "[[13.3        15.8         8.         13.39082364 44.33452973 10.        ]]\n",
      "-39.879999999999995\n",
      "[[ 6.         25.9         8.         36.75273743 41.35156241  9.        ]]\n",
      "42.08000000000001\n",
      "[[ 8.8        35.4         8.          2.92689405 57.6011785   8.        ]]\n",
      "53.44\n",
      "[[ 4.7         8.9         9.         13.37092344 51.73838422  7.        ]]\n",
      "-3.480000000000004\n",
      "[[ 3.5        40.3         9.         45.67799491 33.67968422  6.        ]]\n",
      "105.16000000000003\n",
      "[[ 6.8         7.7        18.         41.86388241 31.97984729  5.        ]]\n",
      "-21.599999999999994\n",
      "[[ 6.    43.3   19.     5.233 24.889  4.   ]]\n",
      "97.76000000000005\n",
      "[[ 3.7        24.5        19.         23.60821681 36.93828215  3.        ]]\n",
      "53.24000000000001\n",
      "[[ 9.4        27.2        20.         47.77532696 17.21686937  2.        ]]\n",
      "23.120000000000005\n",
      "[[ 0.4   45.1   19.     5.081 32.742  1.   ]]\n",
      "1641.6\n",
      "[[ 6.5        39.3        19.         46.40049173 29.54733996  0.        ]]\n",
      "81.56\n",
      "[[19.     7.9   13.    26.129 43.098 -1.   ]]\n",
      "-103.91999999999999\n",
      "[[18.9    8.    13.     3.834 55.943 -1.   ]]\n",
      "-102.91999999999999\n",
      "[[ 6.9        23.8        13.         30.22382549 41.28135426 -1.        ]]\n",
      "29.23999999999998\n",
      "[[ 5.9        30.9        13.         21.64624082  9.49773124 -1.        ]]\n",
      "58.76000000000002\n",
      "[[16.4         5.4        14.         19.92041186 24.90124124 -1.        ]]\n",
      "-94.23999999999998\n",
      "[[16.3         7.6        14.          6.52207245 25.73408384 -1.        ]]\n",
      "-86.51999999999998\n",
      "[[ 3.          7.6        14.          1.34291124 26.43030157 -1.        ]]\n",
      "3.9200000000000017\n",
      "[[ 4.7        20.9        15.          3.05910561 46.40918479 -1.        ]]\n",
      "34.920000000000016\n",
      "[[ 8.5        22.3        15.         26.87619417 40.64941716 -1.        ]]\n",
      "13.560000000000002\n",
      "[[14.4        20.         16.          7.8215699  17.82383043 -1.        ]]\n",
      "-33.91999999999999\n",
      "[[ 3.8        29.2        16.         33.58703108 15.40891838 -1.        ]]\n",
      "67.6\n",
      "[[15.9        27.3        18.          4.75377529 25.91377338 -1.        ]]\n",
      "-20.75999999999999\n",
      "[[ 2.8        12.9        18.          1.99359984 41.42450968 -1.        ]]\n",
      "22.24000000000001\n",
      "[[16.4    5.5   18.     7.975 60.    -1.   ]]\n",
      "-93.91999999999999\n",
      "[[ 5.4        17.6        18.         20.49449701 48.52410486 -1.        ]]\n",
      "19.599999999999994\n",
      "[[15.1    6.3   19.    12.693 29.987 -1.   ]]\n",
      "-82.51999999999998\n",
      "[[16.          5.9        19.          1.42016505 39.97694987 -1.        ]]\n",
      "-89.91999999999999\n",
      "[[ 5.6        28.1        19.         19.12997963 20.01841138 -1.        ]]\n",
      "51.839999999999975\n",
      "[[19.8        14.8        19.         10.89477487 20.3595198  -1.        ]]\n",
      "-87.28\n",
      "[[ 9.1         7.7        19.          9.6102395  21.05790327 -1.        ]]\n",
      "-37.239999999999995\n",
      "[[ 6.         37.         19.         40.94328259 31.74008357 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 8.1        18.7         8.         30.45739459 20.65091679 -1.        ]]\n",
      "4.760000000000019\n",
      "[[ 8.7         5.          8.         20.92564306 13.20589542 -1.        ]]\n",
      "-43.16\n",
      "[[ 8.    24.1    9.     8.777 36.521 -1.   ]]\n",
      "22.72\n",
      "[[ 4.7         1.2         9.          4.24696805 32.80411833 -1.        ]]\n",
      "-28.120000000000005\n",
      "[[ 2.2        36.9         9.         36.0224772  55.39441013 -1.        ]]\n",
      "103.12\n",
      "[[ 4.6        10.4        11.         28.37768002 44.29997853 -1.        ]]\n",
      "2.0\n",
      "[[ 2.7         3.8        12.         26.38518101 50.48971572 -1.        ]]\n",
      "-6.199999999999996\n",
      "[[ 9.9        12.4        12.         27.44755591 35.39416128 -1.        ]]\n",
      "-27.640000000000015\n",
      "[[ 1.3         5.5        13.         24.52112345 29.4614269  -1.        ]]\n",
      "8.760000000000005\n",
      "[[13.3         6.7        15.         11.59632153 16.6570684  -1.        ]]\n",
      "-69.0\n",
      "[[ 8.    30.5   15.     3.415 47.344 -1.   ]]\n",
      "43.19999999999999\n",
      "[[ 8.1        11.6        15.         14.10337216 39.96860934 -1.        ]]\n",
      "-17.95999999999998\n",
      "[[10.6         7.1        15.         17.35563865 48.45761019 -1.        ]]\n",
      "-49.359999999999985\n",
      "[[ 0.8         6.3        15.         12.2677657  43.60292119 -1.        ]]\n",
      "14.720000000000006\n",
      "[[ 9.3        16.4        15.         15.12145507 33.38053869 -1.        ]]\n",
      "-10.759999999999991\n",
      "[[16.3        14.3        16.         12.84763656 19.20057421 -1.        ]]\n",
      "-65.08000000000001\n",
      "[[ 9.6        16.2        17.         20.29486002 22.22459661 -1.        ]]\n",
      "-13.43999999999997\n",
      "[[10.6        23.         17.         46.54213265 18.92318986 -1.        ]]\n",
      "1.5200000000000102\n",
      "[[22.5        18.4        12.          9.80862876 26.33556996 -1.        ]]\n",
      "-94.12\n",
      "[[ 6.         24.3        12.         17.81067092 43.39367517 -1.        ]]\n",
      "36.96000000000001\n",
      "[[10.2        11.2        12.         19.76146566 42.37048509 -1.        ]]\n",
      "-33.51999999999998\n",
      "[[ 4.8        16.5        12.         23.29301498 22.76869675 -1.        ]]\n",
      "20.159999999999997\n",
      "[[ 5.5        16.8        12.         33.18407071 42.27020438 -1.        ]]\n",
      "16.359999999999985\n",
      "[[17.2        24.5        12.          0.90342949 45.64679259 -1.        ]]\n",
      "-38.56\n",
      "[[14.4         8.1        13.          5.08439037 36.74349976 -1.        ]]\n",
      "-72.0\n",
      "[[ 7.6         9.9        13.          6.01525623 40.44053907 -1.        ]]\n",
      "-20.0\n",
      "[[ 4.2    7.5   13.     9.798 49.135 -1.   ]]\n",
      "-4.559999999999988\n",
      "[[12.6        26.2        13.         19.20812337 39.17162922 -1.        ]]\n",
      "-1.839999999999975\n",
      "[[ 3.1        10.4        13.         21.05281452 48.67662219 -1.        ]]\n",
      "12.200000000000003\n",
      "[[ 0.9        37.9        13.         50.38104603 25.20045873 -1.        ]]\n",
      "115.16000000000003\n",
      "[[26.5         1.2        20.         32.70312429 46.07148054 -1.        ]]\n",
      "-176.35999999999999\n",
      "[[22.    12.3   20.     0.845 55.572 -1.   ]]\n",
      "-110.23999999999998\n",
      "[[ 3.5        16.7        21.         18.25426142 45.58950492 -1.        ]]\n",
      "29.640000000000015\n",
      "[[ 6.9        41.2        21.         30.4107725   9.31176178 -1.        ]]\n",
      "84.92000000000002\n",
      "[[19.3   36.1    7.     1.998 56.617 -1.   ]]\n",
      "-15.720000000000027\n",
      "[[22.4        13.          7.         10.14081715 22.18721789 -1.        ]]\n",
      "-110.71999999999997\n",
      "[[16.6         2.8         7.          4.55975566 36.13846796 -1.        ]]\n",
      "-103.92000000000002\n",
      "[[ 2.2        27.6         7.         14.99089979 11.29981019 -1.        ]]\n",
      "73.36000000000001\n",
      "[[22.9        24.5         8.         18.75931946 12.01922733 -1.        ]]\n",
      "-77.32\n",
      "[[ 7.6        10.9         8.         18.64744269  7.88637312 -1.        ]]\n",
      "-16.799999999999997\n",
      "[[ 2.1   10.7    9.     6.767 12.249 -1.   ]]\n",
      "19.960000000000008\n",
      "[[ 6.3        39.          9.         25.54447261 44.70441097 -1.        ]]\n",
      "81.96000000000004\n",
      "[[17.          8.9         9.         14.143324   50.78992767 -1.        ]]\n",
      "-87.11999999999998\n",
      "[[12.4         5.          9.          8.12055629 58.06730208 -1.        ]]\n",
      "-68.32\n",
      "[[ 6.2         3.4        10.         13.75889479 52.05866368 -1.        ]]\n",
      "-31.28\n",
      "[[11.     9.1   10.    16.92  56.105 -1.   ]]\n",
      "-45.68000000000001\n",
      "[[ 8.4         6.         10.          9.72103005 43.58738335 -1.        ]]\n",
      "-37.92\n",
      "[[19.4         2.6        10.         25.59035029 51.56042439 -1.        ]]\n",
      "-123.6\n",
      "[[20.         32.7        11.         34.71517053 36.76045829 -1.        ]]\n",
      "-31.360000000000014\n",
      "[[25.7   37.2   12.     8.071 20.542 -1.   ]]\n",
      "-55.72000000000003\n",
      "[[18.1        33.9        12.         38.72850415 27.73733932 -1.        ]]\n",
      "-14.599999999999966\n",
      "[[17.6        16.3        12.         36.49993456 59.83828461 -1.        ]]\n",
      "-67.52000000000004\n",
      "[[16.2         3.7        14.         28.49046349 45.14100782 -1.        ]]\n",
      "-98.32\n",
      "[[ 5.    25.6   14.     3.806 38.741 -1.   ]]\n",
      "47.91999999999999\n",
      "[[ 3.1         8.7        14.          2.99333896 33.09030822 -1.        ]]\n",
      "6.760000000000005\n",
      "[[12.5        34.         14.         42.05856294 47.07411239 -1.        ]]\n",
      "23.80000000000001\n",
      "[[24.5        14.2        15.         13.43428721 41.07740436 -1.        ]]\n",
      "-121.16000000000003\n",
      "[[11.7        23.         15.         19.26762912 30.81630256 -1.        ]]\n",
      "-5.960000000000008\n",
      "[[14.1         6.2        15.         10.67969281 33.16140654 -1.        ]]\n",
      "-76.03999999999999\n",
      "[[ 3.3        17.1        15.          4.18210198 51.55597244 -1.        ]]\n",
      "32.28\n",
      "[[ 2.5         2.6        15.          2.85778095 52.28149066 -1.        ]]\n",
      "-8.68\n",
      "[[17.1         4.9        15.          6.77390279 34.2067716  -1.        ]]\n",
      "-100.6\n",
      "[[ 3.8        20.7        15.         17.86110535 13.18958188 -1.        ]]\n",
      "40.400000000000006\n",
      "[[18.2        12.         16.         16.05287009 31.74985224 -1.        ]]\n",
      "-85.35999999999999\n",
      "[[12.3        12.1        16.         11.28181195 27.3001329  -1.        ]]\n",
      "-44.91999999999999\n",
      "[[21.8         5.1        16.          6.68397339 44.84081087 -1.        ]]\n",
      "-131.92\n",
      "[[ 7.8        14.         16.         16.19263064 26.00298833 -1.        ]]\n",
      "-8.240000000000009\n",
      "[[ 9.6        15.2        16.         20.94642821 28.9479737  -1.        ]]\n",
      "-16.639999999999986\n",
      "[[20.8        24.8        16.         23.41544054 21.1901275  -1.        ]]\n",
      "-62.079999999999984\n",
      "[[22.4         4.1        16.          3.24344421 32.09706313 -1.        ]]\n",
      "-139.2\n",
      "[[18.1   16.2   16.     3.138 39.865 -1.   ]]\n",
      "-71.23999999999998\n",
      "[[ 4.8        42.5        16.         48.93724649 38.62521003 -1.        ]]\n",
      "103.36000000000001\n",
      "[[ 1.         15.5        21.         34.27827646 42.88013589 -1.        ]]\n",
      "42.8\n",
      "[[10.9         3.9        21.         31.04215077 36.60381196 -1.        ]]\n",
      "-61.64\n",
      "[[  7.1         31.3          0.          59.2892586   23.58461374\n",
      "  140.        ]]\n",
      "51.879999999999995\n",
      "driver reward  -486.8400000000006\n",
      "total reward  1811.0400000000081\n",
      "trips [ 25.  48.  79. 125. 253. 463. 783.]\n",
      "[ 25.  48.  79. 125. 253. 463. 783.]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "\n",
    "pickup_distance_brackets = [1, 2, 3, 4, 6, 10]\n",
    "trip_distance_brackets = [5, 10, 15, 25, 35, 50]\n",
    "    \n",
    "#categorize distance\n",
    "def sortDistance(dist, distance_brackets):\n",
    "    if dist > distance_brackets[-1]:\n",
    "        return len(distance_brackets)\n",
    "    for i in range(len(distance_brackets)):\n",
    "        if dist <= distance_brackets[i]:\n",
    "            return i\n",
    "    \n",
    "#calculate acceptance rates based on distances\n",
    "def evaluatePolicyDistances(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    \n",
    "    pickup_accepted_trips = np.zeros(len(pickup_distance_brackets)+1)\n",
    "    pickup_trip_counts = np.zeros(len(pickup_distance_brackets)+1)\n",
    "    pickup_acceptance_rates = []\n",
    "    \n",
    "    for state_list in eval_env[0]:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "                print(np.array([state_list[i][\"observation\"]]))\n",
    "                pickup_accepted_trips[sortDistance(float(np.array([state_list[i][\"observation\"]])[0][0]), pickup_distance_brackets)] +=1\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "            pickup_trip_counts[sortDistance(float(np.array([state_list[i][\"observation\"]])[0][0]), pickup_distance_brackets)] +=1\n",
    "            \n",
    "            \n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "    \n",
    "    #find average acceptance for each hour\n",
    "    print(\"trips\", pickup_trip_counts )\n",
    "    for j in range(len(pickup_distance_brackets)+1):\n",
    "        pickup_acceptance_rates.append(float(pickup_accepted_trips[j])/float(pickup_trip_counts[j]))\n",
    "    print (pickup_accepted_trips)\n",
    "    print(pickup_acceptance_rates)\n",
    "evaluatePolicyDistances(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.7        19.4         6.         38.13065633 23.72463914 40.        ]]\n",
      "2.920000000000016\n",
      "0\n",
      "0\n",
      "[[ 6.2        27.6         8.          1.28675881 21.7954455  37.        ]]\n",
      "46.15999999999997\n",
      "0\n",
      "[[ 6.1        31.         15.          5.99278098 34.49980792 35.        ]]\n",
      "57.72\n",
      "0\n",
      "[[ 5.4        26.2        16.         13.04517826 31.1342781  33.        ]]\n",
      "47.120000000000005\n",
      "0\n",
      "[[ 7.6        26.6        17.         21.98445076 12.22969952 31.        ]]\n",
      "33.44\n",
      "[[ 1.6   25.2   17.    28.006 36.613 30.   ]]\n",
      "69.75999999999999\n",
      "[[ 7.9        18.1        17.         39.47298694 41.70445438 29.        ]]\n",
      "4.200000000000017\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.6        34.6        22.         27.28885592 17.55701926 25.        ]]\n",
      "72.63999999999999\n",
      "0\n",
      "[[ 8.         28.5        12.         51.78216149 23.3292893  23.        ]]\n",
      "36.80000000000001\n",
      "[[ 7.1   36.8   14.    10.556 18.298 22.   ]]\n",
      "69.48000000000002\n",
      "[[ 4.8        11.3        14.         14.05733979 32.38715642 21.        ]]\n",
      "3.519999999999996\n",
      "0\n",
      "[[ 7.7        29.7        15.         23.30816996  6.9981076  19.        ]]\n",
      "42.68000000000001\n",
      "0\n",
      "[[ 9.1   28.5   19.     9.915 56.093 17.   ]]\n",
      "29.319999999999993\n",
      "0\n",
      "0\n",
      "[[12.9        29.6        19.         32.09777814 58.18795286 14.        ]]\n",
      "7.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.6        29.          7.         35.00059951  5.54753604  8.        ]]\n",
      "47.91999999999999\n",
      "0\n",
      "[[ 3.2         8.1        11.          8.37800632 39.01386326  6.        ]]\n",
      "4.159999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.6        27.2        11.         24.20603856 17.12153836  2.        ]]\n",
      "35.360000000000014\n",
      "[[ 6.    21.9   11.     3.626 13.388  1.   ]]\n",
      "1529.28\n",
      "[[ 2.1        17.7        11.          8.15396392 28.92100985  0.        ]]\n",
      "42.359999999999985\n",
      "[[ 7.         25.6        11.         26.22620263 17.0795537  -1.        ]]\n",
      "34.31999999999999\n",
      "[[ 4.2        35.3        11.         58.89176977  0.97829417 -1.        ]]\n",
      "84.40000000000003\n",
      "driver reward  2300.5600000000004\n",
      "[[ 5.2        39.6         1.         19.05672516 14.76536628 40.        ]]\n",
      "91.35999999999996\n",
      "0\n",
      "0\n",
      "[[ 2.7        21.6         8.          4.9105271  37.47731101 37.        ]]\n",
      "50.75999999999999\n",
      "0\n",
      "[[ 4.4        18.5         8.          3.38403508  4.3407305  35.        ]]\n",
      "29.28\n",
      "[[ 3.5   10.6   11.     6.09  14.011 34.   ]]\n",
      "10.120000000000005\n",
      "[[ 4.         24.6        12.          4.3876003  41.25369412 33.        ]]\n",
      "51.51999999999998\n",
      "[[ 7.2        31.         12.         20.7370446   7.97016401 32.        ]]\n",
      "50.24000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.7        18.3        17.         11.52839793 42.18371107 28.        ]]\n",
      "13.0\n",
      "0\n",
      "[[ 2.8        11.6        17.         15.91948786 50.74396994 26.        ]]\n",
      "18.080000000000013\n",
      "0\n",
      "[[10.7        24.7        17.          6.28934737 25.33499432 24.        ]]\n",
      "6.28000000000003\n",
      "0\n",
      "0\n",
      "[[10.1        36.3        22.         24.13638755 44.07450132 21.        ]]\n",
      "47.48000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.5        10.5         9.          0.54619013 19.63951508 15.        ]]\n",
      "3.0\n",
      "0\n",
      "[[ 3.9        39.8        10.         20.9103921   8.19137374 13.        ]]\n",
      "100.84000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.9        27.9        12.          7.60120855 12.64459618  1.        ]]\n",
      "1555.96\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.1        17.8        13.         12.4497565  42.47574363 -1.        ]]\n",
      "29.080000000000013\n",
      "[[ 1.6        27.5        13.         10.56763873 16.24458578 -1.        ]]\n",
      "77.12\n",
      "[[ 5.1        22.5        13.         14.60628992 36.65130044 -1.        ]]\n",
      "37.31999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.1        17.8        15.         15.18059642 37.66795005 -1.        ]]\n",
      "15.480000000000018\n",
      "0\n",
      "[[ 2.6         9.5        15.         13.47942803 34.42188827 -1.        ]]\n",
      "12.719999999999999\n",
      "0\n",
      "0\n",
      "[[ 6.5        26.6        15.         27.67095333 29.01725876 -1.        ]]\n",
      "40.91999999999999\n",
      "0\n",
      "0\n",
      "[[ 3.2        25.6        17.         17.65357794  8.59517795 -1.        ]]\n",
      "60.16\n",
      "0\n",
      "0\n",
      "[[ 7.3        19.9        20.         27.80366842 19.19737931 -1.        ]]\n",
      "14.04000000000002\n",
      "0\n",
      "0\n",
      "[[ 6.8        32.4         8.         22.47567097 49.39464551 -1.        ]]\n",
      "57.440000000000055\n",
      "0\n",
      "[[16.5        38.8         8.         40.82115521 53.33410967 -1.        ]]\n",
      "11.960000000000036\n",
      "[[ 6.5        25.1        14.         23.19708843 47.21375485 -1.        ]]\n",
      "36.120000000000005\n",
      "0\n",
      "0\n",
      "[[ 3.         14.1        15.          0.20153531 35.77475547 -1.        ]]\n",
      "24.72\n",
      "[[ 6.8        26.4        15.         20.54571501 55.54866727 -1.        ]]\n",
      "38.24000000000004\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.4        33.6        17.         30.11756209 15.68449004 -1.        ]]\n",
      "43.60000000000002\n",
      "0\n",
      "0\n",
      "[[ 3.2        27.4        18.         26.16760877 53.6429197  -1.        ]]\n",
      "65.92000000000002\n",
      "0\n",
      "[[ 7.3        32.3        19.         31.86163261 33.06401063 -1.        ]]\n",
      "53.72000000000003\n",
      "[[ 3.1         7.2        19.         30.94608356 42.95840411 -1.        ]]\n",
      "1.9599999999999937\n",
      "0\n",
      "[[ 3.4        15.6        20.         36.83824123 23.57768618 -1.        ]]\n",
      "26.80000000000001\n",
      "0\n",
      "[[ 4.    11.3   21.     9.156 29.647 -1.   ]]\n",
      "8.959999999999994\n",
      "[[11.8        36.         21.         44.67154002 58.85050753 -1.        ]]\n",
      "34.960000000000036\n",
      "0\n",
      "[[ 2.4        22.7        22.         34.60004375 42.32711933 -1.        ]]\n",
      "56.32000000000002\n",
      "[[ 5.3        30.          6.         59.25892541 29.08757131 -1.        ]]\n",
      "59.960000000000036\n",
      "driver reward  2835.44\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.9         7.5         7.          3.81213295 23.76322052 37.        ]]\n",
      "4.280000000000001\n",
      "[[ 4.1        14.7         7.          7.83980138 12.64070439 36.        ]]\n",
      "19.160000000000025\n",
      "[[ 3.5        34.7         7.         35.73001075 37.15274711 35.        ]]\n",
      "87.24000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.4   29.7   17.    13.087 48.502 31.   ]]\n",
      "65.12\n",
      "[[ 2.9         9.         17.         12.16148807 54.82312786 30.        ]]\n",
      "9.079999999999998\n",
      "[[ 5.6        21.9        17.         25.24574514 34.65358825 29.        ]]\n",
      "32.0\n",
      "0\n",
      "[[ 2.6         9.2        17.         16.74711287 52.83879807 27.        ]]\n",
      "11.760000000000005\n",
      "[[ 8.9        30.5        18.         39.19971551 48.77600349 26.        ]]\n",
      "37.08000000000004\n",
      "[[ 1.7   26.8   19.    15.372 57.926 25.   ]]\n",
      "74.20000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.8        20.          9.         34.34494057 41.14630568 14.        ]]\n",
      "31.360000000000014\n",
      "[[ 2.6   23.5    9.     9.195 47.695 13.   ]]\n",
      "57.51999999999998\n",
      "[[ 1.8         9.8         9.          0.11176767 42.16258845 12.        ]]\n",
      "19.11999999999999\n",
      "[[ 7.2        20.5         9.         24.20358596 51.39385035 11.        ]]\n",
      "16.640000000000015\n",
      "[[ 6.9        29.5        10.         35.60300647 20.88500505 10.        ]]\n",
      "47.48000000000002\n",
      "[[13.3   29.2   14.     4.417 12.847  9.   ]]\n",
      "3.0\n",
      "0\n",
      "0\n",
      "[[ 3.6        19.         17.         20.45144457 26.55777118  6.        ]]\n",
      "36.31999999999999\n",
      "0\n",
      "[[ 8.1        31.5        18.         34.01109761 20.92443832  4.        ]]\n",
      "45.71999999999997\n",
      "0\n",
      "[[10.3   24.4   17.    29.778 48.857  2.   ]]\n",
      "8.039999999999992\n",
      "[[ 2.4        13.5        17.         18.34343865 54.95528301  1.        ]]\n",
      "1526.88\n",
      "0\n",
      "[[ 3.8        26.6        17.         17.90008267 23.12336051 -1.        ]]\n",
      "59.28\n",
      "0\n",
      "0\n",
      "[[ 3.4        22.3        18.          1.65760095 44.22749976 -1.        ]]\n",
      "48.24000000000001\n",
      "[[ 4.6        19.1        18.         22.85873285 34.01538985 -1.        ]]\n",
      "29.839999999999975\n",
      "[[ 5.2        22.4        18.         40.79447547 35.00208273 -1.        ]]\n",
      "36.32000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[12.6   27.8   21.     5.557 25.329 -1.   ]]\n",
      "3.2800000000000296\n",
      "0\n",
      "0\n",
      "[[ 3.2        24.          5.          5.66587807  1.18949828 -1.        ]]\n",
      "55.04000000000002\n",
      "0\n",
      "[[10.2        32.2        12.         45.2039587  26.52406518 -1.        ]]\n",
      "33.67999999999995\n",
      "[[10.4        33.1        16.         21.85000262 36.5352478  -1.        ]]\n",
      "35.19999999999999\n",
      "[[ 5.5        30.         17.         42.93070092 57.01436968 -1.        ]]\n",
      "58.599999999999994\n",
      "[[ 7.3        20.4        18.         54.00677552 44.09573028 -1.        ]]\n",
      "15.640000000000015\n",
      "0\n",
      "0\n",
      "[[ 7.4        35.1        10.         14.26937887 23.4593006  -1.        ]]\n",
      "62.0\n",
      "[[ 4.5        13.7        11.          5.2640531  19.89666105 -1.        ]]\n",
      "13.240000000000009\n",
      "[[ 3.3         7.3        11.         10.61303379 25.08480556 -1.        ]]\n",
      "0.9200000000000017\n",
      "[[ 4.4        17.1        11.          9.963332    6.17306061 -1.        ]]\n",
      "24.80000000000001\n",
      "[[ 5.    20.3   11.     0.681 18.963 -1.   ]]\n",
      "30.960000000000008\n",
      "0\n",
      "[[ 8.3        30.3        12.         27.57891769 13.31962639 -1.        ]]\n",
      "40.51999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.         19.9        18.         17.8819954  39.71414512 -1.        ]]\n",
      "2.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.7        24.1        18.          4.89362583 21.10227698 -1.        ]]\n",
      "11.160000000000025\n",
      "0\n",
      "[[ 1.1        18.9        19.         16.85823717 16.97327008 -1.        ]]\n",
      "53.0\n",
      "[[ 5.8        23.6        19.         33.93292575 38.22310531 -1.        ]]\n",
      "36.079999999999984\n",
      "[[13.    29.2   19.     3.26  49.324 -1.   ]]\n",
      "5.039999999999964\n",
      "[[ 9.5        21.7        19.          2.56044988 18.51350505 -1.        ]]\n",
      "4.840000000000003\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.2   32.3   20.     1.522 42.14  -1.   ]]\n",
      "81.6\n",
      "[[ 3.6        39.         20.         25.91620984  8.859845   -1.        ]]\n",
      "100.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.4        22.9         5.         32.57928234 28.93280412 -1.        ]]\n",
      "43.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "driver reward  3017.440000000001\n",
      "[[ 9.4        34.6         5.         41.248075   47.12082522 40.        ]]\n",
      "46.80000000000001\n",
      "0\n",
      "[[ 1.7        21.3        17.         36.83404257 33.09100974 38.        ]]\n",
      "56.599999999999994\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.2         8.5        20.          8.87697149 50.95232723 33.        ]]\n",
      "5.440000000000012\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.    25.1    6.    15.64  55.155 29.   ]]\n",
      "5.52000000000001\n",
      "0\n",
      "[[ 6.9   27.7    7.    28.247 12.105 27.   ]]\n",
      "41.72\n",
      "[[ 2.3         6.2        10.         30.90871582  7.40115464 26.        ]]\n",
      "4.200000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.4        15.1        13.         20.75484191 36.49662828 20.        ]]\n",
      "18.400000000000006\n",
      "0\n",
      "[[11.9        34.3        14.         38.86052144 40.24508908 18.        ]]\n",
      "28.840000000000032\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.1        26.         17.         19.41993889 20.08217035 12.        ]]\n",
      "28.120000000000005\n",
      "0\n",
      "[[ 1.         15.8        17.          5.91839007 52.24542541 10.        ]]\n",
      "43.760000000000005\n",
      "[[ 3.7        12.3        17.         19.98134217 44.79527918  9.        ]]\n",
      "14.200000000000003\n",
      "0\n",
      "[[ 8.2        20.         18.         24.67544169 44.39671959  7.        ]]\n",
      "8.240000000000009\n",
      "0\n",
      "[[ 5.5        15.2        18.         18.61208831 49.67521513  5.        ]]\n",
      "11.240000000000009\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.3   27.9   19.    35.644 58.475 -1.   ]]\n",
      "66.84\n",
      "0\n",
      "[[ 8.8        31.7         8.         32.82368928 36.29096944 -1.        ]]\n",
      "41.60000000000002\n",
      "[[ 8.5   19.4    8.    11.314 35.675 -1.   ]]\n",
      "4.280000000000001\n",
      "[[ 5.8   13.5    8.     6.842 46.853 -1.   ]]\n",
      "3.759999999999991\n",
      "0\n",
      "[[ 1.5         4.          8.          3.00635598 49.57321287 -1.        ]]\n",
      "2.6000000000000014\n",
      "0\n",
      "[[ 4.3        38.2         9.         37.18003521 28.87240247 -1.        ]]\n",
      "93.0\n",
      "0\n",
      "[[ 9.5        23.1         9.         27.41829884 59.24599659 -1.        ]]\n",
      "9.319999999999993\n",
      "[[ 2.         39.3         9.         57.58201791 36.73615711 -1.        ]]\n",
      "112.16000000000003\n",
      "[[ 2.7         8.6        12.         56.87569882 29.91197792 -1.        ]]\n",
      "9.159999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.9        18.6        17.         26.55863685 46.11690753 -1.        ]]\n",
      "26.200000000000017\n",
      "0\n",
      "0\n",
      "[[ 2.5        23.1        18.         25.48136686 30.15808459 -1.        ]]\n",
      "56.91999999999999\n",
      "[[ 3.1        13.8        18.         40.1767611  32.16260255 -1.        ]]\n",
      "23.079999999999984\n",
      "[[ 7.4        39.9        10.         32.08157979  0.33185881 -1.        ]]\n",
      "77.36000000000001\n",
      "0\n",
      "0\n",
      "[[ 8.8        54.8        12.         58.20860968 13.70057444 -1.        ]]\n",
      "115.52000000000004\n",
      "[[ 2.4   14.2   12.    51.133 26.575 -1.   ]]\n",
      "29.12000000000002\n",
      "driver reward  984.0000000000001\n",
      "[[ 3.2         9.4         7.         36.87496537 16.87851324 40.        ]]\n",
      "8.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 8.4   23.9   13.    30.274 42.665 37.   ]]\n",
      "19.360000000000014\n",
      "0\n",
      "0\n",
      "[[12.1   35.2   16.    16.523  9.147 34.   ]]\n",
      "30.359999999999957\n",
      "0\n",
      "[[10.9        41.         18.         51.67309047 34.2104944  32.        ]]\n",
      "57.08000000000004\n",
      "0\n",
      "[[ 4.    10.1   19.    47.791 20.968 30.   ]]\n",
      "5.1200000000000045\n",
      "[[ 6.3        36.         18.         28.84201996 44.77532661 29.        ]]\n",
      "72.36000000000001\n",
      "[[ 4.    24.7   19.     5.166 55.208 28.   ]]\n",
      "51.84\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.3   19.6   19.     7.107 59.235 24.   ]]\n",
      "26.67999999999998\n",
      "[[ 4.8        15.5        19.         13.44529701 40.12031149 23.        ]]\n",
      "16.960000000000008\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.    31.1   20.     8.473  9.066 18.   ]]\n",
      "38.31999999999999\n",
      "0\n",
      "0\n",
      "[[10.5   26.7   21.     2.48  42.506 15.   ]]\n",
      "14.039999999999992\n",
      "0\n",
      "[[ 8.8        27.8        21.         20.43756166 17.00546649 13.        ]]\n",
      "29.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.1        30.7         9.         29.63177788 58.13577635  6.        ]]\n",
      "70.36000000000001\n",
      "[[ 5.1   22.2    9.     4.112 57.354  5.   ]]\n",
      "36.360000000000014\n",
      "[[ 5.4        20.3         9.         26.66935071 50.26989597  4.        ]]\n",
      "28.23999999999998\n",
      "0\n",
      "[[12.6   27.5    9.     4.228 23.959  2.   ]]\n",
      "2.319999999999993\n",
      "[[ 7.3        17.7        10.         25.19184771 33.23986014  1.        ]]\n",
      "1507.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.1        33.7         1.         35.53523343 36.9087237  -1.        ]]\n",
      "45.95999999999998\n",
      "0\n",
      "[[ 4.5        14.9         8.         39.01986303  5.36318522 -1.        ]]\n",
      "17.080000000000013\n",
      "[[ 5.5   48.5   12.     6.978 34.841 -1.   ]]\n",
      "117.80000000000001\n",
      "[[ 2.2         4.9        12.          2.51365272 30.24955478 -1.        ]]\n",
      "0.7199999999999989\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.4        12.7        13.          4.74803339 42.3260907  -1.        ]]\n",
      "17.520000000000024\n",
      "[[ 2.4         8.9        13.          3.64634624 35.52459822 -1.        ]]\n",
      "12.159999999999997\n",
      "[[ 2.9        10.8        13.          3.25780481 27.40790122 -1.        ]]\n",
      "14.83999999999999\n",
      "0\n",
      "[[ 9.2   26.3   14.    11.812 47.039 -1.   ]]\n",
      "21.599999999999994\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.5   19.9   15.    26.134 49.999 -1.   ]]\n",
      "5.880000000000024\n",
      "[[ 3.1        36.5        16.         34.59285126 13.27928791 -1.        ]]\n",
      "95.71999999999997\n",
      "0\n",
      "[[ 5.6   20.1   19.    23.049  2.701 -1.   ]]\n",
      "26.23999999999998\n",
      "0\n",
      "0\n",
      "[[ 3.4         7.4        21.         22.65692866 14.49475742 -1.        ]]\n",
      "0.5600000000000023\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.4        32.6         8.         17.64645955  4.80681477 -1.        ]]\n",
      "13.199999999999989\n",
      "0\n",
      "0\n",
      "[[16.1        44.6        11.         49.06420315 25.85070489 -1.        ]]\n",
      "33.24000000000001\n",
      "[[ 4.1        18.8        13.         38.21943642  5.73464012 -1.        ]]\n",
      "32.28\n",
      "driver reward  2468.64\n",
      "0\n",
      "[[ 4.1   15.7   19.    58.155 32.214 39.   ]]\n",
      "22.360000000000014\n",
      "[[17.2   37.7    9.    15.225 22.858 38.   ]]\n",
      "3.67999999999995\n",
      "0\n",
      "0\n",
      "[[ 6.2        18.5         9.         17.30649342 41.6228554  35.        ]]\n",
      "17.04000000000002\n",
      "[[11.7        29.3        10.         36.32473038 45.45038756 34.        ]]\n",
      "14.199999999999989\n",
      "[[ 9.5        26.9        10.         50.56286481 34.70371222 33.        ]]\n",
      "21.480000000000018\n",
      "0\n",
      "0\n",
      "[[10.6   24.5   13.    20.467 44.344 30.   ]]\n",
      "6.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 5.5        24.8        13.         14.29011698 27.28444377 27.        ]]\n",
      "41.96000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.5   33.3   14.    37.412 52.147 22.   ]]\n",
      "82.76000000000002\n",
      "0\n",
      "0\n",
      "[[ 9.3        24.6        17.         23.69564803 18.9162301  19.        ]]\n",
      "15.479999999999961\n",
      "[[ 3.         11.5        18.         35.00554754 11.74460889 18.        ]]\n",
      "16.400000000000006\n",
      "0\n",
      "0\n",
      "[[ 3.2        24.5        20.         34.84024713 38.75742604 15.        ]]\n",
      "56.640000000000015\n",
      "[[ 2.1    6.1   20.    30.551 31.769 14.   ]]\n",
      "5.240000000000009\n",
      "0\n",
      "[[18.5        42.1        20.         36.74439918 35.36247028 12.        ]]\n",
      "8.920000000000016\n",
      "[[ 5.3   16.5   21.    16.861 44.075 11.   ]]\n",
      "16.75999999999999\n",
      "0\n",
      "[[ 7.         15.         21.          5.41749903 53.85540111  9.        ]]\n",
      "0.4000000000000057\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.6        16.4         7.         53.08819085 46.3218985   5.        ]]\n",
      "21.200000000000017\n",
      "0\n",
      "0\n",
      "driver reward  350.84000000000003\n",
      "[[ 9.4        21.3         5.         20.79900516  4.4955353  40.        ]]\n",
      "4.239999999999981\n",
      "[[10.3        27.1         9.         40.76106336 12.01423723 39.        ]]\n",
      "16.67999999999998\n",
      "[[ 5.4   33.8   13.    11.971 37.294 38.   ]]\n",
      "71.44000000000005\n",
      "[[ 1.7        25.4        13.         28.11151331 59.01755155 37.        ]]\n",
      "69.72000000000003\n",
      "0\n",
      "[[ 4.2        16.         14.          3.43353056 57.54881884 35.        ]]\n",
      "22.640000000000015\n",
      "[[ 5.1        44.8        14.         29.27607044 20.65753661 34.        ]]\n",
      "108.68\n",
      "[[ 3.1         7.2        19.         27.06216592 24.81308422 33.        ]]\n",
      "1.9599999999999937\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.1        14.6        12.          6.42491219 13.66646657 29.        ]]\n",
      "32.44000000000001\n",
      "[[ 8.3        31.         13.         22.72472677 30.30295807 28.        ]]\n",
      "42.76000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.8        31.3        13.         31.95899423 53.35928457 24.        ]]\n",
      "53.920000000000016\n",
      "[[ 9.4   28.4   14.     6.972 29.639 23.   ]]\n",
      "26.960000000000036\n",
      "[[ 7.1        15.9        14.         14.65519644 24.17572802 22.        ]]\n",
      "2.5999999999999943\n",
      "[[ 8.         26.5        15.          9.4254992  48.05681793 21.        ]]\n",
      "30.400000000000006\n",
      "[[ 1.6         3.6        15.          7.74092748 50.32636721 20.        ]]\n",
      "0.6400000000000006\n",
      "0\n",
      "0\n",
      "[[10.         52.6        16.         30.9455729   3.82910153 17.        ]]\n",
      "100.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.         15.5        20.         23.01418947 54.9044871  12.        ]]\n",
      "29.200000000000003\n",
      "0\n",
      "[[13.6        29.3        21.         24.93313131 54.74913979 10.        ]]\n",
      "1.2800000000000296\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.2        29.3        22.         13.95536725 54.30207637  5.        ]]\n",
      "78.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 6.8        39.4         4.         38.87912585 11.41334824  2.        ]]\n",
      "79.84000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.2   39.8   17.     8.09  43.709 -1.   ]]\n",
      "85.19999999999999\n",
      "[[ 3.5        23.2        18.          1.2889451  20.45783268 -1.        ]]\n",
      "50.44\n",
      "[[ 8.7        37.1        18.         11.62460716 49.16300612 -1.        ]]\n",
      "59.56\n",
      "[[ 7.4        26.2        18.          6.90677661 22.5473652  -1.        ]]\n",
      "33.52000000000001\n",
      "[[ 2.8        13.8        18.         15.12578836  8.33998062 -1.        ]]\n",
      "25.11999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.8        17.         19.         18.89072573  7.35208348 -1.        ]]\n",
      "21.75999999999999\n",
      "0\n",
      "0\n",
      "[[16.2        40.4        21.         22.38063099  4.28811491 -1.        ]]\n",
      "19.12000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.6        28.5        11.         33.41169245 59.58376383 -1.        ]]\n",
      "25.920000000000016\n",
      "[[ 3.3   14.    11.    37.496 43.528 -1.   ]]\n",
      "22.36\n",
      "[[ 6.5        22.2        12.         54.42074085 49.94125131 -1.        ]]\n",
      "26.840000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.6        18.2         8.         41.46204369 44.43882672 -1.        ]]\n",
      "47.359999999999985\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.3        40.9        11.         54.34555392 24.14643958 -1.        ]]\n",
      "74.44\n",
      "[[ 5.4        36.8        13.         22.70551149 34.58800117 -1.        ]]\n",
      "81.04000000000002\n",
      "[[ 6.2        23.         13.         36.22628878 48.28162774 -1.        ]]\n",
      "31.439999999999998\n",
      "0\n",
      "0\n",
      "[[ 1.6   25.    14.    59.041 46.098 -1.   ]]\n",
      "69.12\n",
      "driver reward  1447.7599999999998\n",
      "[[11.7        27.8         6.          2.6259339  46.03700771 40.        ]]\n",
      "9.400000000000034\n",
      "0\n",
      "[[ 4.8   26.9    8.    46.788 36.342 38.   ]]\n",
      "53.44\n",
      "0\n",
      "[[ 4.         33.1         7.         33.02741276 25.36579018 36.        ]]\n",
      "78.72\n",
      "[[ 4.7        25.          7.          6.54496265 17.83869375 35.        ]]\n",
      "48.04000000000002\n",
      "[[ 6.9        35.5         7.         33.61820722 28.80433755 34.        ]]\n",
      "66.68\n",
      "[[ 4.3   24.1    7.    45.634 47.017 33.   ]]\n",
      "47.879999999999995\n",
      "[[ 7.5        32.9        15.         44.92074392 17.67797698 32.        ]]\n",
      "54.28000000000003\n",
      "[[ 3.7        19.1        20.         58.99065376 27.66027517 31.        ]]\n",
      "35.96000000000001\n",
      "0\n",
      "[[ 5.         19.2        14.         43.75712195 30.3418571  29.        ]]\n",
      "27.439999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.9        44.1         5.          2.85048439 50.34350321 25.        ]]\n",
      "107.80000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.2        19.1         7.         19.94689437 54.42328066 21.        ]]\n",
      "12.159999999999997\n",
      "[[14.2        35.7         7.         36.59518461 26.38574748 20.        ]]\n",
      "17.67999999999995\n",
      "0\n",
      "0\n",
      "[[ 5.         17.4         9.         17.30178507 33.08382306 17.        ]]\n",
      "21.680000000000007\n",
      "0\n",
      "[[ 3.9        12.7         9.         28.25725487 37.57248425 15.        ]]\n",
      "14.120000000000019\n",
      "0\n",
      "[[ 6.5        37.2        10.         45.29117475 41.37001345 13.        ]]\n",
      "74.83999999999997\n",
      "0\n",
      "[[ 3.4   10.1   15.    51.622 29.306 11.   ]]\n",
      "9.200000000000003\n",
      "[[ 3.9        18.9        21.         29.57438835 34.70017446 10.        ]]\n",
      "33.960000000000036\n",
      "[[ 6.5        15.9        21.         30.35669491 24.35556912  9.        ]]\n",
      "6.680000000000007\n",
      "0\n",
      "0\n",
      "[[ 3.3        29.4         3.         33.31051275 14.59722801  6.        ]]\n",
      "71.64000000000004\n",
      "[[ 7.8   46.4    8.    22.352 56.874  5.   ]]\n",
      "95.44000000000005\n",
      "[[ 7.6        16.4         8.         18.04337359 36.58571338  4.        ]]\n",
      "0.8000000000000114\n",
      "0\n",
      "[[ 2.3         8.9         8.          0.82930487 20.27884421  2.        ]]\n",
      "12.840000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.1        13.4         9.         19.78222743 20.32272765 -1.        ]]\n",
      "21.799999999999997\n",
      "0\n",
      "0\n",
      "[[ 4.4        15.8         9.         18.85436229 23.78231818 -1.        ]]\n",
      "20.639999999999986\n",
      "0\n",
      "[[10.3        24.8        13.          9.53465684 38.07272347 -1.        ]]\n",
      "9.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 3.1        24.4        13.         25.99645707 25.46245085 -1.        ]]\n",
      "57.0\n",
      "0\n",
      "[[ 4.7        20.2        13.          7.61599263 10.39986823 -1.        ]]\n",
      "32.68000000000001\n",
      "0\n",
      "0\n",
      "[[ 5.4        29.9        14.         33.7435948  42.77358426 -1.        ]]\n",
      "58.960000000000036\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.7        29.4        15.         15.37402636 18.75702423 -1.        ]]\n",
      "21.32000000000005\n",
      "0\n",
      "0\n",
      "[[17.4        45.4        15.         46.22525968 30.96723255 -1.        ]]\n",
      "26.960000000000036\n",
      "driver reward  1149.3600000000006\n",
      "[[ 4.2        20.2         7.         13.2054256   3.13802804 40.        ]]\n",
      "36.08000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.6   11.2    9.     9.295 18.257 36.   ]]\n",
      "11.360000000000014\n",
      "0\n",
      "0\n",
      "[[ 5.2   30.6   13.    13.716 48.896 33.   ]]\n",
      "62.559999999999974\n",
      "[[ 3.9        27.8        13.         28.73449261 21.29952597 32.        ]]\n",
      "62.44\n",
      "0\n",
      "[[ 4.9        25.2        17.         40.16808913 42.6739857  30.        ]]\n",
      "47.31999999999999\n",
      "0\n",
      "0\n",
      "[[ 4.7        24.7        21.         37.84255965 20.18041346 27.        ]]\n",
      "47.08000000000001\n",
      "0\n",
      "0\n",
      "[[ 6.3        28.5        12.         13.42668841  4.82154484 24.        ]]\n",
      "48.360000000000014\n",
      "0\n",
      "[[ 1.9    5.2   14.     7.847 31.996 22.   ]]\n",
      "3.720000000000006\n",
      "[[ 5.4        14.2        14.         19.12314282 33.11091093 21.        ]]\n",
      "8.719999999999999\n",
      "0\n",
      "[[ 3.7        35.3        14.         35.42230336  4.3771728  19.        ]]\n",
      "87.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 6.5        31.          6.         27.23833128 52.85943519 16.        ]]\n",
      "55.0\n",
      "[[ 3.6   27.9    7.     8.228 34.911 15.   ]]\n",
      "64.80000000000001\n",
      "0\n",
      "[[ 4.9        11.6         7.          9.06136295 36.37713047 13.        ]]\n",
      "3.799999999999997\n",
      "[[10.3        29.2         7.         12.96099258 17.09013371 12.        ]]\n",
      "23.400000000000034\n",
      "0\n",
      "0\n",
      "[[ 4.4        20.9         8.          3.61177312 17.9831518   9.        ]]\n",
      "36.960000000000036\n",
      "0\n",
      "[[ 8.         22.6         9.         18.30849528 46.22739142  7.        ]]\n",
      "17.919999999999987\n",
      "0\n",
      "0\n",
      "[[ 4.5        29.6         9.         17.15281266  2.82333386  4.        ]]\n",
      "64.12\n",
      "0\n",
      "0\n",
      "[[ 6.5        14.5        12.          9.41071072 30.42539832  1.        ]]\n",
      "1502.2\n",
      "[[ 5.1        12.3        12.          3.72049468 46.87904781  0.        ]]\n",
      "4.680000000000007\n",
      "[[ 6.9        29.5        12.         20.9402229  21.88480005 -1.        ]]\n",
      "47.48000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.9        17.4        12.         16.83741571 57.35787696 -1.        ]]\n",
      "29.160000000000025\n",
      "[[16.1        41.2        13.         44.01133265 27.79551495 -1.        ]]\n",
      "22.359999999999957\n",
      "[[ 4.2   36.4   13.     6.405 26.516 -1.   ]]\n",
      "87.92000000000002\n",
      "[[ 3.1       18.5       13.         7.986373  48.0652438 -1.       ]]\n",
      "38.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.4        13.7        14.         11.31882928 37.07659836 -1.        ]]\n",
      "13.919999999999987\n",
      "0\n",
      "[[ 9.2        43.2        15.         31.2049037   9.02050246 -1.        ]]\n",
      "75.67999999999995\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.5        18.6        17.         12.28509375 22.70444026 -1.        ]]\n",
      "22.120000000000005\n",
      "[[11.3        25.3        17.         21.86533616 44.42600696 -1.        ]]\n",
      "4.1200000000000045\n",
      "[[ 7.9        18.1        17.         44.04745671 41.97971348 -1.        ]]\n",
      "4.200000000000017\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.6   32.4   13.    19.446 26.11  -1.   ]]\n",
      "58.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 2.         13.         14.         17.60369695 37.63954206 -1.        ]]\n",
      "28.0\n",
      "[[ 5.1        14.4        15.         16.16256746 28.10534584 -1.        ]]\n",
      "11.400000000000006\n",
      "0\n",
      "[[ 1.2        27.         17.         16.32341857 46.52772534 -1.        ]]\n",
      "78.24000000000001\n",
      "[[11.9        29.         17.          6.35247654 12.10190869 -1.        ]]\n",
      "11.879999999999995\n",
      "[[ 3.3        14.4        17.         20.71089772  7.96803917 -1.        ]]\n",
      "23.640000000000015\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.2        24.1        19.         19.32959726 29.88826073 -1.        ]]\n",
      "21.360000000000014\n",
      "0\n",
      "[[ 1.1        25.         19.         27.715318   41.33937079 -1.        ]]\n",
      "72.51999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.5        46.6        19.         53.46145356 42.78179706 -1.        ]]\n",
      "111.72000000000003\n",
      "[[ 2.7        30.         11.         46.44349516 14.4931565  -1.        ]]\n",
      "77.63999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.    12.4    8.     5.536 45.558 -1.   ]]\n",
      "32.879999999999995\n",
      "0\n",
      "0\n",
      "[[ 4.6        35.1         8.         37.87768335  9.43359324 -1.        ]]\n",
      "81.03999999999996\n",
      "0\n",
      "0\n",
      "[[ 5.2        16.2        18.         33.40154373  6.30307042 -1.        ]]\n",
      "16.480000000000018\n",
      "driver reward  3159.0\n",
      "[[ 2.3        15.2         1.         24.93903956 31.75784035 40.        ]]\n",
      "33.0\n",
      "[[11.4        24.9         9.          5.53389523  1.21414998 39.        ]]\n",
      "2.160000000000025\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.9   20.9   12.    12.637 33.9   36.   ]]\n",
      "19.960000000000036\n",
      "[[ 3.5        43.9        12.         54.62687566 32.67496865 35.        ]]\n",
      "116.68\n",
      "[[14.6   39.1   19.    12.053 27.627 34.   ]]\n",
      "25.839999999999975\n",
      "0\n",
      "[[ 1.8    5.8   19.     9.458 17.431 32.   ]]\n",
      "6.32\n",
      "0\n",
      "[[ 7.4        28.3        19.         25.30457287 50.06462159 30.        ]]\n",
      "40.23999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.7        19.5        21.         36.13902289 37.16938482 26.        ]]\n",
      "37.24000000000001\n",
      "0\n",
      "[[ 2.8        12.          3.         46.20008683 46.38487765 24.        ]]\n",
      "19.36\n",
      "[[ 5.4   46.3    9.    11.345 18.448 23.   ]]\n",
      "111.44000000000005\n",
      "[[ 2.2   23.2   11.     4.959 43.    22.   ]]\n",
      "59.28\n",
      "[[11.4        36.7        11.         50.54228694 46.16620187 21.        ]]\n",
      "39.920000000000016\n",
      "0\n",
      "0\n",
      "[[ 2.6   22.7    9.    41.319 35.606 18.   ]]\n",
      "54.96000000000001\n",
      "[[10.9   26.2   11.    42.095 18.17  17.   ]]\n",
      "9.719999999999999\n",
      "[[ 4.8        15.1         6.         22.95002911 23.03372716 16.        ]]\n",
      "15.680000000000007\n",
      "0\n",
      "0\n",
      "[[ 4.    28.2   13.    12.96  31.568 13.   ]]\n",
      "63.03999999999999\n",
      "[[ 5.8        19.1        13.         14.9664308  15.58149264 12.        ]]\n",
      "21.67999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.9   21.    15.     6.578 28.973  8.   ]]\n",
      "27.080000000000013\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.5         5.7        17.          2.46108124 55.30479972  4.        ]]\n",
      "1.240000000000009\n",
      "[[ 0.4         6.6        17.          7.31314957 50.21172802  3.        ]]\n",
      "18.4\n",
      "0\n",
      "0\n",
      "[[ 9.4   20.8   17.     1.948  4.621  0.   ]]\n",
      "2.6399999999999864\n",
      "0\n",
      "[[ 4.5        23.5        18.         25.14145756 40.63946196 -1.        ]]\n",
      "44.599999999999994\n",
      "[[ 9.2        25.1        18.         36.06240025 28.67543388 -1.        ]]\n",
      "17.76000000000002\n",
      "[[ 6.8   26.5   19.     4.011 32.357 -1.   ]]\n",
      "38.56000000000003\n",
      "[[ 1.6        19.3        19.         15.49169136 49.5125283  -1.        ]]\n",
      "50.879999999999995\n",
      "[[ 3.6        13.9        19.         26.20980474 49.79256318 -1.        ]]\n",
      "20.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.3        23.1        22.         45.76673648 42.19046339 -1.        ]]\n",
      "37.879999999999995\n",
      "[[ 7.    30.3    9.    10.824 30.088 -1.   ]]\n",
      "49.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.2        26.5        13.         42.45840725 23.4956081  -1.        ]]\n",
      "42.639999999999986\n",
      "[[ 6.         23.5        19.         29.23920091  5.71172368 -1.        ]]\n",
      "34.400000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[12.9        28.1        10.         26.83103146 31.57055175 -1.        ]]\n",
      "2.1999999999999886\n",
      "0\n",
      "0\n",
      "[[ 5.5        12.3        11.         23.53106943 57.1568157  -1.        ]]\n",
      "1.9599999999999937\n",
      "0\n",
      "0\n",
      "[[ 4.2        32.         12.         14.04589115 56.86199793 -1.        ]]\n",
      "73.83999999999997\n",
      "0\n",
      "[[ 9.4        22.3        13.         19.13397522 21.89640098 -1.        ]]\n",
      "7.439999999999998\n",
      "0\n",
      "[[ 3.6        42.3        13.         48.29279416  2.4023074  -1.        ]]\n",
      "110.88\n",
      "0\n",
      "driver reward  1258.2800000000002\n",
      "[[ 3.3        17.6         6.         13.02072449 20.97807819 40.        ]]\n",
      "33.879999999999995\n",
      "[[ 1.6        16.1         6.         24.57229713  9.30405995 39.        ]]\n",
      "40.639999999999986\n",
      "[[ 9.1   32.7    9.    16.71  42.942 38.   ]]\n",
      "42.75999999999999\n",
      "[[ 2.6        16.2         9.         32.27985763 37.83962416 37.        ]]\n",
      "34.16\n",
      "0\n",
      "[[ 5.7        17.8        12.         15.40101974 21.35843877 35.        ]]\n",
      "18.200000000000017\n",
      "[[ 1.3   23.8   12.    12.576 44.286 34.   ]]\n",
      "67.32\n",
      "0\n",
      "0\n",
      "[[ 3.3   19.2   13.    14.516 33.928 31.   ]]\n",
      "39.0\n",
      "0\n",
      "[[ 4.1        19.8        13.         14.23401888 11.92964954 29.        ]]\n",
      "35.48000000000002\n",
      "0\n",
      "[[ 2.5        25.5        14.         12.46149016 52.45101981 27.        ]]\n",
      "64.6\n",
      "0\n",
      "[[ 2.4        13.5        14.         14.02564814 44.83595902 25.        ]]\n",
      "26.879999999999995\n",
      "0\n",
      "[[11.2        23.9        15.          0.49423329 15.93705432 23.        ]]\n",
      "0.32000000000005\n",
      "0\n",
      "[[ 0.2   11.4   16.    16.078 20.594 21.   ]]\n",
      "35.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.6        28.7        18.         22.40218833 16.53478924 17.        ]]\n",
      "33.360000000000014\n",
      "0\n",
      "0\n",
      "[[22.9   49.4   10.    20.989 32.248 14.   ]]\n",
      "2.3600000000000136\n",
      "0\n",
      "0\n",
      "[[ 7.5   40.6   12.    45.848 48.107 11.   ]]\n",
      "78.92000000000002\n",
      "[[12.4   28.    19.     7.567 46.686 10.   ]]\n",
      "5.28000000000003\n",
      "[[ 0.6        16.8        19.         14.98826555 31.96179082  9.        ]]\n",
      "49.67999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.4   17.3   20.     7.258 14.959  5.   ]]\n",
      "25.439999999999998\n",
      "[[ 4.6        13.7        21.          8.62662719  4.91981334  4.        ]]\n",
      "12.560000000000016\n",
      "0\n",
      "[[ 8.2   28.4   17.     5.846 28.209  2.   ]]\n",
      "35.12000000000003\n",
      "[[ 3.4        11.5        17.         18.53488333 36.14598852  1.        ]]\n",
      "1513.68\n",
      "[[ 2.2        31.9        17.         48.57835351 47.59196667  0.        ]]\n",
      "87.12\n",
      "0\n",
      "0\n",
      "[[ 4.1   43.3    8.    17.315 44.141 -1.   ]]\n",
      "110.68\n",
      "[[ 7.3   22.3    8.     3.272 18.818 -1.   ]]\n",
      "21.72\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 0.9         4.8        11.          0.94162864 53.83151274 -1.        ]]\n",
      "9.240000000000002\n",
      "0\n",
      "0\n",
      "[[ 6.         30.6        12.         31.71495835  0.08873327 -1.        ]]\n",
      "57.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.6        10.8        14.          5.77943955 33.81687814 -1.        ]]\n",
      "16.879999999999995\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.4   20.6   15.     5.529 39.359 -1.   ]]\n",
      "8.800000000000011\n",
      "[[ 4.7        12.7        15.         15.58292199 43.87945917 -1.        ]]\n",
      "8.680000000000007\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.3        21.6        16.         20.18660791 35.96579888 -1.        ]]\n",
      "26.28\n",
      "0\n",
      "[[10.9        25.8        16.          2.23691562 47.64069083 -1.        ]]\n",
      "8.439999999999998\n",
      "0\n",
      "[[ 3.8        16.7        17.         20.04474242 52.90254896 -1.        ]]\n",
      "27.599999999999994\n",
      "0\n",
      "[[ 2.4        10.3        17.         15.32473277 42.03617088 -1.        ]]\n",
      "16.64\n",
      "0\n",
      "[[ 5.7        13.9        17.         18.4238164  48.02508174 -1.        ]]\n",
      "5.719999999999999\n",
      "0\n",
      "0\n",
      "[[17.1        37.5        19.         22.19446878 12.1908013  -1.        ]]\n",
      "3.7200000000000273\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.7        15.7        12.         21.08102581 34.92395461 -1.        ]]\n",
      "11.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.8        35.1        12.         40.37402115 49.68754047 -1.        ]]\n",
      "66.08000000000004\n",
      "0\n",
      "0\n",
      "[[ 2.6        22.         12.         16.25494087 41.55693889 -1.        ]]\n",
      "52.72\n",
      "[[ 4.6        20.3        12.          7.98344213 57.24501825 -1.        ]]\n",
      "33.68000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.9         8.         13.         13.16810513 33.2154033  -1.        ]]\n",
      "5.8799999999999955\n",
      "[[ 1.         15.4        13.          8.70281957 48.74971975 -1.        ]]\n",
      "42.48000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "driver reward  2815.7199999999993\n",
      "[[ 6.8   20.8    5.    33.317 30.845 40.   ]]\n",
      "20.319999999999993\n",
      "[[ 6.1        28.8         7.         14.26178117 17.76475486 39.        ]]\n",
      "50.68000000000001\n",
      "[[ 4.8        17.5         8.         27.06357775 17.76876455 38.        ]]\n",
      "23.359999999999985\n",
      "[[ 2.2   28.7    8.     4.005 37.927 37.   ]]\n",
      "76.88000000000002\n",
      "0\n",
      "0\n",
      "[[ 1.4        16.8        12.         14.33953136 30.35260072 34.        ]]\n",
      "44.24000000000001\n",
      "[[13.5        42.6        12.         36.10266978 53.27281638 33.        ]]\n",
      "44.51999999999998\n",
      "[[ 1.5         5.4        16.         30.85030311 51.617617   32.        ]]\n",
      "7.079999999999998\n",
      "0\n",
      "[[ 0.5        23.4        20.         36.74916609 19.98997783 30.        ]]\n",
      "71.48000000000002\n",
      "0\n",
      "0\n",
      "[[ 5.3        12.          8.         19.51986921 11.0875369  27.        ]]\n",
      "2.3599999999999994\n",
      "[[ 4.6        37.7         9.         35.31978527 43.27970687 26.        ]]\n",
      "89.35999999999996\n",
      "0\n",
      "[[10.7        25.9         9.         33.10140411 55.12565608 24.        ]]\n",
      "10.120000000000033\n",
      "0\n",
      "[[ 4.         30.8        11.         11.97715085 31.83405176 22.        ]]\n",
      "71.36000000000001\n",
      "0\n",
      "[[ 7.6        16.7        12.         27.97127746 37.66827888 20.        ]]\n",
      "1.7600000000000193\n",
      "0\n",
      "[[ 4.5        25.1        15.         16.42565609 44.07315388 18.        ]]\n",
      "49.72\n",
      "[[ 5.8   23.6   15.     2.078 29.824 17.   ]]\n",
      "36.079999999999984\n",
      "[[ 1.6        33.5        16.         29.56250335 51.17623051 16.        ]]\n",
      "96.32\n",
      "[[ 7.    18.9   16.    48.654 57.852 15.   ]]\n",
      "12.880000000000024\n",
      "0\n",
      "0\n",
      "[[ 5.5        20.7        11.         48.84993747 37.48083765 12.        ]]\n",
      "28.840000000000003\n",
      "[[ 2.1        20.8        13.         27.02595797 44.15744678 11.        ]]\n",
      "52.28\n",
      "[[ 2.9   22.2   14.     5.785 52.229 10.   ]]\n",
      "51.32000000000002\n",
      "0\n",
      "0\n",
      "[[ 5.9        19.1        14.         35.27340037 42.0253702   7.        ]]\n",
      "21.0\n",
      "[[ 2.2        12.2        17.         31.55389946 31.19267885  6.        ]]\n",
      "24.080000000000013\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.2         7.5        17.         10.78056582 16.57002907  2.        ]]\n",
      "2.240000000000009\n",
      "[[ 7.1        30.3        17.         38.30645804 25.62225682  1.        ]]\n",
      "1548.68\n",
      "[[ 8.8   27.2   18.     8.866 46.16   0.   ]]\n",
      "27.200000000000017\n",
      "[[ 1.7        17.3        18.         16.91173571 30.57900994 -1.        ]]\n",
      "43.80000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.6        27.6        19.         20.40122621  2.13824839 -1.        ]]\n",
      "43.44\n",
      "0\n",
      "[[ 3.1   19.6   19.     3.32  46.688 -1.   ]]\n",
      "41.639999999999986\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.8        20.5        20.         23.28684872  5.96078149 -1.        ]]\n",
      "19.360000000000014\n",
      "0\n",
      "0\n",
      "[[ 2.4         8.3        22.         23.66501577 26.94812727 -1.        ]]\n",
      "10.239999999999995\n",
      "0\n",
      "0\n",
      "0\n",
      "[[16.2        37.6         5.         50.58650525 36.85815172 -1.        ]]\n",
      "10.160000000000025\n",
      "0\n",
      "[[ 4.1        18.4        18.         42.10658762 48.09243092 -1.        ]]\n",
      "31.0\n",
      "[[ 7.4        33.4        19.         38.01275492  8.6671361  -1.        ]]\n",
      "56.56\n",
      "driver reward  2720.3599999999997\n",
      "[[ 8.1   20.3    5.    11.661 48.691 40.   ]]\n",
      "9.880000000000024\n",
      "0\n",
      "0\n",
      "[[ 9.3        27.3         8.          2.76888332 31.70400845 37.        ]]\n",
      "24.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.6        30.8        18.          2.61885194 23.79041712 31.        ]]\n",
      "40.08000000000004\n",
      "[[ 3.5        17.5        18.         22.80307247 17.91257076 30.        ]]\n",
      "32.20000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.8   11.4   19.     6.967 42.268 26.   ]]\n",
      "3.8400000000000034\n",
      "[[ 1.         12.2        19.         16.88589302 48.00400876 25.        ]]\n",
      "32.24000000000001\n",
      "0\n",
      "[[ 8.1        32.4        19.         59.61273179 50.64983072 23.        ]]\n",
      "48.60000000000002\n",
      "[[ 7.7        21.4         9.         42.60192746 27.04575098 22.        ]]\n",
      "16.120000000000033\n",
      "[[ 5.    10.7    9.    51.337 26.052 21.   ]]\n",
      "0.2400000000000091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 7.8        20.1        14.         42.25464236 26.36864042 19.        ]]\n",
      "11.280000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.         51.9        21.         55.58517366 15.7784101  15.        ]]\n",
      "125.28000000000003\n",
      "[[ 1.9        13.         14.         44.43378213 22.05373861 14.        ]]\n",
      "28.680000000000007\n",
      "0\n",
      "0\n",
      "[[ 2.8        13.1        10.          5.28562754 48.91377433 11.        ]]\n",
      "22.88000000000001\n",
      "[[ 6.8        24.6        10.         27.27114023 40.64308007 10.        ]]\n",
      "32.47999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.1        10.2        12.         10.11943659 43.17489471  5.        ]]\n",
      "11.560000000000016\n",
      "0\n",
      "[[11.9        25.6        12.          1.10155598 56.73590535  3.        ]]\n",
      "1.0\n",
      "[[13.6        46.6        12.         46.31159771 21.28714719  2.        ]]\n",
      "56.639999999999986\n",
      "[[ 5.7   40.    14.    20.792 51.781  1.   ]]\n",
      "1589.24\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.1        29.9        16.         26.79325349 37.29710709 -1.        ]]\n",
      "6.600000000000023\n",
      "0\n",
      "0\n",
      "[[ 5.9        25.7        17.         46.78407233 50.71780874 -1.        ]]\n",
      "42.120000000000005\n",
      "[[ 8.4        18.9         7.         24.61222912 44.01751707 -1.        ]]\n",
      "3.3600000000000136\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.3        23.5         7.          0.76733694 15.32990176 -1.        ]]\n",
      "18.75999999999999\n",
      "[[ 1.8   18.5    9.     9.098 33.697 -1.   ]]\n",
      "46.96000000000001\n",
      "0\n",
      "0\n",
      "[[ 7.9        25.1         9.         27.94131836 55.21091138 -1.        ]]\n",
      "26.599999999999994\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.9        32.3        11.         40.61602099 49.2003528  -1.        ]]\n",
      "2.0400000000000205\n",
      "[[ 6.8   45.    12.     2.921 56.405 -1.   ]]\n",
      "97.76000000000005\n",
      "0\n",
      "[[ 7.5        26.6        13.         19.65376436 22.3475868  -1.        ]]\n",
      "34.120000000000005\n",
      "[[ 2.6        13.4        13.          9.17686735 10.68577395 -1.        ]]\n",
      "25.200000000000003\n",
      "[[ 4.1   17.4   14.     3.991 31.606 -1.   ]]\n",
      "27.80000000000001\n",
      "[[ 6.5        19.3        14.          7.09663155 14.72651555 -1.        ]]\n",
      "17.560000000000002\n",
      "0\n",
      "[[ 5.6        20.2        15.         30.07657783 18.20103341 -1.        ]]\n",
      "26.56000000000003\n",
      "[[ 3.3        22.6        16.          5.93215261 26.67664973 -1.        ]]\n",
      "49.879999999999995\n",
      "[[ 0.7        10.8        16.         16.18086267 26.43673271 -1.        ]]\n",
      "29.799999999999997\n",
      "[[ 5.3        17.         17.         34.92798304 15.31029446 -1.        ]]\n",
      "18.359999999999985\n",
      "0\n",
      "[[ 7.5        16.2        19.         20.43968576 36.88478988 -1.        ]]\n",
      "0.8400000000000034\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2        11.1        20.         14.92239928 50.89871955 -1.        ]]\n",
      "0.1599999999999966\n",
      "[[ 6.4        31.6        20.          0.8965574  15.93361556 -1.        ]]\n",
      "57.60000000000002\n",
      "0\n",
      "[[ 4.6        28.2        20.         35.27829923  5.85179855 -1.        ]]\n",
      "58.960000000000036\n",
      "[[ 5.4   30.     6.    11.982 13.672 -1.   ]]\n",
      "59.28000000000003\n",
      "[[11.7        29.6         7.         36.23382706 32.55211732 -1.        ]]\n",
      "15.160000000000025\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.5        17.2         9.         27.56950338 27.38498566 -1.        ]]\n",
      "4.0400000000000205\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.5        20.3        15.         35.84772532 52.88552991 -1.        ]]\n",
      "34.360000000000014\n",
      "[[ 3.8        16.8        16.         28.536327   34.08557687 -1.        ]]\n",
      "27.919999999999987\n",
      "[[15.5        35.8        16.         57.37787609 41.67919509 -1.        ]]\n",
      "9.160000000000025\n",
      "0\n",
      "driver reward  2827.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.7        31.9        11.         45.86291636 52.07700384 36.        ]]\n",
      "70.12\n",
      "[[ 6.1        15.1        18.         50.31389378 43.91380973 35.        ]]\n",
      "6.840000000000003\n",
      "[[ 0.6         8.6         8.         57.65545603 40.03064111 34.        ]]\n",
      "23.440000000000005\n",
      "[[ 9.1        37.3        13.         49.93996076 12.2568824  33.        ]]\n",
      "57.48000000000002\n",
      "[[ 7.3        24.6        20.         28.19034148 22.61053757 32.        ]]\n",
      "29.079999999999984\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.7        15.3         9.         11.17213581 57.31790549 24.        ]]\n",
      "37.400000000000006\n",
      "0\n",
      "0\n",
      "[[ 5.9        21.6        12.         28.4119775  28.75796186 21.        ]]\n",
      "29.0\n",
      "[[ 2.4   24.7   12.     7.041 18.279 20.   ]]\n",
      "62.72000000000003\n",
      "0\n",
      "0\n",
      "[[ 3.4         7.7        12.          7.69519056 56.22556816 17.        ]]\n",
      "1.5200000000000102\n",
      "[[ 5.5        25.2        12.          2.02217438 26.85234891 16.        ]]\n",
      "43.24000000000001\n",
      "0\n",
      "[[ 3.5        19.1        12.         18.58345879 21.31373974 14.        ]]\n",
      "37.31999999999999\n",
      "0\n",
      "[[ 9.3   20.1   12.     4.824 32.1   12.   ]]\n",
      "1.079999999999984\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.4        30.6        13.         37.4208398  42.06600589  7.        ]]\n",
      "54.400000000000006\n",
      "0\n",
      "[[ 2.1        12.8        17.         40.94557998 50.30016886  5.        ]]\n",
      "26.680000000000007\n",
      "[[ 1.4   45.6   17.     6.159 22.634  4.   ]]\n",
      "136.40000000000003\n",
      "[[ 8.9        19.1        17.         15.9223639  47.02886815  3.        ]]\n",
      "0.5999999999999943\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.4        12.         17.         35.23055686 42.87136609 -1.        ]]\n",
      "1.6800000000000068\n",
      "[[ 4.3        22.6        19.         42.52835854 25.9810165  -1.        ]]\n",
      "43.079999999999984\n",
      "0\n",
      "[[ 5.2   19.4    8.     1.972 31.06  -1.   ]]\n",
      "26.720000000000027\n",
      "0\n",
      "0\n",
      "[[ 7.4        17.8         8.         19.48934534 36.04594283 -1.        ]]\n",
      "6.639999999999986\n",
      "[[16.5        39.2         8.         38.95130786 24.23645629 -1.        ]]\n",
      "13.240000000000009\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.1        31.3        10.         49.91112704 46.82655586 -1.        ]]\n",
      "31.480000000000018\n",
      "0\n",
      "0\n",
      "[[ 6.8        16.8        12.         23.95548592 19.53486696 -1.        ]]\n",
      "7.519999999999982\n",
      "0\n",
      "0\n",
      "[[10.4        22.6        13.         27.23466293 24.14168739 -1.        ]]\n",
      "1.5999999999999943\n",
      "0\n",
      "0\n",
      "[[ 6.2        18.6        17.          7.11154504 48.94278529 -1.        ]]\n",
      "17.360000000000014\n",
      "0\n",
      "[[ 6.1        23.3        17.         21.34861454 16.39897232 -1.        ]]\n",
      "33.08000000000001\n",
      "0\n",
      "[[ 1.1        47.8        17.         53.27996214 20.90872871 -1.        ]]\n",
      "145.48000000000002\n",
      "[[ 6.7   29.8   17.    31.63  29.056 -1.   ]]\n",
      "49.80000000000001\n",
      "0\n",
      "[[ 3.1         7.9        18.          1.97806386 37.39400665 -1.        ]]\n",
      "4.200000000000003\n",
      "0\n",
      "0\n",
      "[[ 5.8        12.9        18.         20.54974187 41.58432379 -1.        ]]\n",
      "1.8400000000000034\n",
      "0\n",
      "0\n",
      "[[15.5        45.3        19.         43.51749436 28.52308327 -1.        ]]\n",
      "39.56\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.2        22.         20.          6.24288937 20.96871778 -1.        ]]\n",
      "28.24000000000001\n",
      "[[ 3.7         8.9        20.         12.2522013  25.04339921 -1.        ]]\n",
      "3.319999999999993\n",
      "[[ 7.2        32.         20.         31.10989473  1.24106328 -1.        ]]\n",
      "53.44\n",
      "driver reward  1125.6000000000001\n",
      "[[ 1.9   27.7    8.    14.492 60.    40.   ]]\n",
      "75.72000000000003\n",
      "[[ 2.1         8.6         8.         16.90961739 50.48410157 39.        ]]\n",
      "13.240000000000009\n",
      "0\n",
      "[[ 7.1        17.3         9.         22.34903085 33.55102187 37.        ]]\n",
      "7.0800000000000125\n",
      "[[ 5.2        11.1        10.         21.42004184 47.00305473 36.        ]]\n",
      "0.1599999999999966\n",
      "[[12.3        27.5        10.         32.49181953 36.4790786  35.        ]]\n",
      "4.360000000000014\n",
      "0\n",
      "[[ 6.7        18.7        13.         24.1567011  27.78216967 33.        ]]\n",
      "14.280000000000001\n",
      "[[ 2.9   22.8   13.    13.996 51.293 32.   ]]\n",
      "53.24000000000001\n",
      "0\n",
      "[[ 3.6        33.9        14.         27.55205046 19.8657774  30.        ]]\n",
      "84.0\n",
      "0\n",
      "[[ 5.6   33.1   17.     7.165 43.351 28.   ]]\n",
      "67.83999999999997\n",
      "[[ 1.         13.6        17.          6.79511711 30.17604333 27.        ]]\n",
      "36.72\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.7        16.         18.         22.17723111 44.89507415 23.        ]]\n",
      "26.04000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.9        41.5         7.         39.03195744 22.9096902  12.        ]]\n",
      "99.48000000000002\n",
      "0\n",
      "[[ 5.4        38.6        11.         40.96984863 54.8777167  10.        ]]\n",
      "86.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 6.3        14.1        15.         19.88296987 38.00564758  7.        ]]\n",
      "2.280000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.1        17.6         8.         19.21570013 29.72815017 -1.        ]]\n",
      "14.839999999999975\n",
      "[[ 3.9   11.1    8.     8.519 38.516 -1.   ]]\n",
      "9.0\n",
      "[[ 6.8        40.3         8.         47.17024328 51.96935179 -1.        ]]\n",
      "82.72000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.5        33.9        20.         39.01225051 42.0816239  -1.        ]]\n",
      "91.48000000000002\n",
      "[[ 4.         15.4        12.         54.13190759 38.8893723  -1.        ]]\n",
      "22.080000000000013\n",
      "[[ 2.2         9.4        17.         48.3046387  43.25573848 -1.        ]]\n",
      "15.11999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.2   11.8    9.     4.819 34.997 -1.   ]]\n",
      "16.0\n",
      "[[ 3.6        17.6         9.         18.77302576 28.63472557 -1.        ]]\n",
      "31.839999999999975\n",
      "0\n",
      "[[ 4.3       29.4       12.        11.8541165 55.6914082 -1.       ]]\n",
      "64.84000000000003\n",
      "[[ 8.9        19.7        13.         16.87033663 41.2616185  -1.        ]]\n",
      "2.519999999999982\n",
      "0\n",
      "0\n",
      "[[ 8.3   19.8   14.    20.567 39.044 -1.   ]]\n",
      "6.9199999999999875\n",
      "[[ 3.2        19.7        14.          6.31978268 56.73121664 -1.        ]]\n",
      "41.28\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.8        35.2        17.         26.51257915  8.94647988 -1.        ]]\n",
      "59.60000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.9        13.8         8.         46.81986089 50.45460943 -1.        ]]\n",
      "4.039999999999992\n",
      "0\n",
      "0\n",
      "[[ 3.5        14.2        17.          5.73286122 35.60901998 -1.        ]]\n",
      "21.640000000000015\n",
      "[[ 6.5        14.4        17.         15.29288569 46.67940263 -1.        ]]\n",
      "1.8800000000000239\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.5        27.         18.          9.14231136 18.78427787 -1.        ]]\n",
      "49.0\n",
      "[[ 6.6        22.3        18.         26.27068991 30.29221961 -1.        ]]\n",
      "26.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.1       25.7       18.        30.2521175 21.7638176 -1.       ]]\n",
      "74.75999999999999\n",
      "[[ 2.2        11.8        18.         26.47253359  9.6413702  -1.        ]]\n",
      "22.799999999999997\n",
      "0\n",
      "[[14.         42.6        18.         50.43247007 32.48660683 -1.        ]]\n",
      "41.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.5        29.4        10.         49.49310894 29.24019998 -1.        ]]\n",
      "15.879999999999995\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.8   12.2   17.     6.447 43.316 -1.   ]]\n",
      "6.400000000000006\n",
      "0\n",
      "[[ 3.4         8.9        17.         15.83368964 50.73191244 -1.        ]]\n",
      "5.359999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.1        23.4        18.         10.66636    13.87538693 -1.        ]]\n",
      "19.80000000000001\n",
      "0\n",
      "[[ 6.         20.9        19.         19.75880494 15.85576125 -1.        ]]\n",
      "26.080000000000013\n",
      "[[ 8.2        21.         20.          4.94519289 11.87822231 -1.        ]]\n",
      "11.439999999999998\n",
      "0\n",
      "[[ 8.9        23.9        20.         23.65770678  6.59425757 -1.        ]]\n",
      "15.960000000000036\n",
      "driver reward  1372.1200000000003\n",
      "[[ 4.8        20.3         9.         32.30992297 54.26509248 40.        ]]\n",
      "32.31999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.    15.3   13.     4.41  39.854 36.   ]]\n",
      "21.75999999999999\n",
      "0\n",
      "0\n",
      "[[ 3.    10.4   14.    13.864 51.747 33.   ]]\n",
      "12.879999999999995\n",
      "[[ 7.1        16.         14.         11.43143741 34.35208904 32.        ]]\n",
      "2.9199999999999875\n",
      "0\n",
      "0\n",
      "[[ 4.2        27.8        15.         45.45695948 51.74676998 29.        ]]\n",
      "60.400000000000006\n",
      "[[ 8.         21.          9.         45.27991012 28.64975588 28.        ]]\n",
      "12.800000000000011\n",
      "[[ 1.8        15.9         9.         32.67743136 35.77009546 27.        ]]\n",
      "38.640000000000015\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.         30.1         7.         29.67736689 10.5486055  15.        ]]\n",
      "28.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 3.1        36.8         9.         10.44786095 59.22723281 12.        ]]\n",
      "96.68\n",
      "[[ 9.3        32.5         9.         34.84066364 28.25557846 11.        ]]\n",
      "40.76000000000005\n",
      "0\n",
      "[[ 6.         32.6        11.         28.38029199 39.32056244  9.        ]]\n",
      "63.51999999999998\n",
      "[[ 1.7        13.1        11.         32.69197366 51.57009833  8.        ]]\n",
      "30.360000000000014\n",
      "0\n",
      "[[ 6.7        22.         14.         10.23703998 26.09864785  6.        ]]\n",
      "24.840000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.6        10.5        14.          8.02218108 22.49195539  2.        ]]\n",
      "9.120000000000005\n",
      "[[ 7.5        19.6        14.         25.1074995  22.56144684  1.        ]]\n",
      "1511.72\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2        35.3        15.         42.56738584 19.35270657 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 6.5   40.2    8.     9.019 30.436 -1.   ]]\n",
      "84.44\n",
      "0\n",
      "[[ 5.1        31.7         9.         36.91912382 57.01097464 -1.        ]]\n",
      "66.76000000000002\n",
      "0\n",
      "0\n",
      "[[ 4.7        16.4        18.         24.62874688 47.64730065 -1.        ]]\n",
      "20.52000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.         26.4        19.         20.12121533  5.66670173 -1.        ]]\n",
      "36.880000000000024\n",
      "[[11.6   26.8   20.    11.095 39.292 -1.   ]]\n",
      "6.8799999999999955\n",
      "0\n",
      "[[ 3.9        32.4        20.         33.68697164 31.40708516 -1.        ]]\n",
      "77.16000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.4    5.9    6.    26.613 39.061 -1.   ]]\n",
      "9.36\n",
      "0\n",
      "[[ 3.5        13.7         7.         15.67530809 50.13580015 -1.        ]]\n",
      "20.040000000000006\n",
      "[[12.7        29.3         7.         36.95323225 59.17856342 -1.        ]]\n",
      "7.400000000000034\n",
      "0\n",
      "[[ 6.6   17.3    8.     4.221 43.586 -1.   ]]\n",
      "10.480000000000018\n",
      "[[ 0.6        18.          8.          9.84592984 25.87197691 -1.        ]]\n",
      "53.519999999999996\n",
      "[[ 5.3   23.     8.     4.678 47.865 -1.   ]]\n",
      "37.56\n",
      "[[ 3.5        24.7         8.          5.42978686 23.96164719 -1.        ]]\n",
      "55.24000000000001\n",
      "0\n",
      "[[ 3.         30.1         9.         32.4369614  43.84742933 -1.        ]]\n",
      "75.91999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.2        18.         11.         10.6595841  53.59718523 -1.        ]]\n",
      "42.640000000000015\n",
      "[[ 5.2        12.2        12.         10.87301342 45.00342405 -1.        ]]\n",
      "3.680000000000007\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.9        16.         16.         23.01096654 12.82058837 -1.        ]]\n",
      "4.280000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.4   34.    18.    31.689  4.699 -1.   ]]\n",
      "65.28000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.9    8.7    5.     5.522 41.079 -1.   ]]\n",
      "8.120000000000005\n",
      "0\n",
      "0\n",
      "[[ 3.7        19.6         6.          5.31070741 56.2310067  -1.        ]]\n",
      "37.56\n",
      "0\n",
      "[[10.         26.8         7.         52.93656265 50.66680839 -1.        ]]\n",
      "17.76000000000002\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "driver reward  2806.1200000000003\n",
      "[[ 6.8   21.3    9.     1.126 23.789 40.   ]]\n",
      "21.919999999999987\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.6        20.1        12.         33.16158278 53.34811225 36.        ]]\n",
      "53.44\n",
      "[[ 3.3   22.1   13.    13.867 36.995 35.   ]]\n",
      "48.28\n",
      "0\n",
      "[[ 4.9        32.3        13.         41.7433306  30.04729416 33.        ]]\n",
      "70.04000000000005\n",
      "[[ 7.7        28.8        20.         53.22145488  8.64956664 32.        ]]\n",
      "39.80000000000001\n",
      "0\n",
      "[[14.1   42.6   11.    22.036 30.844 30.   ]]\n",
      "40.44\n",
      "[[ 1.5        18.7        12.         16.27869454 47.85361327 29.        ]]\n",
      "49.640000000000015\n",
      "0\n",
      "0\n",
      "[[10.1        24.7        12.         21.61443451 36.60901182 26.        ]]\n",
      "10.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.3        15.9        12.         26.1461981  47.51552312 22.        ]]\n",
      "8.04000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.         17.3        13.         25.27311976 31.90668563 17.        ]]\n",
      "0.960000000000008\n",
      "0\n",
      "[[ 4.8        10.5        13.         11.44283077 37.6390886  15.        ]]\n",
      "0.9599999999999937\n",
      "[[11.4        34.1        13.         37.73893491 33.12362746 14.        ]]\n",
      "31.600000000000023\n",
      "[[ 1.6    7.7   14.    43.951 38.607 13.   ]]\n",
      "13.759999999999998\n",
      "0\n",
      "[[ 5.5        20.1        19.         22.34247298 49.7049571  11.        ]]\n",
      "26.919999999999987\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.1        49.3        21.         35.56274631 10.77262667  4.        ]]\n",
      "61.879999999999995\n",
      "0\n",
      "0\n",
      "[[ 3.6        17.7        17.         24.95137896 24.56335119  1.        ]]\n",
      "1532.16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.3        18.4        19.         21.47491045 41.83325292 -1.        ]]\n",
      "36.44\n",
      "0\n",
      "0\n",
      "[[10.2        35.1        20.         24.50378597 15.70292933 -1.        ]]\n",
      "42.960000000000036\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.1        16.8        21.         16.51257317 39.53162301 -1.        ]]\n",
      "19.080000000000013\n",
      "0\n",
      "0\n",
      "[[ 6.4        15.7        22.         18.902406   48.89878296 -1.        ]]\n",
      "6.719999999999999\n",
      "0\n",
      "0\n",
      "[[ 8.6   25.9    7.     9.643 32.642 -1.   ]]\n",
      "24.400000000000006\n",
      "[[ 0.4         9.9         7.         18.8815343  34.94135315 -1.        ]]\n",
      "28.959999999999994\n",
      "0\n",
      "0\n",
      "[[11.         31.8         7.         36.79112015 20.46832595 -1.        ]]\n",
      "26.960000000000036\n",
      "0\n",
      "0\n",
      "[[ 3.8        14.3         9.         18.53128096  5.11875981 -1.        ]]\n",
      "19.919999999999987\n",
      "0\n",
      "[[ 3.2   17.8   13.     3.244 19.73  -1.   ]]\n",
      "35.20000000000002\n",
      "[[ 7.2        17.3        13.          0.53410642 32.10193098 -1.        ]]\n",
      "6.400000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.6        26.6        14.         26.97549094 29.23582542 -1.        ]]\n",
      "33.44\n",
      "[[ 3.3  13.1  14.   13.66 36.26 -1.  ]]\n",
      "19.480000000000018\n",
      "0\n",
      "0\n",
      "[[ 7.1        16.7        17.          3.64532307 34.2854898  -1.        ]]\n",
      "5.160000000000025\n",
      "0\n",
      "[[12.1        42.4        17.         38.94424905 18.64295261 -1.        ]]\n",
      "53.400000000000034\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.         26.6        20.         30.13072857 29.42071058 -1.        ]]\n",
      "23.919999999999987\n",
      "0\n",
      "0\n",
      "[[ 3.4        11.4         7.         38.01267262 35.58598613 -1.        ]]\n",
      "13.36\n",
      "[[ 7.5        43.          7.          1.66712865  0.64328555 -1.        ]]\n",
      "86.60000000000002\n",
      "0\n",
      "0\n",
      "[[ 5.5        31.7        12.         22.88190127 12.99812741 -1.        ]]\n",
      "64.03999999999999\n",
      "[[ 4.2   13.4   14.     5.374 13.397 -1.   ]]\n",
      "14.319999999999993\n",
      "[[ 3.2        14.3        14.          3.64513722  1.48997072 -1.        ]]\n",
      "24.0\n",
      "[[ 1.6        32.2        18.         32.00255721 19.90878154 -1.        ]]\n",
      "92.15999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.9        19.5        19.         13.08951646 26.50840711 -1.        ]]\n",
      "8.680000000000007\n",
      "0\n",
      "0\n",
      "[[ 9.4        31.5        19.         32.82506277 45.30042964 -1.        ]]\n",
      "36.879999999999995\n",
      "0\n",
      "[[ 9.4   23.5   20.     9.988 21.047 -1.   ]]\n",
      "11.28000000000003\n",
      "[[ 7.3   26.7   20.    26.138  1.812 -1.   ]]\n",
      "35.80000000000001\n",
      "0\n",
      "[[13.6        31.8        21.         31.71925571  8.8805083  -1.        ]]\n",
      "9.28000000000003\n",
      "0\n",
      "[[ 1.7        12.5         8.         23.03567673 24.23116427 -1.        ]]\n",
      "28.440000000000012\n",
      "[[ 4.3   15.4    8.     5.29  28.918 -1.   ]]\n",
      "20.04000000000002\n",
      "0\n",
      "[[ 3.3        26.9         8.         32.75312083 23.74094657 -1.        ]]\n",
      "63.640000000000015\n",
      "[[ 7.6        29.6         9.         44.11811247  3.60712727 -1.        ]]\n",
      "43.03999999999999\n",
      "0\n",
      "0\n",
      "[[ 8.7   26.2   17.     0.134 37.586 -1.   ]]\n",
      "24.680000000000007\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.7        15.3        18.         35.00429627 26.47183565 -1.        ]]\n",
      "30.60000000000001\n",
      "[[ 5.2   15.9   18.    22.143 17.407 -1.   ]]\n",
      "15.519999999999982\n",
      "0\n",
      "[[ 6.8        17.6        21.         17.96224165 12.77064964 -1.        ]]\n",
      "10.079999999999984\n",
      "0\n",
      "[[12.7        27.4        21.         22.25814524 30.98182109 -1.        ]]\n",
      "1.32000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "driver reward  3026.4000000000005\n",
      "[[ 4.3        35.6         6.         16.45340871 28.87283074 40.        ]]\n",
      "84.68\n",
      "0\n",
      "[[ 5.2        21.4         8.         24.94559287 40.91664577 38.        ]]\n",
      "33.12000000000003\n",
      "[[ 1.7         5.7         9.         30.88540023 45.30227992 37.        ]]\n",
      "6.68\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.         26.5        13.         11.91317199 16.18787663 33.        ]]\n",
      "44.0\n",
      "0\n",
      "[[ 5.1        13.8        14.         13.61454627 29.98145518 31.        ]]\n",
      "9.480000000000018\n",
      "[[ 6.5        17.4        14.          8.61625287 52.21444173 30.        ]]\n",
      "11.480000000000018\n",
      "0\n",
      "[[ 9.7        23.6        15.          1.90367551 30.8054939  28.        ]]\n",
      "9.56000000000003\n",
      "0\n",
      "[[ 8.8   24.2   17.    19.971 31.36  26.   ]]\n",
      "17.599999999999994\n",
      "[[ 0.6        43.5        17.         51.2321607   0.83942539 25.        ]]\n",
      "135.12\n",
      "[[ 8.5        39.3         0.          6.07808237 16.48721313 24.        ]]\n",
      "67.96000000000004\n",
      "[[ 3.6        13.4         7.         11.55727385  8.00453779 23.        ]]\n",
      "18.400000000000006\n",
      "0\n",
      "0\n",
      "[[ 8.1        22.4        12.         27.86553095 25.97758058 20.        ]]\n",
      "16.599999999999994\n",
      "0\n",
      "[[ 4.5        10.5        14.         32.64940019 35.29895291 18.        ]]\n",
      "3.0\n",
      "[[11.5        24.5        15.         10.96655801 23.00744904 17.        ]]\n",
      "0.20000000000001705\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.3   23.5   17.    26.225 13.209 13.   ]]\n",
      "45.96000000000001\n",
      "[[ 6.6        20.         18.         22.08300399 39.44628893 12.        ]]\n",
      "19.120000000000005\n",
      "0\n",
      "[[14.2        32.7        19.         44.8788339  18.24727028 10.        ]]\n",
      "8.079999999999984\n",
      "[[ 9.    48.8   13.     5.251 60.     9.   ]]\n",
      "94.96000000000004\n",
      "0\n",
      "[[ 6.         25.8        13.         30.51068821 31.71417081  7.        ]]\n",
      "41.75999999999999\n",
      "0\n",
      "[[ 5.2   36.1   15.     5.695 42.959  5.   ]]\n",
      "80.15999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.6        16.4        16.         18.89090985 38.71394482  1.        ]]\n",
      "1541.6\n",
      "[[ 4.         26.7        16.         41.15427928 44.55867395  0.        ]]\n",
      "58.24000000000001\n",
      "[[ 3.4   20.1   20.    51.334 58.185 -1.   ]]\n",
      "41.20000000000002\n",
      "[[ 1.6        25.8        12.         38.83210238 35.69157478 -1.        ]]\n",
      "71.67999999999998\n",
      "0\n",
      "[[11.4        26.2        17.          4.2562781  59.15272949 -1.        ]]\n",
      "6.319999999999993\n",
      "[[ 0.1        27.6        17.         22.47826111 38.57886051 -1.        ]]\n",
      "87.63999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 0.8        16.8        19.         18.93398421 20.42893839 -1.        ]]\n",
      "48.31999999999999\n",
      "0\n",
      "[[ 6.1        21.8        19.         21.25453623 29.70856972 -1.        ]]\n",
      "28.28\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.8   11.1   18.    47.64  46.962 -1.   ]]\n",
      "9.680000000000007\n",
      "[[ 1.8        37.5        19.         22.25667285 19.15666121 -1.        ]]\n",
      "107.76000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.4        19.         21.         16.30894315 52.21742718 -1.        ]]\n",
      "37.68000000000001\n",
      "[[ 4.3        20.5        21.         34.61663513 44.94175346 -1.        ]]\n",
      "36.360000000000014\n",
      "0\n",
      "0\n",
      "driver reward  2822.6800000000003\n",
      "0\n",
      "[[10.2   24.1    7.    13.137 24.582 39.   ]]\n",
      "7.760000000000019\n",
      "[[ 5.2        17.7         8.         10.86263877 44.86478411 38.        ]]\n",
      "21.28\n",
      "[[ 5.6        26.2         8.          3.74133453 17.97640896 37.        ]]\n",
      "45.76000000000002\n",
      "[[ 4.1    9.1    9.    11.656 12.199 36.   ]]\n",
      "1.240000000000009\n",
      "[[ 7.         31.4         9.         33.77318952 40.93843746 35.        ]]\n",
      "52.879999999999995\n",
      "[[ 2.2   20.3   14.    15.088 44.082 34.   ]]\n",
      "50.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.3   29.1   15.    42.764 54.087 30.   ]]\n",
      "63.880000000000024\n",
      "[[ 6.4   35.8   18.     5.361 41.839 29.   ]]\n",
      "71.04000000000002\n",
      "0\n",
      "[[ 8.1   32.    18.     5.875 55.38  27.   ]]\n",
      "47.31999999999999\n",
      "[[ 4.3        11.2        18.         11.32727035 41.5330002  26.        ]]\n",
      "6.6000000000000085\n",
      "0\n",
      "[[ 1.5   11.2   19.     6.679 12.248 24.   ]]\n",
      "25.64\n",
      "[[19.8        42.4        20.         48.47303232 32.35691802 23.        ]]\n",
      "1.0399999999999636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 0.1        13.6        14.         36.13994334 45.60280766 15.        ]]\n",
      "42.84\n",
      "0\n",
      "0\n",
      "[[ 3.9        31.1        16.         19.36032489 21.98371349 12.        ]]\n",
      "73.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.4         9.2        18.         34.69893732 13.35546741  8.        ]]\n",
      "19.92\n",
      "0\n",
      "[[ 6.6        38.9         9.         31.82195924  4.53450822  6.        ]]\n",
      "79.60000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.9        21.3        13.         20.5881804  29.65111333  0.        ]]\n",
      "21.23999999999998\n",
      "0\n",
      "0\n",
      "[[ 1.2        10.         14.         16.53626197 27.66844682 -1.        ]]\n",
      "23.840000000000003\n",
      "0\n",
      "0\n",
      "[[ 2.7        21.3        15.         28.32304888 50.56921269 -1.        ]]\n",
      "49.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 6.5        15.8         5.          7.22768811 29.70819211 -1.        ]]\n",
      "6.359999999999985\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 0.6        34.9        12.         47.0297968  11.20136108 -1.        ]]\n",
      "107.6\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.5        16.2        13.         38.77687194 47.43724363 -1.        ]]\n",
      "14.439999999999998\n",
      "[[ 5.5   17.8   14.    21.072 45.358 -1.   ]]\n",
      "19.560000000000002\n",
      "0\n",
      "[[ 0.8        22.9        14.         27.06759082 42.79017461 -1.        ]]\n",
      "67.84\n",
      "0\n",
      "0\n",
      "[[12.6        30.2        14.         37.73860562  1.78885425 -1.        ]]\n",
      "10.960000000000036\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.1        19.8        17.         25.10116203 50.56049228 -1.        ]]\n",
      "15.080000000000013\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.2        33.2        20.         40.67617972  4.28808661 -1.        ]]\n",
      "64.07999999999998\n",
      "[[ 7.5   34.7   18.     9.076 19.772 -1.   ]]\n",
      "60.039999999999964\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2        20.2        19.         21.13793929 30.18053202 -1.        ]]\n",
      "29.28\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "driver reward  1099.92\n",
      "0\n",
      "0\n",
      "[[ 9.7   26.5    9.    13.792 30.765 38.   ]]\n",
      "18.839999999999975\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.8        28.2        11.         19.61763798 19.26199528 31.        ]]\n",
      "44.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.8        13.2        12.         12.43513198 41.45402496 27.        ]]\n",
      "2.8000000000000114\n",
      "[[ 0.4        22.8        12.         31.71866027 29.30992968 26.        ]]\n",
      "70.24000000000001\n",
      "0\n",
      "0\n",
      "[[ 4.         13.3        17.         22.79641933 58.52092288 23.        ]]\n",
      "15.36\n",
      "0\n",
      "[[ 3.2   40.5   18.     5.586 17.648 21.   ]]\n",
      "107.83999999999997\n",
      "[[ 8.8        19.6        18.          8.22646142 45.94317559 20.        ]]\n",
      "2.8799999999999955\n",
      "0\n",
      "[[ 4.5        20.6        18.         40.30429437 50.51111691 18.        ]]\n",
      "35.31999999999999\n",
      "0\n",
      "[[ 6.6        25.5        21.          0.06092643 47.65194015 16.        ]]\n",
      "36.72\n",
      "0\n",
      "[[ 4.4        22.1         5.         26.08084736 45.959791   14.        ]]\n",
      "40.80000000000001\n",
      "[[ 2.4        20.9         6.          5.95385586 57.67922999 13.        ]]\n",
      "50.56000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.         25.9         8.         36.75273743 41.35156241  9.        ]]\n",
      "42.08000000000001\n",
      "[[ 8.8        35.4         8.          2.92689405 57.6011785   8.        ]]\n",
      "53.44\n",
      "0\n",
      "[[ 3.5        40.3         9.         45.67799491 33.67968422  6.        ]]\n",
      "105.16000000000003\n",
      "0\n",
      "[[ 6.    43.3   19.     5.233 24.889  4.   ]]\n",
      "97.76000000000005\n",
      "[[ 3.7        24.5        19.         23.60821681 36.93828215  3.        ]]\n",
      "53.24000000000001\n",
      "[[ 9.4        27.2        20.         47.77532696 17.21686937  2.        ]]\n",
      "23.120000000000005\n",
      "[[ 0.4   45.1   19.     5.081 32.742  1.   ]]\n",
      "1641.6\n",
      "[[ 6.5        39.3        19.         46.40049173 29.54733996  0.        ]]\n",
      "81.56\n",
      "0\n",
      "0\n",
      "[[ 6.9        23.8        13.         30.22382549 41.28135426 -1.        ]]\n",
      "29.23999999999998\n",
      "[[ 5.9        30.9        13.         21.64624082  9.49773124 -1.        ]]\n",
      "58.76000000000002\n",
      "0\n",
      "0\n",
      "[[ 3.          7.6        14.          1.34291124 26.43030157 -1.        ]]\n",
      "3.9200000000000017\n",
      "[[ 4.7        20.9        15.          3.05910561 46.40918479 -1.        ]]\n",
      "34.920000000000016\n",
      "[[ 8.5        22.3        15.         26.87619417 40.64941716 -1.        ]]\n",
      "13.560000000000002\n",
      "0\n",
      "[[ 3.8        29.2        16.         33.58703108 15.40891838 -1.        ]]\n",
      "67.6\n",
      "0\n",
      "[[ 2.8        12.9        18.          1.99359984 41.42450968 -1.        ]]\n",
      "22.24000000000001\n",
      "0\n",
      "[[ 5.4        17.6        18.         20.49449701 48.52410486 -1.        ]]\n",
      "19.599999999999994\n",
      "0\n",
      "0\n",
      "[[ 5.6        28.1        19.         19.12997963 20.01841138 -1.        ]]\n",
      "51.839999999999975\n",
      "0\n",
      "0\n",
      "[[ 6.         37.         19.         40.94328259 31.74008357 -1.        ]]\n",
      "77.60000000000002\n",
      "[[ 8.1        18.7         8.         30.45739459 20.65091679 -1.        ]]\n",
      "4.760000000000019\n",
      "0\n",
      "[[ 8.    24.1    9.     8.777 36.521 -1.   ]]\n",
      "22.72\n",
      "0\n",
      "[[ 2.2        36.9         9.         36.0224772  55.39441013 -1.        ]]\n",
      "103.12\n",
      "[[ 4.6        10.4        11.         28.37768002 44.29997853 -1.        ]]\n",
      "2.0\n",
      "0\n",
      "0\n",
      "[[ 1.3         5.5        13.         24.52112345 29.4614269  -1.        ]]\n",
      "8.760000000000005\n",
      "0\n",
      "[[ 8.    30.5   15.     3.415 47.344 -1.   ]]\n",
      "43.19999999999999\n",
      "0\n",
      "0\n",
      "[[ 0.8         6.3        15.         12.2677657  43.60292119 -1.        ]]\n",
      "14.720000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.6        23.         17.         46.54213265 18.92318986 -1.        ]]\n",
      "1.5200000000000102\n",
      "0\n",
      "[[ 6.         24.3        12.         17.81067092 43.39367517 -1.        ]]\n",
      "36.96000000000001\n",
      "0\n",
      "[[ 4.8        16.5        12.         23.29301498 22.76869675 -1.        ]]\n",
      "20.159999999999997\n",
      "[[ 5.5        16.8        12.         33.18407071 42.27020438 -1.        ]]\n",
      "16.359999999999985\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.1        10.4        13.         21.05281452 48.67662219 -1.        ]]\n",
      "12.200000000000003\n",
      "[[ 0.9        37.9        13.         50.38104603 25.20045873 -1.        ]]\n",
      "115.16000000000003\n",
      "0\n",
      "0\n",
      "[[ 3.5        16.7        21.         18.25426142 45.58950492 -1.        ]]\n",
      "29.640000000000015\n",
      "[[ 6.9        41.2        21.         30.4107725   9.31176178 -1.        ]]\n",
      "84.92000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.2        27.6         7.         14.99089979 11.29981019 -1.        ]]\n",
      "73.36000000000001\n",
      "0\n",
      "0\n",
      "[[ 2.1   10.7    9.     6.767 12.249 -1.   ]]\n",
      "19.960000000000008\n",
      "[[ 6.3        39.          9.         25.54447261 44.70441097 -1.        ]]\n",
      "81.96000000000004\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.    25.6   14.     3.806 38.741 -1.   ]]\n",
      "47.91999999999999\n",
      "[[ 3.1         8.7        14.          2.99333896 33.09030822 -1.        ]]\n",
      "6.760000000000005\n",
      "[[12.5        34.         14.         42.05856294 47.07411239 -1.        ]]\n",
      "23.80000000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.3        17.1        15.          4.18210198 51.55597244 -1.        ]]\n",
      "32.28\n",
      "0\n",
      "0\n",
      "[[ 3.8        20.7        15.         17.86110535 13.18958188 -1.        ]]\n",
      "40.400000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.8        42.5        16.         48.93724649 38.62521003 -1.        ]]\n",
      "103.36000000000001\n",
      "[[ 1.         15.5        21.         34.27827646 42.88013589 -1.        ]]\n",
      "42.8\n",
      "0\n",
      "[[  7.1         31.3          0.          59.2892586   23.58461374\n",
      "  140.        ]]\n",
      "51.879999999999995\n",
      "driver reward  3943.280000000001\n",
      "total reward  43530.84\n",
      "trips [  6.  12.   0.   4.   4.  27.  34.  87. 103. 136.  59.  91. 144. 139.\n",
      " 120.  94.  76. 148. 143. 152. 103.  61.  25.   8.]\n",
      "[0.3333333333333333, 0.25, nan, 0.5, 0.25, 0.4074074074074074, 0.38235294117647056, 0.39080459770114945, 0.47572815533980584, 0.4338235294117647, 0.288135593220339, 0.3516483516483517, 0.4097222222222222, 0.4460431654676259, 0.4583333333333333, 0.40425531914893614, 0.2894736842105263, 0.49324324324324326, 0.3916083916083916, 0.3815789473684211, 0.33980582524271846, 0.4426229508196721, 0.28, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-ff47d4624acb>:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hrly_acceptance_rates.append(hrly_accepted_trips[j]/hrly_trip_counts[j])\n"
     ]
    }
   ],
   "source": [
    "evaluatePolicy(accpt_positive_trips_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average returnstep\n",
    "def compute_avg_return(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation([policy])\n",
    "        episode_reward = 0\n",
    "        for state_list in state_lists[0]:\n",
    "            states = []\n",
    "            driver_reward = 0\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                driver_reward += state_tf.reward\n",
    "            episode_reward += driver_reward\n",
    "        \n",
    "        #take average reward for all drivers in the episode\n",
    "        episode_reward = episode_reward / len(state_lists)\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_return = total_reward / num_episodes\n",
    "    print(avg_return)\n",
    "    return avg_return.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imitation learning strategy\n",
    "#parse pickme data into trajectories\n",
    "#pickup distance ignored since it is nto available in dataset\n",
    "#load data\n",
    "import pandas as pd \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "driver_actions =  pd.read_csv(\"data/driver-action.csv\") \n",
    "trip_data =  pd.read_csv(\"data/trip-data.csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerslogid</th>\n",
       "      <th>driverid</th>\n",
       "      <th>pickuplatitude</th>\n",
       "      <th>pickuplongitude</th>\n",
       "      <th>droplatitude</th>\n",
       "      <th>droplongitude</th>\n",
       "      <th>actualpickuptime</th>\n",
       "      <th>paymentmethod</th>\n",
       "      <th>isdirectionalhire</th>\n",
       "      <th>distance</th>\n",
       "      <th>travelstatus</th>\n",
       "      <th>taximodelid</th>\n",
       "      <th>createddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212553104</td>\n",
       "      <td>188</td>\n",
       "      <td>6.03305</td>\n",
       "      <td>80.2168</td>\n",
       "      <td>6.07534</td>\n",
       "      <td>80.2346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:08:28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212554705</td>\n",
       "      <td>188</td>\n",
       "      <td>6.93233</td>\n",
       "      <td>79.8492</td>\n",
       "      <td>6.91947</td>\n",
       "      <td>79.9114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:25:27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212555287</td>\n",
       "      <td>0</td>\n",
       "      <td>6.93257</td>\n",
       "      <td>79.8486</td>\n",
       "      <td>6.82994</td>\n",
       "      <td>79.9101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:30:43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212557334</td>\n",
       "      <td>18930</td>\n",
       "      <td>6.90514</td>\n",
       "      <td>79.8788</td>\n",
       "      <td>6.89894</td>\n",
       "      <td>79.8771</td>\n",
       "      <td>2020-02-01 01:00:37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:53:54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212558261</td>\n",
       "      <td>117344</td>\n",
       "      <td>6.88920</td>\n",
       "      <td>79.8895</td>\n",
       "      <td>6.91505</td>\n",
       "      <td>79.9531</td>\n",
       "      <td>2020-02-01 01:11:07.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 01:06:08.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>212560424</td>\n",
       "      <td>0</td>\n",
       "      <td>7.21436</td>\n",
       "      <td>79.8471</td>\n",
       "      <td>7.22546</td>\n",
       "      <td>79.8520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 01:52:20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>212565638</td>\n",
       "      <td>0</td>\n",
       "      <td>6.88655</td>\n",
       "      <td>79.8583</td>\n",
       "      <td>6.92075</td>\n",
       "      <td>79.8561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 05:36:30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>212565684</td>\n",
       "      <td>107426</td>\n",
       "      <td>6.84864</td>\n",
       "      <td>79.9635</td>\n",
       "      <td>6.85434</td>\n",
       "      <td>79.8923</td>\n",
       "      <td>2020-02-01 05:42:07.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 05:37:34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>212567228</td>\n",
       "      <td>16925</td>\n",
       "      <td>6.86133</td>\n",
       "      <td>79.8781</td>\n",
       "      <td>6.85090</td>\n",
       "      <td>79.8661</td>\n",
       "      <td>2020-02-01 06:22:50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 06:17:46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>212571010</td>\n",
       "      <td>0</td>\n",
       "      <td>7.03675</td>\n",
       "      <td>79.9023</td>\n",
       "      <td>7.02326</td>\n",
       "      <td>79.9092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 07:19:46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passengerslogid  driverid  pickuplatitude  pickuplongitude  droplatitude  \\\n",
       "0        212553104       188         6.03305          80.2168       6.07534   \n",
       "1        212554705       188         6.93233          79.8492       6.91947   \n",
       "2        212555287         0         6.93257          79.8486       6.82994   \n",
       "3        212557334     18930         6.90514          79.8788       6.89894   \n",
       "4        212558261    117344         6.88920          79.8895       6.91505   \n",
       "5        212560424         0         7.21436          79.8471       7.22546   \n",
       "6        212565638         0         6.88655          79.8583       6.92075   \n",
       "7        212565684    107426         6.84864          79.9635       6.85434   \n",
       "8        212567228     16925         6.86133          79.8781       6.85090   \n",
       "9        212571010         0         7.03675          79.9023       7.02326   \n",
       "\n",
       "   droplongitude       actualpickuptime  paymentmethod  isdirectionalhire  \\\n",
       "0        80.2346                    NaN              1                NaN   \n",
       "1        79.9114                    NaN              1                NaN   \n",
       "2        79.9101                    NaN              1                NaN   \n",
       "3        79.8771  2020-02-01 01:00:37.0              1                NaN   \n",
       "4        79.9531  2020-02-01 01:11:07.0              1                NaN   \n",
       "5        79.8520                    NaN              1                NaN   \n",
       "6        79.8561                    NaN              2                NaN   \n",
       "7        79.8923  2020-02-01 05:42:07.0              1                NaN   \n",
       "8        79.8661  2020-02-01 06:22:50.0              1                NaN   \n",
       "9        79.9092                    NaN              1                NaN   \n",
       "\n",
       "   distance  travelstatus  taximodelid            createddate  \n",
       "0       NaN            11            1  2020-02-01 00:08:28.0  \n",
       "1       NaN            12            1  2020-02-01 00:25:27.0  \n",
       "2       NaN            11            1  2020-02-01 00:30:43.0  \n",
       "3      1.28             1            1  2020-02-01 00:53:54.0  \n",
       "4      9.78             1            1  2020-02-01 01:06:08.0  \n",
       "5       NaN            11            1  2020-02-01 01:52:20.0  \n",
       "6       NaN            11            1  2020-02-01 05:36:30.0  \n",
       "7      9.80             1            1  2020-02-01 05:37:34.0  \n",
       "8      2.28             1            1  2020-02-01 06:17:46.0  \n",
       "9       NaN            11            1  2020-02-01 07:19:46.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>created_driver_id</th>\n",
       "      <th>assigned_driver_id</th>\n",
       "      <th>trip_recieved_driver_id</th>\n",
       "      <th>rejected_driver_id</th>\n",
       "      <th>accepted_driver_id</th>\n",
       "      <th>arrived_driver_id</th>\n",
       "      <th>started_driver_id</th>\n",
       "      <th>ended_driver_id</th>\n",
       "      <th>completed_driver_id</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>created_date</th>\n",
       "      <th>rejection_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>212552700</td>\n",
       "      <td>0</td>\n",
       "      <td>38457.0</td>\n",
       "      <td>38457.0</td>\n",
       "      <td>38457.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>SYSTEM_REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212552700</td>\n",
       "      <td>0</td>\n",
       "      <td>79308.0</td>\n",
       "      <td>79308.0</td>\n",
       "      <td>79308.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>SYSTEM_REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>212552700</td>\n",
       "      <td>0</td>\n",
       "      <td>60511.0</td>\n",
       "      <td>60511.0</td>\n",
       "      <td>60511.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>MANUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>212552700</td>\n",
       "      <td>0</td>\n",
       "      <td>11251.0</td>\n",
       "      <td>11251.0</td>\n",
       "      <td>11251.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>SYSTEM_REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212552700</td>\n",
       "      <td>0</td>\n",
       "      <td>22304.0</td>\n",
       "      <td>22304.0</td>\n",
       "      <td>22304.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>MANUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>212553052</td>\n",
       "      <td>0</td>\n",
       "      <td>66075.0</td>\n",
       "      <td>66075.0</td>\n",
       "      <td>66075.0</td>\n",
       "      <td>66075.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>AFTER_ACCEPTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>212553052</td>\n",
       "      <td>0</td>\n",
       "      <td>25440.0</td>\n",
       "      <td>25440.0</td>\n",
       "      <td>25440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>SYSTEM_REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>212553052</td>\n",
       "      <td>0</td>\n",
       "      <td>40092.0</td>\n",
       "      <td>40092.0</td>\n",
       "      <td>40092.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>SYSTEM_REJECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>212553052</td>\n",
       "      <td>0</td>\n",
       "      <td>56333.0</td>\n",
       "      <td>56333.0</td>\n",
       "      <td>56333.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>MANUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>212553052</td>\n",
       "      <td>0</td>\n",
       "      <td>75565.0</td>\n",
       "      <td>75565.0</td>\n",
       "      <td>75565.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>MANUAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_id  created_driver_id  assigned_driver_id  trip_recieved_driver_id  \\\n",
       "0  212552700                  0             38457.0                  38457.0   \n",
       "1  212552700                  0             79308.0                  79308.0   \n",
       "2  212552700                  0             60511.0                  60511.0   \n",
       "3  212552700                  0             11251.0                  11251.0   \n",
       "4  212552700                  0             22304.0                  22304.0   \n",
       "5  212553052                  0             66075.0                  66075.0   \n",
       "6  212553052                  0             25440.0                  25440.0   \n",
       "7  212553052                  0             40092.0                  40092.0   \n",
       "8  212553052                  0             56333.0                  56333.0   \n",
       "9  212553052                  0             75565.0                  75565.0   \n",
       "\n",
       "   rejected_driver_id  accepted_driver_id  arrived_driver_id  \\\n",
       "0             38457.0                 NaN                NaN   \n",
       "1             79308.0                 NaN                NaN   \n",
       "2             60511.0                 NaN                NaN   \n",
       "3             11251.0                 NaN                NaN   \n",
       "4             22304.0                 NaN                NaN   \n",
       "5             66075.0             66075.0                NaN   \n",
       "6             25440.0                 NaN                NaN   \n",
       "7             40092.0                 NaN                NaN   \n",
       "8             56333.0                 NaN                NaN   \n",
       "9             75565.0                 NaN                NaN   \n",
       "\n",
       "   started_driver_id  ended_driver_id  completed_driver_id  vehicle_type  \\\n",
       "0                NaN              NaN                  NaN             1   \n",
       "1                NaN              NaN                  NaN             1   \n",
       "2                NaN              NaN                  NaN             1   \n",
       "3                NaN              NaN                  NaN             1   \n",
       "4                NaN              NaN                  NaN             1   \n",
       "5                NaN              NaN                  NaN             1   \n",
       "6                NaN              NaN                  NaN             1   \n",
       "7                NaN              NaN                  NaN             1   \n",
       "8                NaN              NaN                  NaN             1   \n",
       "9                NaN              NaN                  NaN             1   \n",
       "\n",
       "  created_date   rejection_type  \n",
       "0   2020-02-01  SYSTEM_REJECTED  \n",
       "1   2020-02-01  SYSTEM_REJECTED  \n",
       "2   2020-02-01           MANUAL  \n",
       "3   2020-02-01  SYSTEM_REJECTED  \n",
       "4   2020-02-01           MANUAL  \n",
       "5   2020-02-01   AFTER_ACCEPTED  \n",
       "6   2020-02-01  SYSTEM_REJECTED  \n",
       "7   2020-02-01  SYSTEM_REJECTED  \n",
       "8   2020-02-01           MANUAL  \n",
       "9   2020-02-01           MANUAL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_actions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6732166\n",
      "accepted 1989673\n",
      "rejected 4742493\n",
      "trips 4046020\n"
     ]
    }
   ],
   "source": [
    "driver_behaviours = []\n",
    "\n",
    "#trajectory format - trip distance, time of day, drop location_long, drop_location_lat, (trips_till_weekly_reward)\n",
    "\n",
    "#MANUAL and SYSTEM_REJECTED count as rejected, null counts as acccepted\n",
    "\n",
    "bool_accepted_series = pd.notnull(driver_actions[\"accepted_driver_id\"])  \n",
    "accepted_driver_actions = driver_actions[bool_accepted_series]\n",
    "bool_rejected_series = pd.isnull(driver_actions[\"accepted_driver_id\"])\n",
    "rejected_driver_actions = driver_actions[bool_rejected_series]\n",
    "print(\"total\", len(driver_actions.index))\n",
    "print(\"accepted\", len(accepted_driver_actions.index))\n",
    "print(\"rejected\", len(rejected_driver_actions.index))\n",
    "print(\"trips\", len(trip_data.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO distance scale\n",
    "# for rides without distance, estimate distance based on l2 distance and scaling \n",
    "# factor calculated from ratio between l2 distance and real distance\n",
    "scaling_factor = 1.3\n",
    "\n",
    "#get l2 distance (assume that locations are close to equator)\n",
    "def l2_dist(x1, x2, y1, y2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1))*111 #mult by 111 to turn cordinates to km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drop_lat': 6.93233,\n",
       " 'drop_long': 79.8492,\n",
       " 'time': Timestamp('2020-02-01 00:25:27'),\n",
       " 'trip_dist': 9.165287626627732}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup drop location, trip distance, time from trip_data\n",
    "def get_trip_data(trips, trip_id):\n",
    "    trip = trips.loc[trips['passengerslogid'] == trip_id]\n",
    "    #if exactly one record is found\n",
    "    if (len(trip.index)) ==1:\n",
    "        #trip = trip[\"droplatitude\"]\n",
    "        for tr in trip.iterrows():\n",
    "            lat  = tr[1][\"pickuplatitude\"]\n",
    "            long  = tr[1][\"pickuplongitude\"]\n",
    "            time  = pd.to_datetime(tr[1][\"createddate\"])\n",
    "            \n",
    "            #check if distance is available\n",
    "            if (math.isnan(tr[1][\"distance\"])):\n",
    "                distance = scaling_factor * l2_dist(tr[1][\"pickuplatitude\"],\n",
    "                                                   tr[1][\"droplatitude\"],\n",
    "                                                   tr[1][\"pickuplongitude\"],\n",
    "                                                   tr[1][\"droplongitude\"])                \n",
    "            else:\n",
    "\n",
    "                distance  = tr[1][\"distance\"]\n",
    "            return {\n",
    "                \"drop_lat\": lat,\n",
    "                \"drop_long\": long,\n",
    "                \"time\": time,\n",
    "                \"trip_dist\": distance\n",
    "            }\n",
    "    else:\n",
    "        return None\n",
    "    #print(trip)\n",
    "\n",
    "\n",
    "get_trip_data(trip_data, 212554705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - lookup trips_till_weekly_reward from achievements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9999it [00:34, 288.30it/s]\n",
      "9999it [00:36, 271.65it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group behaviour by drivers, sorted by time\n",
    "#rejected_driver_actions['rejected_driver_id'].nunique()\n",
    "#create dictionary with entry for each driver. the entry is a list of actions\n",
    "driver_actions ={}\n",
    "count = 0\n",
    "#loop through accepted trips\n",
    "for id, row in tqdm(accepted_driver_actions.iterrows()):\n",
    "    count+=1\n",
    "    if count == 10000: #temporary limit for testing\n",
    "        break\n",
    "        \n",
    "    #print(row[\"trip_id\"])\n",
    "    action = {\"accept\": 1, \"observation\": get_trip_data(trip_data, row[\"trip_id\"])}\n",
    "    key = str(int(row[\"accepted_driver_id\"]))\n",
    "\n",
    "    #add to driver actions table\n",
    "    if key not in driver_actions:\n",
    "        driver_actions[key] = [action]\n",
    "    else:\n",
    "        driver_actions[key].append(action)\n",
    "        \n",
    "#loop through rejected trips\n",
    "count = 0\n",
    "\n",
    "for id, row in tqdm(rejected_driver_actions.iterrows()):\n",
    "    count+=1\n",
    "    if count == 10000:\n",
    "        break\n",
    "        \n",
    "    #print(row[\"trip_id\"])\n",
    "    action = {\"accept\": 0, \"observation\": get_trip_data(trip_data, row[\"trip_id\"])}\n",
    "    if (math.isnan(row[\"rejected_driver_id\"])):\n",
    "        continue\n",
    "    key = str(int(row[\"rejected_driver_id\"]))\n",
    "    if key not in driver_actions:\n",
    "        driver_actions[key] = [action]\n",
    "    else:\n",
    "        driver_actions[key].append(action)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort action in order (based on time)\n",
    "\n",
    "#remove invalid values\n",
    "for driver in driver_actions:\n",
    "    refined_actions = []\n",
    "    for action in driver_actions[driver]:\n",
    "        try:\n",
    "            time = action['observation'][\"time\"].value\n",
    "            refined_actions.append(action)\n",
    "        except:\n",
    "            continue\n",
    "    driver_actions[driver] = refined_actions\n",
    "    \n",
    "for driver in driver_actions:\n",
    "        driver_actions[driver] = sorted(driver_actions[driver], key = lambda i: i['observation'][\"time\"].value)\n",
    "\n",
    "#switch time to hour of day\n",
    "for driver in driver_actions:\n",
    "    for action in driver_actions[driver]:\n",
    "        action[\"observation\"][\"time\"] = action[\"observation\"][\"time\"].hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accept': 1, 'observation': {'drop_lat': 6.88177, 'drop_long': 79.8663, 'time': 0, 'trip_dist': 10.28613164788601}}, {'accept': 0, 'observation': {'drop_lat': 6.84149, 'drop_long': 79.8811, 'time': 0, 'trip_dist': 2.17}}, {'accept': 1, 'observation': {'drop_lat': 6.8613, 'drop_long': 79.864, 'time': 2, 'trip_dist': 1.3}}]\n"
     ]
    }
   ],
   "source": [
    "print(driver_actions[\"66075\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12633\n"
     ]
    }
   ],
   "source": [
    "# create trajectory collection from pickme trajectory set\n",
    "#reward is set to 0 because it is  not available in the dataset and at this stage the agent does not explore\n",
    "\n",
    "driver_trajectories = []\n",
    "\n",
    "#loop through drivers\n",
    "for key in driver_actions:\n",
    "    #driver must have at least 2 rides to create a trajectory\n",
    "    if len(driver_actions[key]) > 1:\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        #convert states directly to tf timesteps\n",
    "        for i in range(len(driver_actions[key])):\n",
    "            #create time step\n",
    "            if i == 0:\n",
    "                #initial trajectory\n",
    "                obs = driver_actions[key][i][\"observation\"]\n",
    "                obs_list = [obs[\"trip_dist\"], obs[\"drop_lat\"], obs[\"drop_long\"], obs[\"time\"]]\n",
    "                state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "            elif i < (len(driver_actions[key]) - 1):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "            else:\n",
    "                #terminating tranjectory\n",
    "                state_tf = ts.TimeStep(tf.constant([2]), tf.constant([0.0]), tf.constant([0.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "            #create action\n",
    "            #action = state_list[i][\"action\"]\n",
    "            action = policy_step.PolicyStep(tf.constant([driver_actions[key][i][\"accept\"]], dtype=tf.int64), ())\n",
    "            #print (action)\n",
    "            states.append(state_tf)\n",
    "            actions.append(action)\n",
    "\n",
    "        for j in range(len(states)-1):\n",
    "            present_state = states[j]\n",
    "            #print(present_state)\n",
    "            next_state = states[j+1]\n",
    "            action = actions[j]\n",
    "\n",
    "            traj = trajectory.from_transition(present_state, action, next_state)\n",
    "            #print(action)\n",
    "            # Add trajectory to the replay buffer\n",
    "            driver_trajectories.append(traj)\n",
    "            #replay_buffer.add_batch(traj)\n",
    "            #print(traj)\n",
    "\n",
    "print(len(driver_trajectories))\n",
    "#cache trajectories as json to disk?\n",
    "\n",
    "#convert trajectories to tf agents format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add subset of trajectorise to replay buffer\n",
    "def collect_data_imitation(num_trajectories, replay_buffer, driver_trajectories):\n",
    "    #sample from trajectories\n",
    "    sample_trajectories = sample(driver_trajectories, num_trajectories)\n",
    "    for traj in sample_trajectories:\n",
    "        replay_buffer.add_batch(traj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajectories = 500\n",
    "#collect_data_imitation(num_trajectories, replay_buffer, driver_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories - regular q learning\n",
    "\n",
    "def collect_data(num_iterations, policy, replay_buffer):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation([policy])\n",
    "        print(\"driver count : \", len(state_lists[0]))\n",
    "        for state_list in state_lists[0]:\n",
    "            states = []\n",
    "            actions = []\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                print(state_list[i])\n",
    "                #create time step\n",
    "                if i == 0:\n",
    "                    #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                    state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                    #print (state_tf)\n",
    "                elif i < (len(state_list) - 1):\n",
    "                    #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                else:\n",
    "                    state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                #create action\n",
    "                \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                    action = tf.constant([1], dtype=tf.int32)\n",
    "                else:\n",
    "                    action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                action = state_list[i][\"action\"]\n",
    "                #print\n",
    "                #print (\"action\", state_list[i][\"action\"])\n",
    "                #print(\"obs\", state_list[i][\"observation\"])\n",
    "                states.append(state_tf)\n",
    "                actions.append(action)\n",
    "\n",
    "            for j in range(len(states)-1):\n",
    "                present_state = states[j]\n",
    "                print(present_state)\n",
    "                next_state = states[j+1]\n",
    "                action = actions[j]\n",
    "                traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                #print(action)\n",
    "                # Add trajectory to the replay buffer\n",
    "                replay_buffer.add_batch(traj)\n",
    "                #print(traj)\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hex count  114\n",
      "Number of trips generated: 1974\n",
      "hex count  114\n",
      "Number of trips generated: 1918\n",
      "tf.Tensor(-3582.2002, shape=(), dtype=float32)\n",
      " Average Return = -3582.2001953125\n",
      "hex count  114\n",
      "Number of trips generated: 2121\n",
      "driver count :  20\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f64308e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.9, 3.8, 2, 7.584000000002083, 41.82799999999984, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e50d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.9, 8.5, 6, 2.4409720177717364, 37.72140120808937, 40], 'reward': -121.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.9, 15.2, 7, 4.6745403763371955, 34.62463023286752, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.0, 16.2, 9, 15.000000000001327, 42.587999999999866, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ecd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.6, 17.3, 9, 14.695180855824894, 58.96893354115201, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61feca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 20.3, 9, 6.561069024183675, 14.805875263112963, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 8.1, 9, 5.976392245046368, 44.553790585861904, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.1, 11.7, 9, 8.62981637578337, 35.69282830699598, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a8b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.5, 16.2, 9, 13.737392133327909, 48.169266317657616, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c98b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.8, 21.3, 10, 3.8277655810327356, 27.234600753952403, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61749a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.1, 16.6, 11, 28.894319676681267, 25.33125814685462, 39], 'reward': -35.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 20.4, 11, 3.237657110999983, 39.84112069796166, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d3d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.2, 6.3, 12, 5.593685200020351, 43.16807732871872, 38], 'reward': -144.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60faf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 14.2, 12, 5.242856438813538, 45.210399609535266, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 13.2, 12, 25.496728609609825, 41.65580231170768, 37], 'reward': -18.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ebdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.3, 12.3, 12, 3.189320390178912, 34.09010116234906, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ac280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 20.9, 12, 13.213000000001704, 51.10199999999982, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 43.9, 12, 51.95701908847167, 28.41161260316954, 36], 'reward': 42.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606b0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 42.6, 13, 13.509000000001704, 43.029999999999816, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a539d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.2, 35.2, 16, 16.166000000002462, 17.96500000000012, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62335e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.6, 15.7, 17, 44.198059134288584, 31.444804841127386, 35], 'reward': -21.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 25.5, 19, 3.850980273802019, 43.56873968278472, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d5dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.5, 24.3, 20, 1.5806690097163951, 53.113867530136844, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d57f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.1, 20.9, 20, 1.094949662786636, 43.58732068158452, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eba9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.0, 15.9, 7, 49.21501450847947, 9.647283737679512, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec9fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.3, 40.6, 9, 7.9142791835001205, 36.00341194861859, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 34.1, 11, 8.027000000004357, 52.679999999999964, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.2, 28.1, 13, 13.750000000001325, 44.10099999999972, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e715b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.9, 23.5, 15, 2.7424026607556016, 34.87934493211107, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66aebb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.4, 4.7, 17, 25.453000000001325, 16.976999999999844, 34], 'reward': -116.87999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9aa00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 29.3, 18, 4.136999999999432, 48.655000000000044, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66aea00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.4, 34.0, 18, 33.3203646370288, 56.47240744572596, 33], 'reward': -70.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f65519d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.7, 13.4, 19, 7.131999999999621, 50.45699999999962, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 22.6, 20, 7.919000000004168, 45.45400000000002, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629dca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.2, 10.2, 21, 11.160454102142761, 32.654192511393006, 32], 'reward': -125.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ecb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.5, 19.7, 21, 7.20562676607118, 45.80101813297665, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 10.8, 22, 14.698600456868288, 48.31833395931457, 31], 'reward': -52.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6356a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.0, 10.5, 22, 17.007287248081646, 35.49028841161614, 30], 'reward': -102.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6356f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 11.2, 23, 11.193543713551154, 53.15770689677003, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63567c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.6, 26.9, 23, 51.64109027289288, 40.684915995007536, 29], 'reward': 7.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 37.3, 8, 18.77585145159305, 40.665021818544986, 28], 'reward': 87.40000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ee700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.8, 5.1, 8, 11.530779874743747, 56.9966471858745, 27], 'reward': -131.92}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ee760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 48.8, 8, 54.38749465624662, 40.50360653948123, 26], 'reward': 86.12000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.4, 31.9, 13, 18.492999999999242, 56.77499999999992, 25], 'reward': 38.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.1, 16.1, 13, 1.3682326631920423, 23.514248887036633, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.3, 7.5, 13, 17.943331356729107, 43.16435887480586, 24], 'reward': -86.84}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f71c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 23.3, 13, 4.9970000000047365, 60.0, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.5, 3.5, 13, 10.039111342705034, 56.10669437888374, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62228e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 11.5, 13, 1.2875672219348742, 46.74688565883218, 23], 'reward': -69.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 18.2, 14, 18.51443360089792, 32.41350845208662, 22], 'reward': 26.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 19.1, 14, 4.0486389635973925, 45.19640189829088, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.0, 22.4, 14, 35.54120040246569, 50.403347555381366, 21], 'reward': 51.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.4, 14.7, 14, 10.352000000000567, 47.43599999999979, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.9, 13.1, 15, 27.170336345805094, 47.280491954848856, 20], 'reward': -32.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 5.1, 16, 8.36982912338616, 50.75836392546525, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.7, 6.1, 16, 7.526000000000189, 55.91899999999994, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 9.9, 17, 11.716911450953988, 48.96095787140497, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 9.8, 17, 1.72737667816439, 50.8448122340634, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 25.9, 17, 31.408144759752616, 30.0366649645477, 19], 'reward': -23.87999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.3, 24.6, 17, 3.6030000000030316, 21.059000000000147, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.6, 9.9, 17, 18.378682305449544, 43.91513894231114, 18], 'reward': -108.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.8, 14.4, 17, 5.1060000000036, 48.91499999999956, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d20a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 2.9, 17, 7.4429239034608905, 40.69766900956731, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.2, 21.9, 17, 15.82128208563281, 18.695284991034836, 17], 'reward': 14.320000000000022}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c92e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.1, 16.9, 17, 21.791549185483422, 19.009838974114963, 16], 'reward': -28.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 4.3, 18, 17.17883941058299, 32.31526276159664, 15], 'reward': -101.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.4, 12.4, 18, 6.5200000000036, 35.187999999999626, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 24.6, 18, 27.13602841320713, 56.70554580516305, 14], 'reward': -60.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 26.8, 18, 31.947470467562656, 20.656422683861575, 13], 'reward': -19.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.4, 8.7, 19, 23.574757421698575, 33.487223389046285, 12], 'reward': -76.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.5, 7.5, 19, 19.161613163999828, 37.66202055638135, 11], 'reward': 13.800000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631b4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 6.5, 19, 1.001373970281322, 34.77679867995266, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.7, 15.3, 19, 21.589252271267043, 41.0462323982535, 10], 'reward': -44.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 13.2, 19, 13.761763428283526, 53.56472242654716, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63144f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.2, 3.5, 19, 4.221514702050328, 42.56444513783718, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631eb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.9, 17.7, 19, 7.310000000001705, 32.47100000000013, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616ca00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.0, 22.2, 19, 4.342945186992238, 55.16373699565099, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.9, 20.3, 19, 17.779239781914143, 42.74181869390369, 9], 'reward': -97.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616cb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 27.5, 19, 27.239925401753872, 34.74025230375867, 8], 'reward': -14.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616cf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 14.3, 19, 7.9630000000036, 39.61299999999986, 7], 'reward': -4.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.0, 51.0, 19, 51.72040298693002, 27.49926289310068, 6], 'reward': 95.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.4, 30.6, 7, 6.759999999998105, 34.23600000000005, 5], 'reward': -6.800000000000011}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 10.9, 7, 14.405000000001326, 52.00099999999981, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.8, 6.4, 7, 2.2750905287635375, 22.59399606045527, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6192370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.3, 5.5, 7, 0.9755651848023055, 33.541532075682326, 4], 'reward': -32.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6192ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.4, 2.8, 7, 5.751999999999241, 35.48399999999965, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 10.0, 8, 14.516660298923199, 52.80300417232819, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 7.6, 8, 11.516105983989, 46.21538717740867, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 9.0, 8, 7.210000000000189, 39.44300000000007, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ad490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.6, 7.6, 8, 15.980075841889661, 53.74253242438742, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.7, 6.6, 8, 12.678787713563501, 39.373087286045156, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.9, 14.1, 8, 10.432879276525648, 45.311624523849396, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69aeeb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 0.9, 9, 13.21900000000322, 45.173000000000044, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69aeb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 21.3, 9, 5.010937811024361, 59.52116440113012, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69aef10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.8, 1.4, 9, 6.954778597737881, 45.059256374689, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.6, 10.7, 9, 11.6073880871141, 53.13754794234947, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69380d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 5.2, 9, 3.292191453040761, 38.48657322540579, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69385b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.7, 7.1, 10, 7.389800479710109, 44.43036956959282, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.6, 7.7, 10, 2.4760000000056843, 38.546999999999635, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.1, 15.9, 10, 2.002000000001516, 47.94200000000036, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.3, 2.5, 10, 3.4100000000018946, 56.9709999999997, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fe50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 5.7, 10, 6.298365667907424, 44.98565633993309, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.7, 13.2, 11, 9.207707110431326, 51.692095870232805, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 33.6, 11, 37.49888330296499, 36.824224419478064, 3], 'reward': -8.759999999999991}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 37.0, 13, 6.85599999999981, 57.09099999999971, 2], 'reward': 87.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613cfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.3, 7.7, 13, 3.0952522666768827, 42.61722085997155, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 26.8, 13, 35.90874113960225, 51.47511390944287, 1], 'reward': 1552.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bd430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 20.2, 14, 7.168000000002463, 52.08600000000031, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 15.8, 14, 4.741000000005684, 58.52399999999958, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.7, 18.6, 15, 12.945000000000757, 55.52699999999999, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61551c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.0, 4.4, 16, 15.500230591423035, 49.47306256057078, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.9, 28.2, 17, 3.429000000005684, 38.322000000000266, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f8b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.4, 8.9, 17, 12.346000000000188, 49.94999999999997, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 5.9, 18, 30.787525336082016, 56.160864351176144, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653b460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.6, 13.9, 18, 20.480716880103138, 52.34900899242892, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a8190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 6.9, 18, 12.29449923056293, 59.812926406152755, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.4, 26.5, 20, 43.16453699129366, 54.20042072163302, 0], 'reward': -47.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d3400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 29.8, 21, 8.008000000000568, 28.15800000000016, -1], 'reward': -10.039999999999964}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.7, 2.0, 21, 2.473764622054804, 24.239323575111158, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.3, 6.3, 22, 5.549149530662853, 38.16101129515673, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d34c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.9, 12.2, 22, 18.031348588779696, 30.0132024405151, -1], 'reward': -1.0800000000000125}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.9, 34.9, 22, 37.245925303879055, 43.05476674137453, -1], 'reward': -50.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.6, 33.3, 2, 58.18185835777179, 16.413571540736903, -1], 'reward': 88.88000000000002}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.9  ,  3.8  ,  2.   ,  7.584, 41.828, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     ,  8.5     ,  6.      ,  2.440972, 37.7214  , 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-121.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9      , 15.2      ,  7.       ,  4.6745405, 34.62463  ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.   , 16.2  ,  9.   , 15.   , 42.588, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 17.3     ,  9.      , 14.695181, 58.968933, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 20.3     ,  9.      ,  6.561069, 14.805875, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3      ,  8.1      ,  9.       ,  5.9763923, 44.55379  ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.1     , 11.7     ,  9.      ,  8.629816, 35.69283 , 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.5     , 16.2     ,  9.      , 13.737392, 48.169266, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.8      , 21.3      , 10.       ,  3.8277655, 27.2346   ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.1     , 16.6     , 11.      , 28.89432 , 25.331259, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7     , 20.4     , 11.      ,  3.237657, 39.84112 , 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.2     ,  6.3     , 12.      ,  5.593685, 43.168076, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-144.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3      , 14.2      , 12.       ,  5.2428565, 45.2104   ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 13.2     , 12.      , 25.496729, 41.655804, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-18.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.3      , 12.3      , 12.       ,  3.1893203, 34.0901   ,\n",
      "        36.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.7  , 20.9  , 12.   , 13.213, 51.102, 36.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4     , 43.9     , 12.      , 51.95702 , 28.411613, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=42.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.   , 42.6  , 13.   , 13.509, 43.03 , 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.2  , 35.2  , 16.   , 16.166, 17.965, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     , 15.7     , 17.      , 44.19806 , 31.444805, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.9      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.5     , 24.3     , 20.      ,  1.580669, 53.11387 , 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.1      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.      , 15.9     ,  7.      , 49.215015,  9.647284, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.3     , 40.6     ,  9.      ,  7.914279, 36.00341 , 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.6  , 34.1  , 11.   ,  8.027, 52.68 , 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.2  , 28.1  , 13.   , 13.75 , 44.101, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9      , 23.5      , 15.       ,  2.7424026, 34.879345 ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.4  ,  4.7  , 17.   , 25.453, 16.977, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-116.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.7  , 29.3  , 18.   ,  4.137, 48.655, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.4     , 34.      , 18.      , 33.320366, 56.47241 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-70.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.7  , 13.4  , 19.   ,  7.132, 50.457, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.8  , 22.6  , 20.   ,  7.919, 45.454, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.2     , 10.2     , 21.      , 11.160454, 32.654194, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-125.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5     , 19.7     , 21.      ,  7.205627, 45.801018, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     , 10.8     , 22.      , 14.698601, 48.318333, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.      , 10.5     , 22.      , 17.007288, 35.490288, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-102.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 11.2     , 23.      , 11.193543, 53.157707, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6     , 26.9     , 23.      , 51.64109 , 40.684917, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     , 37.3     ,  8.      , 18.775852, 40.66502 , 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=87.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.8     ,  5.1     ,  8.      , 11.53078 , 56.996647, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-131.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3     , 48.8     ,  8.      , 54.387493, 40.503605, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=86.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.4  , 31.9  , 13.   , 18.493, 56.775, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=38.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.1      , 16.1      , 13.       ,  1.3682326, 23.51425  ,\n",
      "        24.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.3    ,  7.5    , 13.     , 17.94333, 43.16436, 24.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-86.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.7  , 23.3  , 13.   ,  4.997, 60.   , 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5     ,  3.5     , 13.      , 10.039111, 56.106693, 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7      , 11.5      , 13.       ,  1.2875673, 46.746887 ,\n",
      "        23.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 18.2     , 14.      , 18.514433, 32.41351 , 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     , 19.1     , 14.      ,  4.048639, 45.196404, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.      , 22.4     , 14.      , 35.5412  , 50.403347, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=51.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.4  , 14.7  , 14.   , 10.352, 47.436, 20.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.9     , 13.1     , 15.      , 27.170336, 47.28049 , 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2     ,  5.1     , 16.      ,  8.369829, 50.758366, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.7  ,  6.1  , 16.   ,  7.526, 55.919, 19.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     ,  9.9     , 17.      , 11.716911, 48.960957, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1      ,  9.8      , 17.       ,  1.7273767, 50.84481  ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7     , 25.9     , 17.      , 31.408144, 30.036665, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.3  , 24.6  , 17.   ,  3.603, 21.059, 18.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.6     ,  9.9     , 17.      , 18.378683, 43.91514 , 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-108.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.8  , 14.4  , 17.   ,  5.106, 48.915, 17.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.      ,  2.9     , 17.      ,  7.442924, 40.69767 , 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.2     , 21.9     , 17.      , 15.821282, 18.695286, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1     , 16.9     , 17.      , 21.79155 , 19.009838, 16.      ]],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9     ,  4.3     , 18.      , 17.178839, 32.31526 , 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-101.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.4  , 12.4  , 18.   ,  6.52 , 35.188, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     , 24.6     , 18.      , 27.136028, 56.705547, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-60.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 26.8     , 18.      , 31.94747 , 20.656424, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-19.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.4     ,  8.7     , 19.      , 23.574757, 33.487225, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-76.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.5     ,  7.5     , 19.      , 19.161613, 37.66202 , 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     ,  6.5     , 19.      ,  1.001374, 34.7768  , 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7     , 15.3     , 19.      , 21.589252, 41.046234, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-44.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6     , 13.2     , 19.      , 13.761764, 53.564724,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.2      ,  3.5      , 19.       ,  4.2215147, 42.564445 ,\n",
      "         9.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.9  , 17.7  , 19.   ,  7.31 , 32.471,  9.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.      , 22.2     , 19.      ,  4.342945, 55.16374 ,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.9     , 20.3     , 19.      , 17.77924 , 42.741817,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-97.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     , 27.5     , 19.      , 27.239925, 34.740253,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-14.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.4  , 14.3  , 19.   ,  7.963, 39.613,  7.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-4.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.      , 51.      , 19.      , 51.7204  , 27.499264,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=95.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.4  , 30.6  ,  7.   ,  6.76 , 34.236,  5.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-6.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.   , 10.9  ,  7.   , 14.405, 52.001,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8      ,  6.4      ,  7.       ,  2.2750905, 22.593996 ,\n",
      "         4.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3      ,  5.5      ,  7.       ,  0.9755652, 33.54153  ,\n",
      "         4.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.4  ,  2.8  ,  7.   ,  5.752, 35.484,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      , 10.      ,  8.      , 14.516661, 52.803005,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     ,  7.6     ,  8.      , 11.516106, 46.215385,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.2  ,  9.   ,  8.   ,  7.21 , 39.443,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.6     ,  7.6     ,  8.      , 15.980076, 53.74253 ,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.7     ,  6.6     ,  8.      , 12.678788, 39.37309 ,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9     , 14.1     ,  8.      , 10.432879, 45.311623,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.4  ,  0.9  ,  9.   , 13.219, 45.173,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5      , 21.3      ,  9.       ,  5.0109377, 59.521164 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.8      ,  1.4      ,  9.       ,  6.9547787, 45.059258 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6      , 10.7      ,  9.       , 11.6073885, 53.137547 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2      ,  5.2      ,  9.       ,  3.2921915, 38.486572 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.7      ,  7.1      , 10.       ,  7.3898005, 44.43037  ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.6  ,  7.7  , 10.   ,  2.476, 38.547,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.1  , 15.9  , 10.   ,  2.002, 47.942,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.3  ,  2.5  , 10.   ,  3.41 , 56.971,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8      ,  5.7      , 10.       ,  6.2983656, 44.985657 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.7     , 13.2     , 11.      ,  9.207707, 51.692097,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     , 33.6     , 11.      , 37.498882, 36.824223,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  , 37.   , 13.   ,  6.856, 57.091,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=87.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3      ,  7.7      , 13.       ,  3.0952523, 42.61722  ,\n",
      "         1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 26.8     , 13.      , 35.90874 , 51.475113,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1552.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.   , 20.2  , 14.   ,  7.168, 52.086,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.   , 15.8  , 14.   ,  4.741, 58.524,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.7  , 18.6  , 15.   , 12.945, 55.527,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.      ,  4.4     , 16.      , 15.500231, 49.473064,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.9  , 28.2  , 17.   ,  3.429, 38.322,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.4  ,  8.9  , 17.   , 12.346, 49.95 ,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     ,  5.9     , 18.      , 30.787525, 56.160866,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     , 13.9     , 18.      , 20.480717, 52.34901 ,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.6     ,  6.9     , 18.      , 12.294499, 59.812927,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.4     , 26.5     , 20.      , 43.164536, 54.20042 ,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  , 29.8  , 21.   ,  8.008, 28.158, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7      ,  2.       , 21.       ,  2.4737647, 24.239323 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3      ,  6.3      , 22.       ,  5.5491495, 38.16101  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9     , 12.2     , 22.      , 18.03135 , 30.013203, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.9     , 34.9     , 22.      , 37.245926, 43.054768, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.3, 20.3, 5, 13.491988683063822, 32.5624276645147, 40], 'reward': -93.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.6, 41.7, 6, 0.10651689007654852, 20.249702331225194, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.8, 8.5, 6, 2.4409720177717364, 37.72140120808937, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 7.5, 7, 7.267997586802156, 33.64508434019359, 39], 'reward': -46.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.9, 7.3, 8, 10.578974928303072, 46.31767260562961, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.4, 28.7, 8, 32.08920123014499, 45.655716450583746, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.4, 17.5, 8, 8.501000000000378, 50.86099999999994, 38], 'reward': -123.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d3d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.2, 9.1, 9, 14.308146994237074, 39.12392121199203, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 17.3, 9, 14.695180855824894, 58.96893354115201, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.1, 11.7, 9, 8.62981637578337, 35.69282830699598, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 8.1, 9, 5.976392245046368, 44.553790585861904, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 20.3, 9, 6.561069024183675, 14.805875263112963, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.4, 21.3, 9, 3.8277655810327356, 27.234600753952403, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 9.8, 10, 12.13935073453639, 45.19214927317786, 37], 'reward': -71.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623dfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.7, 11.1, 10, 13.784131778403822, 49.46973811184348, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.9, 13.9, 11, 22.97512810285116, 37.75330979367955, 36], 'reward': 24.760000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.1, 18.4, 11, 7.203164535153548, 37.9354166498884, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.1, 20.4, 11, 3.237657110999983, 39.84112069796166, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61482b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.9, 7.5, 11, 18.044666652471918, 35.376473045927554, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610ebe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 9.0, 12, 7.632744241841025, 57.14468542013553, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.1, 6.3, 12, 5.593685200020351, 43.16807732871872, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61480a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.0, 14.2, 12, 5.242856438813538, 45.210399609535266, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 12.3, 12, 3.189320390178912, 34.09010116234906, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d3160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.9, 4.5, 12, 13.4297216252235, 42.29700721717827, 35], 'reward': -80.11999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.3, 20.1, 12, 13.210213898261479, 28.418819013014602, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60731f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.5, 23.8, 12, 37.16820068917117, 42.70476674899008, 34], 'reward': 65.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.2, 20.9, 12, 13.213000000001704, 51.10199999999982, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.3, 19.2, 13, 12.948000000001704, 50.62399999999985, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.8, 15.3, 13, 5.625000000003031, 28.882000000000087, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 13.8, 14, 6.096000000003221, 44.2699999999998, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f6ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 6.2, 15, 11.92723918660634, 51.040886091628515, 33], 'reward': -118.87999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.1, 5.0, 15, 13.438491207084553, 43.889369835322924, 32], 'reward': -39.08}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.4, 27.8, 15, 8.463621507659713, 8.67367987973308, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0c670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.0, 23.6, 15, 33.73027385829613, 27.4555364644817, 31], 'reward': 55.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 35.2, 16, 16.166000000002462, 17.96500000000012, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a58580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.7, 25.2, 16, 18.612000000000187, 56.221000000000274, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe07c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.3, 18.1, 16, 10.359035249533493, 53.89971316739471, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f13a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.6, 16.8, 17, 7.554000000002274, 24.984999999999864, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 26.8, 18, 13.805930385081641, 50.100141967298384, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f9e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.2, 18.5, 19, 4.2998082506770725, 50.62779506014854, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.1, 15.5, 19, 14.970586992809892, 59.15848111257537, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62aca30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.1, 14.4, 19, 1.937897531352645, 45.08205453318258, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.3, 25.5, 19, 3.850980273802019, 43.56873968278472, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.9, 24.7, 19, 1.8069997798133635, 52.3187538622671, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 18.5, 19, 22.981614779525323, 52.81320655257284, 30], 'reward': -2.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.8, 22.8, 19, 4.058230478217324, 15.692403001503234, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bc5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.4, 39.3, 20, 8.363896599348827, 43.753778725841585, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 6.3, 20, 5.022705489636889, 39.74307881140388, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e19d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 24.3, 20, 1.5806690097163951, 53.113867530136844, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb0970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 20.9, 20, 1.094949662786636, 43.58732068158452, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 23.0, 20, 4.906000000000379, 30.547000000000402, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb09a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.3, 27.7, 21, 0.4697131324205088, 49.6066487378464, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.5, 13.4, 21, 13.02811427247202, 39.13109757977746, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb0e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.5, 23.5, 22, 14.520912043830162, 28.299761980683716, 29], 'reward': 58.20000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.1, 20.1, 22, 1.4450000000039789, 49.71700000000016, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb05e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.8, 41.5, 22, 34.1430509417203, 48.36824362688141, 28], 'reward': 38.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.8, 35.4, 0, 6.0235842463972205, 44.49812699024557, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb6580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 21.0, 6, 39.37611691041026, 25.147996580483998, 27], 'reward': 35.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.9, 40.6, 9, 7.9142791835001205, 36.00341194861859, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 34.1, 11, 8.027000000004357, 52.679999999999964, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.8, 4.1, 13, 38.425145687395116, 19.675123894264107, 26], 'reward': -5.919999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.8, 12.3, 14, 18.207853720786684, 19.822328492706504, 25], 'reward': -115.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edeb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.7, 3.6, 14, 10.07614520322453, 34.477690681238464, 24], 'reward': -115.64000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eded90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.0, 26.3, 14, 16.06242181394798, 44.06112119343821, 23], 'reward': -17.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee37c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.7, 4.3, 14, 19.426216019288713, 31.587617731694866, 22], 'reward': -45.39999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edcf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.3, 6.1, 14, 6.892999964811055, 19.57281506029485, 21], 'reward': -57.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.7, 12.6, 14, 17.148635053017166, 0.7853383228407953, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.9, 4.5, 14, 0.3285955711138504, 29.465399586671918, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edeeb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 15.0, 14, 3.8350000000003788, 35.624999999999595, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.2, 14.5, 14, 9.91593016043457, 34.168799290152485, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e694c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.0, 0.6, 14, 8.39776345485768, 16.992415675143633, 20], 'reward': -18.48}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.7, 18.3, 14, 5.885000000003031, 10.869, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.8, 8.5, 14, 3.6810000000036, 32.538000000000046, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.8, 24.7, 15, 29.918978326451416, 36.66824651166041, 19], 'reward': -55.599999999999966}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 23.5, 15, 2.7424026607556016, 34.87934493211107, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 9.7, 15, 3.829483404988631, 42.957989109389764, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 15.2, 15, 2.0320000000009473, 53.97700000000002, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 21.8, 16, 10.995490178383212, 12.034589519781708, 18], 'reward': 6.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e749a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 32.7, 16, 20.98652676765847, 48.8523024809135, 17], 'reward': 67.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.1, 2.4, 16, 14.45781019462693, 47.677999274644286, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.4, 8.8, 16, 7.623000000003979, 49.350999999999765, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.7, 10.5, 16, 20.148489282249052, 53.110376052267874, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.6, 10.8, 16, 13.065753781800113, 40.44119820690583, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e714f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 10.7, 16, 7.089000000004358, 35.80199999999991, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e774f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 19.1, 16, 25.208585130316585, 45.49147656339024, 16], 'reward': -45.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e718b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.7, 16.4, 17, 21.892423570073213, 46.141280372054375, 15], 'reward': -74.67999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 16.8, 17, 24.744178186478937, 39.18430647815613, 14], 'reward': -53.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.3, 28.5, 17, 9.035687176931429, 28.170792567459657, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.5, 5.3, 17, 5.166311719073908, 26.594187330420784, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.4, 16.9, 17, 8.935000000001326, 49.27699999999974, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e772e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.1, 13.1, 17, 20.560847003262296, 35.670323013179285, 13], 'reward': -67.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.1, 10.8, 17, 11.597007850642193, 54.59538428688107, 12], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e775e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.2, 29.6, 17, 17.181507109068477, 18.179115961703445, 12], 'reward': -35.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 24.5, 17, 28.819512720362674, 21.249849474430388, 11], 'reward': -13.399999999999977}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.7, 27.2, 17, 22.430877020251184, 52.61084872532363, 10], 'reward': 27.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 17.3, 17, 23.8949628896425, 39.549778518888075, 9], 'reward': -69.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f02b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.4, 8.2, 17, 13.788707627156418, 50.04504097165263, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.4, 13.6, 17, 4.692000000002842, 35.981000000000186, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630fe80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 2.1, 17, 21.650999999998863, 27.6839999999997, 8], 'reward': -61.959999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67406a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.2, 9.0, 17, 11.967000000000947, 31.732999999999837, 7], 'reward': -13.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 24.7, 17, 2.1454639992604534, 16.62262530300575, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 6.4, 17, 24.317763708441305, 34.108197099148526, 6], 'reward': -23.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.1, 13.7, 17, 16.110343291887713, 38.7646732407273, 5], 'reward': -113.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 23.0, 18, 24.286224272722688, 49.667784641325696, 4], 'reward': -13.43999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.5, 29.3, 18, 19.2228530163923, 32.83872353165326, 3], 'reward': -52.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630fcd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.6, 10.6, 18, 4.031405077871448, 12.271095239672677, 2], 'reward': -92.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66aed90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.9, 16.1, 18, 21.02044249643827, 10.853855693348716, 1], 'reward': 1511.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 4.7, 18, 6.796385519065481, 22.284945169586315, 0], 'reward': -129.79999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 11.3, 18, 6.020023681524686, 14.71800801145506, -1], 'reward': 0.11999999999999034}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 39.4, 18, 22.37885622086838, 54.858438736192994, -1], 'reward': 98.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.7, 10.9, 18, 11.689666444681508, 59.34299326420155, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 5.5, 18, 14.168794456506745, 49.3327091621981, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.6, 24.3, 19, 30.612281437373365, 55.111554988996254, -1], 'reward': -55.52000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ecdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 13.4, 19, 7.131999999999621, 50.45699999999962, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69eccd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.6, 30.1, 19, 16.278954376158815, 11.766529864268232, -1], 'reward': -57.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.4, 30.0, 19, 37.54773407967906, 21.742113327172085, -1], 'reward': -29.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9af70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.3, 9.0, 21, 36.36140476997682, 27.932278480942042, -1], 'reward': -20.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.7, 29.6, 21, 7.519999999999242, 40.96600000000002, -1], 'reward': 55.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 36.8, 22, 30.425031963284958, 8.71886257620826, -1], 'reward': 93.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fef70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.1, 19.9, 9, 0.6420000000022738, 37.57900000000011, -1], 'reward': -93.39999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 3.6, 9, 2.320847698105544, 32.86548016145754, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.4, 5.0, 9, 1.4494150545804763, 33.44615338739398, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 7.4, 9, 9.081701011367619, 33.13541414507219, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62067f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.7, 22.2, 9, 27.178402487523858, 35.90801241078489, -1], 'reward': 11.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 9.7, 10, 4.18000000000379, 28.13800000000036, -1], 'reward': -68.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.1, 6.7, 10, 8.733771671669347, 49.470447225575185, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fec70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.5, 9.8, 10, 2.8667163980259236, 40.75219464115571, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62061f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 10.5, 10, 12.850608953984898, 42.37527984525556, -1], 'reward': -11.280000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.0, 13.7, 11, 9.051939219392205, 56.53124835572259, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.2, 43.2, 11, 44.37003627309852, 17.002648066035796, -1], 'reward': 96.07999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [31.0, 14.7, 14, 10.352000000000567, 47.43599999999979, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631b160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.3, 40.2, 18, 8.494000000005684, 39.13699999999983, -1], 'reward': 113.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.9, 6.3, 18, 6.448000000003031, 44.39999999999991, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.3, 5.7, 18, 5.4590000000036, 42.11400000000024, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631bd00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.4, 33.4, 18, 23.270267876715565, 56.98262311005284, -1], 'reward': 36.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 13.2, 19, 13.761763428283526, 53.56472242654716, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631eb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.0, 32.7, 19, 20.82955043864216, 14.149895167608133, -1], 'reward': -44.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.0, 22.2, 19, 4.342945186992238, 55.16373699565099, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.6, 16.5, 19, 4.26931776691743, 15.552146177553919, -1], 'reward': -114.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.6, 18.2, 19, 0.6945295881499121, 22.02318376009886, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6323850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.9, 27.2, 20, 27.88316692762562, 35.72268766684297, -1], 'reward': 60.52000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 17.7, 20, 4.039000000003221, 24.648000000000394, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.2, 17.1, 22, 5.581000000001895, 54.38999999999993, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.1, 27.3, 0, 49.21376646342881, 54.57037937652233, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61749d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 17.8, 2, 4.298000000004168, 47.74800000000007, -1], 'reward': -30.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 8.0, 6, 10.805018138378202, 38.40937177561687, -1], 'reward': -66.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61848b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 13.2, 6, 22.472277837193413, 45.21715153482536, -1], 'reward': 17.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.4, 17.2, 7, 2.3806756844589403, 21.46889145919354, -1], 'reward': -56.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618cbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 8.7, 7, 3.582606222184735, 47.641675553329925, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.7, 2.0, 7, 5.685000000003599, 21.177999999999674, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618cfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.0, 6.4, 7, 2.2750905287635375, 22.59399606045527, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a57f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.8, 5.5, 7, 0.9755651848023055, 33.541532075682326, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.7, 2.8, 8, 5.751999999999241, 35.48399999999965, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.9, 6.6, 8, 12.678787713563501, 39.373087286045156, -1], 'reward': -80.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62adbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.7, 21.3, 8, 5.010937811024361, 59.52116440113012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ada30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 0.9, 8, 13.21900000000322, 45.173000000000044, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ae730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.1, 21.0, 8, 0.4213554665351449, 25.726687751319602, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ad520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.2, 20.2, 9, 30.063372835675395, 46.293853501548185, -1], 'reward': 22.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.3, 15.9, 10, 2.002000000001516, 47.94200000000036, -1], 'reward': -100.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69387c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.0, 2.5, 10, 3.4100000000018946, 56.9709999999997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 7.1, 10, 7.389800479710109, 44.43036956959282, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 7.7, 10, 2.4760000000056843, 38.546999999999635, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b13d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [0.5, 5.7, 10, 6.298365667907424, 44.98565633993309, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 16.1, 10, 18.88462256062126, 46.88067330275168, -1], 'reward': 2.5600000000000023}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619f6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.0, 13.2, 11, 9.207707110431326, 51.692095870232805, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 25.7, 11, 37.384000000003404, 56.84200000000021, -1], 'reward': 44.84}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61269d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 27.4, 12, 10.347000000000568, 55.81299999999993, -1], 'reward': 33.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.3, 5.5, 12, 7.494000000001895, 40.19599999999995, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61265b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 12.9, 12, 15.185379491892622, 38.575201795146434, -1], 'reward': -21.960000000000008}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.4, 17.6, 12, 3.893000000004168, 44.93199999999993, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61261c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.0, 9.5, 12, 3.9489075321998555, 19.336140007530577, -1], 'reward': -85.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.3, 4.8, 13, 4.232504417663771, 33.54930403616702, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.2, 1.7, 13, 2.749134754406489, 7.251628039156114, -1], 'reward': -77.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bd970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.1, 11.3, 14, 9.551000000002462, 19.86399999999978, -1], 'reward': 15.079999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.9, 20.2, 14, 10.73250465132228, 15.654797267971961, -1], 'reward': -43.47999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 9.4, 15, 2.0560446519675377, 22.50475705019034, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.4, 23.1, 15, 27.08800000000227, 29.102999999999874, -1], 'reward': 10.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.1, 18.6, 15, 12.945000000000757, 55.52699999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.0, 8.5, 16, 22.689755041929317, 44.94328973423688, -1], 'reward': -122.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.2, 4.4, 16, 15.500230591423035, 49.47306256057078, -1], 'reward': -41.67999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60eb1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.0, 11.7, 16, 22.154045497764915, 35.337541358165865, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61556a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.5, 4.6, 16, 12.001000000002652, 57.51299999999988, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 17.6, 16, 23.0295625527001, 42.33211646449877, -1], 'reward': -12.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 5.0, 17, 18.26061415189011, 36.23021019178849, -1], 'reward': -8.479999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 17.3, 17, 6.470645260641511, 49.105805109496146, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610ea30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.5, 4.7, 17, 19.958000000000567, 32.28900000000015, -1], 'reward': -8.759999999999991}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61486d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.0, 17.8, 17, 1.7650358135482218, 34.98279272962623, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.8, 15.9, 17, 19.33283586598013, 44.10059933270286, -1], 'reward': -104.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 3.3, 17, 15.495577835887172, 43.332947889115644, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 6.1, 17, 5.874465894788705, 50.74712715483066, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610ee50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 5.8, 17, 4.129944224758205, 44.63478126740394, -1], 'reward': -73.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 8.9, 17, 12.346000000000188, 49.94999999999997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60eb430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 18.5, 17, 14.544087231132888, 29.196906897255392, -1], 'reward': 26.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60eb280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 5.8, 17, 4.241665609516133, 46.82404973813961, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a80d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 36.9, 17, 41.735475464143036, 27.901148257660708, -1], 'reward': 50.76000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.4, 14.0, 19, 25.8639579761986, 25.413421583509614, -1], 'reward': -12.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.1, 9.9, 19, 3.08821696999055, 38.98517340898783, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.1, 34.9, 19, 56.41736318296096, 8.231715991797689, -1], 'reward': 77.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 11.4, 18, 51.69500000000208, 0.903000000000122, -1], 'reward': -80.48000000000002}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3     , 20.3     ,  5.      , 13.491989, 32.562428, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-93.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.6       , 41.7       ,  6.        ,  0.10651689, 20.249702  ,\n",
      "        39.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     ,  8.5     ,  6.      ,  2.440972, 37.7214  , 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3      ,  7.5      ,  7.       ,  7.2679977, 33.645084 ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9     ,  7.3     ,  8.      , 10.578975, 46.317673, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.4     , 28.7     ,  8.      , 32.089203, 45.655716, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[26.4  , 17.5  ,  8.   ,  8.501, 50.861, 38.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-123.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     ,  9.1     ,  9.      , 14.308147, 39.12392 , 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     , 17.3     ,  9.      , 14.695181, 58.968933, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.1     , 11.7     ,  9.      ,  8.629816, 35.69283 , 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9      ,  8.1      ,  9.       ,  5.9763923, 44.55379  ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 20.3     ,  9.      ,  6.561069, 14.805875, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.4      , 21.3      ,  9.       ,  3.8277655, 27.2346   ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     ,  9.8     , 10.      , 12.139351, 45.19215 , 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-71.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     , 11.1     , 10.      , 13.784132, 49.469738, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9     , 13.9     , 11.      , 22.975128, 37.75331 , 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1      , 18.4      , 11.       ,  7.2031646, 37.935417 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     , 20.4     , 11.      ,  3.237657, 39.84112 , 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9     ,  7.5     , 11.      , 18.044666, 35.376472, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8      ,  9.       , 12.       ,  7.6327443, 57.144684 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1     ,  6.3     , 12.      ,  5.593685, 43.168076, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.       , 14.2      , 12.       ,  5.2428565, 45.2104   ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4      , 12.3      , 12.       ,  3.1893203, 34.0901   ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     ,  4.5     , 12.      , 13.429722, 42.29701 , 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.3     , 20.1     , 12.      , 13.210214, 28.41882 , 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.5     , 23.8     , 12.      , 37.1682  , 42.704765, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=65.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.2  , 20.9  , 12.   , 13.213, 51.102, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.3  , 19.2  , 13.   , 12.948, 50.624, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.8  , 15.3  , 13.   ,  5.625, 28.882, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.   , 13.8  , 14.   ,  6.096, 44.27 , 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     ,  6.2     , 15.      , 11.927239, 51.040886, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-118.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     ,  5.      , 15.      , 13.438491, 43.88937 , 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.4     , 27.8     , 15.      ,  8.463621,  8.67368 , 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.      , 23.6     , 15.      , 33.730274, 27.455536, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=55.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.3  , 35.2  , 16.   , 16.166, 17.965, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.7  , 25.2  , 16.   , 18.612, 56.221, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.3      , 18.1      , 16.       , 10.3590355, 53.89971  ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.6  , 16.8  , 17.   ,  7.554, 24.985, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7     , 26.8     , 18.      , 13.80593 , 50.100143, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2     , 18.5     , 19.      ,  4.299808, 50.627796, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.1     , 15.5     , 19.      , 14.970587, 59.15848 , 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.1      , 14.4      , 19.       ,  1.9378976, 45.082054 ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.9      , 24.7      , 19.       ,  1.8069998, 52.318752 ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 18.5     , 19.      , 22.981615, 52.813206, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.8      , 22.8      , 19.       ,  4.0582304, 15.692403 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.4     , 39.3     , 20.      ,  8.363896, 43.75378 , 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.       ,  6.3      , 20.       ,  5.0227056, 39.74308  ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     , 24.3     , 20.      ,  1.580669, 53.11387 , 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.6  , 23.   , 20.   ,  4.906, 30.547, 29.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3       , 27.7       , 21.        ,  0.46971312, 49.606647  ,\n",
      "        29.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.5     , 13.4     , 21.      , 13.028114, 39.131096, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.5     , 23.5     , 22.      , 14.520912, 28.299763, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=58.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.1  , 20.1  , 22.   ,  1.445, 49.717, 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.8     , 41.5     , 22.      , 34.14305 , 48.368244, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=38.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8      , 35.4      ,  0.       ,  6.0235844, 44.498127 ,\n",
      "        27.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     , 21.      ,  6.      , 39.376118, 25.147997, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.9     , 40.6     ,  9.      ,  7.914279, 36.00341 , 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 34.1  , 11.   ,  8.027, 52.68 , 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.8     ,  4.1     , 13.      , 38.425144, 19.675123, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.8     , 12.3     , 14.      , 18.207853, 19.822329, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-115.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.7     ,  3.6     , 14.      , 10.076145, 34.47769 , 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-115.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      , 26.3     , 14.      , 16.062422, 44.061123, 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     ,  4.3     , 14.      , 19.426216, 31.587618, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.3     ,  6.1     , 14.      ,  6.893   , 19.572815, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-57.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7       , 12.6       , 14.        , 17.148636  ,  0.78533834,\n",
      "        20.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9       ,  4.5       , 14.        ,  0.32859558, 29.465399  ,\n",
      "        20.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  , 15.   , 14.   ,  3.835, 35.625, 20.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2    , 14.5    , 14.     ,  9.91593, 34.1688 , 20.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.      ,  0.6     , 14.      ,  8.397763, 16.992416, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-18.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.7  , 18.3  , 14.   ,  5.885, 10.869, 19.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.8  ,  8.5  , 14.   ,  3.681, 32.538, 19.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.8     , 24.7     , 15.      , 29.918978, 36.668247, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2      , 23.5      , 15.       ,  2.7424026, 34.879345 ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2      ,  9.7      , 15.       ,  3.8294835, 42.95799  ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.   , 15.2  , 15.   ,  2.032, 53.977, 18.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3    , 21.8    , 16.     , 10.99549, 12.03459, 18.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=6.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5     , 32.7     , 16.      , 20.986526, 48.852303, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     ,  2.4     , 16.      , 14.45781 , 47.677998, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.4  ,  8.8  , 16.   ,  7.623, 49.351, 16.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.7     , 10.5     , 16.      , 20.148489, 53.110374, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 10.8     , 16.      , 13.065754, 40.441196, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.6  , 10.7  , 16.   ,  7.089, 35.802, 16.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7     , 19.1     , 16.      , 25.208586, 45.491478, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.7     , 16.4     , 17.      , 21.892424, 46.14128 , 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-74.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7     , 16.8     , 17.      , 24.744179, 39.184307, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-53.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3     , 28.5     , 17.      ,  9.035687, 28.170792, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.5      ,  5.3      , 17.       ,  5.1663117, 26.594187 ,\n",
      "        13.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.4  , 16.9  , 17.   ,  8.935, 49.277, 13.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1     , 13.1     , 17.      , 20.560846, 35.670322, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     , 10.8     , 17.      , 11.597008, 54.595383, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2     , 29.6     , 17.      , 17.181507, 18.179115, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.5     , 24.5     , 17.      , 28.819513, 21.24985 , 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-13.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     , 27.2     , 17.      , 22.430878, 52.610847, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=27.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.3     , 17.3     , 17.      , 23.894962, 39.549778,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4     ,  8.2     , 17.      , 13.788708, 50.04504 ,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.4  , 13.6  , 17.   ,  4.692, 35.981,  8.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.1  ,  2.1  , 17.   , 21.651, 27.684,  8.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-61.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.2  ,  9.   , 17.   , 11.967, 31.733,  7.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-13.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.      , 24.7     , 17.      ,  2.145464, 16.622625,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     ,  6.4     , 17.      , 24.317764, 34.108196,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.1     , 13.7     , 17.      , 16.110344, 38.764675,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-113.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     , 23.      , 18.      , 24.286224, 49.667786,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-13.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.5     , 29.3     , 18.      , 19.222853, 32.838722,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.6     , 10.6     , 18.      ,  4.031405, 12.271095,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-92.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9     , 16.1     , 18.      , 21.020443, 10.853856,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1511.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3      ,  4.7      , 18.       ,  6.7963853, 22.284945 ,\n",
      "         0.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-129.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 11.3     , 18.      ,  6.020024, 14.718008, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1     , 39.4     , 18.      , 22.378857, 54.85844 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=98.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.7     , 10.9     , 18.      , 11.689667, 59.342995, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     ,  5.5     , 18.      , 14.168795, 49.33271 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.6     , 24.3     , 19.      , 30.612282, 55.111553, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.8  , 13.4  , 19.   ,  7.132, 50.457, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.6     , 30.1     , 19.      , 16.278954, 11.76653 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-57.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.4     , 30.      , 19.      , 37.547733, 21.742113, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-29.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3     ,  9.      , 21.      , 36.361404, 27.932278, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.7  , 29.6  , 21.   ,  7.52 , 40.966, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=55.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 36.8     , 22.      , 30.425032,  8.718863, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=93.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.1  , 19.9  ,  9.   ,  0.642, 37.579, -1.   ]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-93.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4      ,  3.6      ,  9.       ,  2.3208477, 32.86548  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4      ,  5.       ,  9.       ,  1.4494151, 33.44615  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     ,  7.4     ,  9.      ,  9.081701, 33.135414, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     , 22.2     ,  9.      , 27.178402, 35.908012, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=11.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.7  ,  9.7  , 10.   ,  4.18 , 28.138, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-68.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     ,  6.7     , 10.      ,  8.733771, 49.470448, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.5      ,  9.8      , 10.       ,  2.8667164, 40.752193 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     , 10.5     , 10.      , 12.850609, 42.37528 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.      , 13.7     , 11.      ,  9.051939, 56.53125 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.2     , 43.2     , 11.      , 44.370037, 17.002647, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=96.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[31.   , 14.7  , 14.   , 10.352, 47.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.3  , 40.2  , 18.   ,  8.494, 39.137, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=113.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.9  ,  6.3  , 18.   ,  6.448, 44.4  , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.3  ,  5.7  , 18.   ,  5.459, 42.114, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.4     , 33.4     , 18.      , 23.270267, 56.982624, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=36.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 13.2     , 19.      , 13.761764, 53.564724, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      , 32.7     , 19.      , 20.82955 , 14.149895, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-44.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.      , 22.2     , 19.      ,  4.342945, 55.16374 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.6      , 16.5      , 19.       ,  4.2693176, 15.552146 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-114.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.6      , 18.2      , 19.       ,  0.6945296, 22.023184 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.9     , 27.2     , 20.      , 27.883167, 35.722687, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=60.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.   , 17.7  , 20.   ,  4.039, 24.648, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.2  , 17.1  , 22.   ,  5.581, 54.39 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.1     , 27.3     ,  0.      , 49.213768, 54.57038 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.8  , 17.8  ,  2.   ,  4.298, 47.748, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     ,  8.      ,  6.      , 10.805018, 38.40937 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-66.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     , 13.2     ,  6.      , 22.472279, 45.21715 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=17.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.4      , 17.2      ,  7.       ,  2.3806758, 21.468891 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7      ,  8.7      ,  7.       ,  3.5826063, 47.641674 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.7  ,  2.   ,  7.   ,  5.685, 21.178, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.       ,  6.4      ,  7.       ,  2.2750905, 22.593996 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.8      ,  5.5      ,  7.       ,  0.9755652, 33.54153  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.7  ,  2.8  ,  8.   ,  5.752, 35.484, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.9     ,  6.6     ,  8.      , 12.678788, 39.37309 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.7      , 21.3      ,  8.       ,  5.0109377, 59.521164 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.8  ,  0.9  ,  8.   , 13.219, 45.173, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1       , 21.        ,  8.        ,  0.42135546, 25.726688  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.2     , 20.2     ,  9.      , 30.063374, 46.293854, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.3  , 15.9  , 10.   ,  2.002, 47.942, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-100.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   ,  2.5  , 10.   ,  3.41 , 56.971, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1      ,  7.1      , 10.       ,  7.3898005, 44.43037  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.1  ,  7.7  , 10.   ,  2.476, 38.547, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.5      ,  5.7      , 10.       ,  6.2983656, 44.985657 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2     , 16.1     , 10.      , 18.884623, 46.880672, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=2.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.      , 13.2     , 11.      ,  9.207707, 51.692097, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.5  , 25.7  , 11.   , 37.384, 56.842, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=44.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   , 27.4  , 12.   , 10.347, 55.813, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=33.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.3  ,  5.5  , 12.   ,  7.494, 40.196, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     , 12.9     , 12.      , 15.185379, 38.575203, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.4  , 17.6  , 12.   ,  3.893, 44.932, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.       ,  9.5      , 12.       ,  3.9489076, 19.33614  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-85.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3      ,  4.8      , 13.       ,  4.2325044, 33.549305 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.2      ,  1.7      , 13.       ,  2.7491348,  7.251628 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-77.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.1  , 11.3  , 14.   ,  9.551, 19.864, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9     , 20.2     , 14.      , 10.732505, 15.654798, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1      ,  9.4      , 15.       ,  2.0560446, 22.504757 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.4  , 23.1  , 15.   , 27.088, 29.103, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.1  , 18.6  , 15.   , 12.945, 55.527, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      ,  8.5     , 16.      , 22.689754, 44.94329 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-122.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.2     ,  4.4     , 16.      , 15.500231, 49.473064, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.      , 11.7     , 16.      , 22.154045, 35.33754 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.5  ,  4.6  , 16.   , 12.001, 57.513, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.1     , 17.6     , 16.      , 23.029562, 42.332115, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     ,  5.      , 17.      , 18.260614, 36.23021 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.2      , 17.3      , 17.       ,  6.4706454, 49.105804 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.5  ,  4.7  , 17.   , 19.958, 32.289, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.       , 17.8      , 17.       ,  1.7650359, 34.98279  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.8     , 15.9     , 17.      , 19.332836, 44.1006  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-104.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     ,  3.3     , 17.      , 15.495578, 43.332947, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1     ,  6.1     , 17.      ,  5.874466, 50.747128, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.5      ,  5.8      , 17.       ,  4.1299443, 44.63478  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.6  ,  8.9  , 17.   , 12.346, 49.95 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     , 18.5     , 17.      , 14.544087, 29.196907, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     ,  5.8     , 17.      ,  4.241666, 46.82405 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     , 36.9     , 17.      , 41.735474, 27.901148, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     , 14.      , 19.      , 25.863958, 25.413422, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1     ,  9.9     , 19.      ,  3.088217, 38.985172, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.1     , 34.9     , 19.      , 56.417362,  8.231716, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a3040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 31.4, 9, 11.001000000001325, 41.27000000000002, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 42.6, 13, 13.509000000001704, 43.029999999999816, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61071c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.9, 16.8, 17, 7.554000000002274, 24.984999999999864, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb6bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 17.9, 4, 37.0570000000036, 19.387999999999863, 40], 'reward': 42.32000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.3, 7.5, 8, 13.941624537833253, 6.775338863862422, 39], 'reward': -107.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.7, 18.1, 8, 0.33100000000132634, 22.36000000000009, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec54c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.4, 10.6, 8, 7.8874153165102125, 16.974174272045865, 38], 'reward': -118.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec57f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 38.6, 9, 3.0539110335721737, 53.028310710166735, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec9400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.1, 14.6, 9, 3.286248095436692, 38.38435630413236, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 4.5, 10, 1.065999999999621, 16.654000000000142, 37], 'reward': -32.519999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec99d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 13.0, 10, 2.0519999999996212, 28.9170000000004, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eccbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 24.6, 11, 15.685259452313689, 46.89103358682864, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecfdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 13.9, 11, 3.019000000000379, 12.487000000000213, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecf6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 4.9, 11, 2.3550000000036, 41.55699999999985, 36], 'reward': -129.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecfa60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.8, 17.3, 11, 24.7651691672987, 46.9923035503318, 35], 'reward': -38.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecfd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.8, 4.2, 11, 4.726856037882229, 53.792973203940846, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 19.7, 11, 50.63908356772069, 35.481663901389794, 34], 'reward': -0.19999999999998863}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 28.1, 13, 13.750000000001325, 44.10099999999972, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 29.3, 18, 4.136999999999432, 48.655000000000044, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6356c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 22.7, 22, 34.7532183853568, 23.28924542350322, 33], 'reward': -15.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6356760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.4, 37.0, 22, 12.098508902598095, 53.6396882710389, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 20.7, 6, 8.744208783218479, 32.5092333464284, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.4, 27.9, 7, 6.497999999999242, 48.02400000000011, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ee9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 19.3, 7, 45.97828166566239, 21.097285543648194, 32], 'reward': 8.039999999999992}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 15.1, 17, 52.171730800307316, 17.19925391607439, 31], 'reward': -4.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.4, 30.6, 7, 6.759999999998105, 34.23600000000005, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 45.0, 19, 3.0190000000056845, 47.76900000000032, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603ea00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 5.0, 15, 42.2020592581517, 22.78444923810732, 30], 'reward': -31.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605dca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.2, 17.2, 16, 56.07301296720052, 29.80574180422799, 29], 'reward': 19.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9bee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 18.8, 17, 29.582249570067358, 31.41106477534512, 28], 'reward': 7.1200000000000045}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.4, 16.1, 17, 3.538999999999621, 42.1479999999999, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f524f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.9, 12.9, 18, 5.482533497719135, 29.1806900875728, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 6.2, 18, 14.992831364970167, 28.183470794526997, 27], 'reward': -110.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.4, 14.5, 18, 3.6160000000020838, 47.98400000000023, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.6, 16.8, 18, 8.743847042752279, 28.21628310217631, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.4, 14.2, 18, 12.111094992676046, 44.39862012217112, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.9, 14.7, 18, 9.85075443300669, 55.4241820198175, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 2.6, 18, 4.775679457911068, 27.036169253856887, 26], 'reward': -61.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 6.2, 18, 0.5429980665941949, 22.602137105706237, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.9, 12.0, 18, 5.60400000000379, 42.435999999999865, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 4.9, 19, 6.198576146655095, 37.25758810310518, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 5.1, 19, 11.960830499715888, 38.73695865283263, 25], 'reward': -43.519999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e66d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.3, 41.5, 19, 14.436483649076479, 3.3572658692135278, 24], 'reward': 83.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f5a940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.6, 4.1, 19, 5.786000000002084, 21.53899999999994, 23], 'reward': -113.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.4, 47.8, 19, 51.05968030666854, 7.431393676070192, 22], 'reward': 150.24000000000007}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.6  , 31.4  ,  9.   , 11.001, 41.27 , 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.7  , 42.6  , 13.   , 13.509, 43.03 , 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.9  , 16.8  , 17.   ,  7.554, 24.985, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.2  , 17.9  ,  4.   , 37.057, 19.388, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=42.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3      ,  7.5      ,  8.       , 13.941625 ,  6.7753386,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-107.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.7  , 18.1  ,  8.   ,  0.331, 22.36 , 38.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.4      , 10.6      ,  8.       ,  7.8874154, 16.974174 ,\n",
      "        38.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-118.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.2     , 38.6     ,  9.      ,  3.053911, 53.02831 , 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1      , 14.6      ,  9.       ,  3.2862482, 38.384357 ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.9  ,  4.5  , 10.   ,  1.066, 16.654, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.   , 13.   , 10.   ,  2.052, 28.917, 36.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     , 24.6     , 11.      , 15.68526 , 46.891033, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.1  , 13.9  , 11.   ,  3.019, 12.487, 36.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.3  ,  4.9  , 11.   ,  2.355, 41.557, 36.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-129.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.8     , 17.3     , 11.      , 24.76517 , 46.992302, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-38.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.8     ,  4.2     , 11.      ,  4.726856, 53.792973, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     , 19.7     , 11.      , 50.639084, 35.481663, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 28.1  , 13.   , 13.75 , 44.101, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.7  , 29.3  , 18.   ,  4.137, 48.655, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 22.7     , 22.      , 34.75322 , 23.289246, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     , 37.      , 22.      , 12.098509, 53.639687, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3     , 20.7     ,  6.      ,  8.744208, 32.50923 , 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.4  , 27.9  ,  7.   ,  6.498, 48.024, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.9     , 19.3     ,  7.      , 45.978283, 21.097286, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=8.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     , 15.1     , 17.      , 52.17173 , 17.199253, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-4.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.4  , 30.6  ,  7.   ,  6.76 , 34.236, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.   , 45.   , 19.   ,  3.019, 47.769, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.      ,  5.      , 15.      , 42.20206 , 22.784449, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-31.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.2     , 17.2     , 16.      , 56.073013, 29.805742, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=19.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     , 18.8     , 17.      , 29.582249, 31.411064, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.4  , 16.1  , 17.   ,  3.539, 42.148, 27.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9      , 12.9      , 18.       ,  5.4825335, 29.18069  ,\n",
      "        27.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     ,  6.2     , 18.      , 14.992831, 28.183472, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-110.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.4  , 14.5  , 18.   ,  3.616, 47.984, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.6     , 16.8     , 18.      ,  8.743847, 28.216284, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.4     , 14.2     , 18.      , 12.111095, 44.39862 , 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     , 14.7     , 18.      ,  9.850755, 55.424183, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2      ,  2.6      , 18.       ,  4.7756796, 27.03617  ,\n",
      "        26.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-61.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4      ,  6.2      , 18.       ,  0.5429981, 22.602137 ,\n",
      "        25.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.9  , 12.   , 18.   ,  5.604, 42.436, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     ,  4.9     , 19.      ,  6.198576, 37.257587, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8     ,  5.1     , 19.      , 11.960831, 38.736958, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3     , 41.5     , 19.      , 14.436483,  3.357266, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=83.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.6  ,  4.1  , 19.   ,  5.786, 21.539, 23.   ]], dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.2, 41.7, 6, 0.10651689007654852, 20.249702331225194, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.9, 16.1, 6, 14.608178088977258, 35.61922542408384, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63541c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 15.2, 7, 4.6745403763371955, 34.62463023286752, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 7.3, 8, 10.578974928303072, 46.31767260562961, 40], 'reward': -60.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.5, 17.5, 8, 8.501000000000378, 50.86099999999994, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bac40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.6, 27.7, 8, 41.876220268386405, 51.12687287720279, 39], 'reward': 30.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 20.9, 12, 13.213000000001704, 51.10199999999982, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a534f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 25.2, 16, 9.300863694491312, 24.401981858319413, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.2, 41.0, 16, 13.58700000000019, 36.147999999999875, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62339d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.8, 30.8, 17, 9.036209088721225, 37.03642350893827, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f17f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 15.1, 17, 52.39214671149906, 48.495776357882164, 38], 'reward': 0.040000000000020464}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 39.3, 19, 8.363896599348827, 43.753778725841585, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb20a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 35.4, 0, 6.0235842463972205, 44.49812699024557, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebdd30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 9.0, 8, 30.92399999999867, 60.0, 37], 'reward': -86.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 40.6, 9, 7.9142791835001205, 36.00341194861859, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.4, 14.6, 10, 24.45995198195122, 28.776518983484568, 36], 'reward': -132.8}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 14.6, 10, 3.286248095436692, 38.38435630413236, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.7, 16.7, 10, 26.319511623389985, 47.88675589608593, 35], 'reward': -60.119999999999976}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eccc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 28.5, 10, 45.84398242334734, 14.883336226355592, 34], 'reward': -1.2800000000000296}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 7.7, 16, 33.58525516994894, 18.585975616698367, 33], 'reward': -19.559999999999988}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ae5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.3, 23.0, 17, 45.91999999999867, 35.998000000000246, 32], 'reward': 30.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 29.3, 18, 4.136999999999432, 48.655000000000044, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 13.6, 10, 41.52517010314183, 19.403840900841544, 31], 'reward': 14.959999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.9, 17.7, 13, 55.910465141382616, 31.149345828789606, 30], 'reward': 50.520000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616cbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 24.9, 19, 25.95400000000227, 25.353000000000247, 29], 'reward': 35.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 18.2, 19, 0.6945295881499121, 22.02318376009886, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.2, 24.3, 19, 6.162000000003031, 39.92499999999976, 28], 'reward': -5.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630dc70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.2, 4.3, 20, 5.304563635192457, 55.84457412557084, 27], 'reward': -123.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.2, 8.1, 20, 7.608629745722144, 52.250173619572976, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 17.0, 20, 7.2060000000036, 33.99799999999995, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6323250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.3, 9.3, 20, 11.195622517336766, 37.90150647014478, 26], 'reward': -67.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.1, 15.0, 20, 19.457292922879088, 24.495348804063703, 25], 'reward': -7.0800000000000125}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631b370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 21.9, 20, 28.1294315295866, 30.720997456933382, 24], 'reward': -29.879999999999967}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.8, 17.7, 20, 4.039000000003221, 24.648000000000394, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.9, 17.1, 22, 5.581000000001895, 54.38999999999993, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.1, 27.3, 0, 49.21376646342881, 54.57037937652233, 23], 'reward': 32.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.2, 28.2, 5, 15.559000000002841, 34.62500000000005, 22], 'reward': 14.080000000000041}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.7, 17.3, 6, 19.831282313799093, 58.32128633688948, 21], 'reward': -31.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.5, 8.7, 7, 3.582606222184735, 47.641675553329925, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.3, 6.0, 7, 9.284000000005685, 54.67300000000032, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6192670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 7.7, 7, 10.401077474184014, 52.13736875687872, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.5, 10.0, 8, 14.516660298923199, 52.80300417232819, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618cdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.9, 23.7, 8, 29.393237326830906, 56.24609479017282, 20], 'reward': -18.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69aee80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.1, 21.3, 9, 5.010937811024361, 59.52116440113012, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 4.1, 9, 23.369212037231144, 47.501534123402735, 19], 'reward': -79.35999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.0, 10.7, 9, 11.6073880871141, 53.13754794234947, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61997c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 26.4, 9, 28.002381356776, 43.656890155385966, 18], 'reward': -54.23999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b1700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.1, 12.8, 11, 26.448107398548007, 47.47836398554826, 17], 'reward': -20.919999999999987}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fcd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.3, 13.2, 11, 9.207707110431326, 51.692095870232805, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 16.7, 11, 5.404000000003031, 36.56499999999977, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dc10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.5, 5.7, 11, 2.154568900143198, 39.4968255537382, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619f580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.8, 42.4, 11, 5.597448525334562, 17.549325445467353, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.2, 19.1, 11, 7.742370033792335, 38.76200483491238, 16], 'reward': -110.23999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61263d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.1, 23.5, 12, 28.152961752331596, 26.677676623754248, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d8b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 18.3, 12, 2.1639999999996213, 30.41600000000012, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 23.2, 12, 26.858194828979077, 42.07985852146708, 15], 'reward': 46.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.8, 5.5, 12, 7.494000000001895, 40.19599999999995, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61261f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.2, 17.6, 13, 3.893000000004168, 44.93199999999993, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b58e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.6, 6.3, 13, 6.587785537274513, 41.80863673087923, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.2, 4.8, 13, 4.232504417663771, 33.54930403616702, 14], 'reward': -122.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.3, 8.8, 13, 17.178779986094668, 56.934232223962525, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.7, 26.5, 13, 30.827050805206905, 42.30934780618175, 13], 'reward': 73.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bd610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 7.2, 13, 34.95875126852185, 51.24860509741899, 12], 'reward': -1.4399999999999977}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 20.2, 14, 7.168000000002463, 52.08600000000031, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.0, 15.8, 14, 4.741000000005684, 58.52399999999958, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 18.6, 15, 12.945000000000757, 55.52699999999999, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 4.4, 16, 15.500230591423035, 49.47306256057078, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.9, 28.2, 17, 3.429000000005684, 38.322000000000266, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.5, 8.9, 17, 12.346000000000188, 49.94999999999997, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.8, 30.3, 17, 12.321999999999052, 32.917000000000165, 11], 'reward': 84.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611faf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.4, 5.8, 17, 4.241665609516133, 46.82404973813961, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a8640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.0, 25.9, 17, 30.719633875936342, 32.25811692025722, 10], 'reward': 8.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.5, 32.3, 18, 0.03446957712973209, 16.680753063303683, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60acee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.7, 32.2, 19, 54.50434473768111, 25.03564406278782, 9], 'reward': 37.079999999999984}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.9, 21.1, 13, 14.904999999999431, 19.560999999999925, 8], 'reward': -74.59999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 15.1, 13, 11.645492539322968, 48.86764871513695, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61676d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 7.7, 13, 2.4204350532305106, 17.458433409814997, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 23.2, 13, 19.90414725943262, 41.29743473266815, 7], 'reward': 22.56000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 6.5, 13, 21.250112913374828, 41.2090987834204, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 35.1, 13, 24.845508285519504, 10.076499779902363, 6], 'reward': 52.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd09a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [29.0, 5.0, 14, 8.088840232326625, 33.66816720506517, 5], 'reward': -181.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 13.4, 14, 18.582105638969146, 36.552203493661615, 4], 'reward': -4.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.4, 5.8, 14, 8.063319851712262, 56.95122125145516, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624f280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.9, 15.2, 14, 14.034531493416184, 54.17665326845435, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.2, 13.0, 15, 10.270000000002083, 45.28899999999986, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fc10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.9, 29.4, 15, 40.5717940277697, 9.718713810821665, 3], 'reward': 53.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 31.3, 15, 20.952000000000755, 44.41599999999982, 2], 'reward': 8.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.7, 3.7, 15, 8.621220926902565, 59.849562578599574, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e53d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.3, 44.6, 15, 35.255977182653595, 14.647722833724607, 1], 'reward': 1538.6799999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625fee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.8, 20.3, 17, 6.1697933083871135, 31.528598336299456, 0], 'reward': -42.47999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.6, 8.0, 17, 2.4738646510867874, 42.45796508777647, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.0, 29.3, 17, 26.15080382971205, 23.47050297670788, -1], 'reward': 12.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8ab20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.7, 0.6, 17, 6.851737695045719, 34.48286875578675, -1], 'reward': -145.64000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9bdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 13.2, 17, 17.33134627001393, 35.69275849137202, -1], 'reward': 10.960000000000022}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 9.5, 17, 5.499906323349843, 39.97621362593936, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9ba00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.3, 24.4, 17, 12.955200633071676, 46.665532471278695, -1], 'reward': -39.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8a1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.6, 2.6, 17, 12.92952127826278, 58.17478341206964, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 40.3, 17, 15.56263226910307, 20.43090030851875, -1], 'reward': 29.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8a160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 9.9, 17, 5.750000000002084, 33.21199999999997, -1], 'reward': -51.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 19.8, 17, 15.739000000001894, 52.119999999999905, -1], 'reward': 40.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 22.5, 17, 5.923092604329807, 32.37921145020607, -1], 'reward': -11.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.6, 25.6, 17, 18.4258941970454, 53.203615565431235, -1], 'reward': 71.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9ba60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.3, 11.0, 18, 31.350609493516217, 46.41653030822231, -1], 'reward': -21.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62587c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.7, 3.7, 18, 21.394205622184323, 48.68807679576751, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.5, 12.9, 18, 5.482533497719135, 29.1806900875728, -1], 'reward': -84.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 29.4, 18, 9.67365941448104, 15.462141233368833, -1], 'reward': -11.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.3, 14.5, 18, 3.6160000000020838, 47.98400000000023, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f522b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.4, 6.2, 18, 0.5429980665941949, 22.602137105706237, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 12.0, 19, 5.60400000000379, 42.435999999999865, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 24.8, 19, 27.135125364594128, 7.423777302830739, -1], 'reward': 41.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f5a8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.9, 27.2, 19, 38.62192946286699, 32.526227162921664, -1], 'reward': 67.32000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef45e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 16.5, 20, 8.118000000004736, 56.63099999999988, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.4, 26.2, 20, 0.7740000000013263, 40.49299999999983, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ecf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 16.2, 20, 35.75100000000151, 46.627000000000336, -1], 'reward': 13.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee7370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 17.1, 21, 24.663512978419348, 38.17220596286005, -1], 'reward': -47.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ecb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.1, 6.1, 21, 0.8702605246779855, 54.886062598422605, -1], 'reward': -137.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ec760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 20.6, 22, 10.251582575374407, 27.792802237222368, -1], 'reward': 4.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f65e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 8.7, 22, 13.54699999999962, 22.049999999999926, -1], 'reward': -15.679999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ece80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 9.2, 22, 5.239999999999999, 48.620999999999874, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f62e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 34.7, 22, 33.68231297855769, 56.739372601450825, -1], 'reward': 26.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f67c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 15.9, 2, 11.632523252537972, 59.77998559941738, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f08190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 33.4, 5, 50.67528168256091, 29.174099737092188, -1], 'reward': 74.92000000000002}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2       , 41.7       ,  6.        ,  0.10651689, 20.249702  ,\n",
      "        40.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9     , 16.1     ,  6.      , 14.608178, 35.619225, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4      , 15.2      ,  7.       ,  4.6745405, 34.62463  ,\n",
      "        40.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     ,  7.3     ,  8.      , 10.578975, 46.317673, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-60.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.5  , 17.5  ,  8.   ,  8.501, 50.861, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.6     , 27.7     ,  8.      , 41.87622 , 51.126873, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 20.9  , 12.   , 13.213, 51.102, 38.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     , 25.2     , 16.      ,  9.300863, 24.401981, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.2  , 41.   , 16.   , 13.587, 36.148, 38.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     , 30.8     , 17.      ,  9.036209, 37.036423, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1     , 15.1     , 17.      , 52.392147, 48.495777, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     , 39.3     , 19.      ,  8.363896, 43.75378 , 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5      , 35.4      ,  0.       ,  6.0235844, 44.498127 ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.9  ,  9.   ,  8.   , 30.924, 60.   , 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-86.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.      , 40.6     ,  9.      ,  7.914279, 36.00341 , 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.4     , 14.6     , 10.      , 24.459951, 28.77652 , 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-132.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2      , 14.6      , 10.       ,  3.2862482, 38.384357 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7     , 16.7     , 10.      , 26.319511, 47.886757, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-60.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     , 28.5     , 10.      , 45.843983, 14.883336, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     ,  7.7     , 16.      , 33.585255, 18.585976, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-19.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.3  , 23.   , 17.   , 45.92 , 35.998, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.   , 29.3  , 18.   ,  4.137, 48.655, 31.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     , 13.6     , 10.      , 41.52517 , 19.403841, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9     , 17.7     , 13.      , 55.910465, 31.149345, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.5  , 24.9  , 19.   , 25.954, 25.353, 29.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6      , 18.2      , 19.       ,  0.6945296, 22.023184 ,\n",
      "        28.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.2  , 24.3  , 19.   ,  6.162, 39.925, 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2      ,  4.3      , 20.       ,  5.3045635, 55.844574 ,\n",
      "        27.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-123.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2      ,  8.1      , 20.       ,  7.6086297, 52.250175 ,\n",
      "        26.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.8  , 17.   , 20.   ,  7.206, 33.998, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3     ,  9.3     , 20.      , 11.195622, 37.90151 , 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     , 15.      , 20.      , 19.457293, 24.495348, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-7.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     , 21.9     , 20.      , 28.12943 , 30.720997, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-29.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.8  , 17.7  , 20.   ,  4.039, 24.648, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.9  , 17.1  , 22.   ,  5.581, 54.39 , 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     , 27.3     ,  0.      , 49.213768, 54.57038 , 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=32.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.2  , 28.2  ,  5.   , 15.559, 34.625, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.7     , 17.3     ,  6.      , 19.831282, 58.321285, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-31.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.5      ,  8.7      ,  7.       ,  3.5826063, 47.641674 ,\n",
      "        20.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.3  ,  6.   ,  7.   ,  9.284, 54.673, 20.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     ,  7.7     ,  7.      , 10.401077, 52.137367, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5     , 10.      ,  8.      , 14.516661, 52.803005, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 23.7     ,  8.      , 29.393238, 56.246094, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-18.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.1      , 21.3      ,  9.       ,  5.0109377, 59.521164 ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     ,  4.1     ,  9.      , 23.369211, 47.501534, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-79.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.       , 10.7      ,  9.       , 11.6073885, 53.137547 ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     , 26.4     ,  9.      , 28.002382, 43.65689 , 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-54.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1     , 12.8     , 11.      , 26.448107, 47.478363, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.3     , 13.2     , 11.      ,  9.207707, 51.692097, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.2  , 16.7  , 11.   ,  5.404, 36.565, 16.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.5     ,  5.7     , 11.      ,  2.154569, 39.496826, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.8      , 42.4      , 11.       ,  5.5974483, 17.549326 ,\n",
      "        16.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.2     , 19.1     , 11.      ,  7.74237 , 38.762005, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-110.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     , 23.5     , 12.      , 28.152962, 26.677677, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 18.3  , 12.   ,  2.164, 30.416, 15.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1     , 23.2     , 12.      , 26.858194, 42.079857, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=46.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.8  ,  5.5  , 12.   ,  7.494, 40.196, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.2  , 17.6  , 13.   ,  3.893, 44.932, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6      ,  6.3      , 13.       ,  6.5877857, 41.808636 ,\n",
      "        14.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2      ,  4.8      , 13.       ,  4.2325044, 33.549305 ,\n",
      "        14.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-122.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3    ,  8.8    , 13.     , 17.17878, 56.93423, 13.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.7     , 26.5     , 13.      , 30.827051, 42.30935 , 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=73.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     ,  7.2     , 13.      , 34.95875 , 51.248604, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.1  , 20.2  , 14.   ,  7.168, 52.086, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.   , 15.8  , 14.   ,  4.741, 58.524, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.8  , 18.6  , 15.   , 12.945, 55.527, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.      ,  4.4     , 16.      , 15.500231, 49.473064, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.9  , 28.2  , 17.   ,  3.429, 38.322, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.5  ,  8.9  , 17.   , 12.346, 49.95 , 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.8  , 30.3  , 17.   , 12.322, 32.917, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=84.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.4     ,  5.8     , 17.      ,  4.241666, 46.82405 , 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.      , 25.9     , 17.      , 30.719633, 32.258118, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=8.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.5       , 32.3       , 18.        ,  0.03446958, 16.680754  ,\n",
      "         9.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.7     , 32.2     , 19.      , 54.504345, 25.035645,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=37.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.9  , 21.1  , 13.   , 14.905, 19.561,  8.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-74.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.8     , 15.1     , 13.      , 11.645493, 48.86765 ,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  7.7     , 13.      ,  2.420435, 17.458433,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 23.2     , 13.      , 19.904148, 41.297436,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2     ,  6.5     , 13.      , 21.250113, 41.2091  ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8     , 35.1     , 13.      , 24.845509, 10.0765  ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=52.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[29.       ,  5.       , 14.       ,  8.0888405, 33.668167 ,\n",
      "         5.       ]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-181.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.      , 13.4     , 14.      , 18.582106, 36.552204,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-4.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.4    ,  5.8    , 14.     ,  8.06332, 56.95122,  3.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.9     , 15.2     , 14.      , 14.034532, 54.176655,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.2  , 13.   , 15.   , 10.27 , 45.289,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9     , 29.4     , 15.      , 40.571793,  9.718714,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.5  , 31.3  , 15.   , 20.952, 44.416,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=8.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.7     ,  3.7     , 15.      ,  8.621221, 59.849564,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     , 44.6     , 15.      , 35.255978, 14.647723,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1538.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8     , 20.3     , 17.      ,  6.169793, 31.528599,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-42.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6      ,  8.       , 17.       ,  2.4738646, 42.457966 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 29.3     , 17.      , 26.150805, 23.470503, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.7      ,  0.6      , 17.       ,  6.8517375, 34.48287  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-145.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 13.2     , 17.      , 17.331347, 35.692757, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7      ,  9.5      , 17.       ,  5.4999065, 39.976215 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.3    , 24.4    , 17.     , 12.9552 , 46.66553, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     ,  2.6     , 17.      , 12.929522, 58.17478 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     , 40.3     , 17.      , 15.562633, 20.4309  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.3  ,  9.9  , 17.   ,  5.75 , 33.212, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.3  , 19.8  , 17.   , 15.739, 52.12 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=40.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     , 22.5     , 17.      ,  5.923093, 32.37921 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.6     , 25.6     , 17.      , 18.425894, 53.203617, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=71.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.3     , 11.      , 18.      , 31.350609, 46.41653 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.7     ,  3.7     , 18.      , 21.394205, 48.688076, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5      , 12.9      , 18.       ,  5.4825335, 29.18069  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-84.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 29.4     , 18.      ,  9.673659, 15.462141, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.3  , 14.5  , 18.   ,  3.616, 47.984, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4      ,  6.2      , 18.       ,  0.5429981, 22.602137 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.3  , 12.   , 19.   ,  5.604, 42.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 24.8     , 19.      , 27.135126,  7.423777, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=41.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9     , 27.2     , 19.      , 38.62193 , 32.526226, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.7  , 16.5  , 20.   ,  8.118, 56.631, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.4  , 26.2  , 20.   ,  0.774, 40.493, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.6  , 16.2  , 20.   , 35.751, 46.627, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     , 17.1     , 21.      , 24.663513, 38.172207, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.1       ,  6.1       , 21.        ,  0.87026054, 54.886063  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-137.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 20.6     , 22.      , 10.251582, 27.792803, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=4.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.4  ,  8.7  , 22.   , 13.547, 22.05 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.6  ,  9.2  , 22.   ,  5.24 , 48.621, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.4     , 34.7     , 22.      , 33.682312, 56.739372, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 15.9     ,  2.      , 11.632524, 59.779987, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f652eee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 34.6, 2, 40.024787826465975, 19.048793830980145, 40], 'reward': 49.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 3.3, 7, 43.249946263675554, 21.66200751084424, 39], 'reward': -21.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.9, 21.8, 8, 26.61304022716918, 12.24472766042173, 38], 'reward': 50.04000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.7, 17.7, 8, 9.600091904012055, 29.22853197988893, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.4, 32.7, 8, 6.2391544345880305, 41.044004161456044, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 26.2, 8, 16.147000000001327, 12.042000000000083, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.0, 31.4, 9, 11.001000000001325, 41.27000000000002, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ecc70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 27.1, 9, 12.595000000001324, 45.19299999999999, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a8a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.4, 4.8, 10, 12.783711845500015, 2.0121023375534373, 37], 'reward': -102.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 16.6, 12, 14.497000000001705, 4.449999999999936, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d38b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.5, 11.0, 13, 6.331692533578526, 4.392608227159448, 36], 'reward': -36.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602a430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.5, 15.3, 13, 5.625000000003031, 28.882000000000087, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.4, 11.2, 14, 2.7656139877731363, 28.969535971170618, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2fd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 33.9, 14, 6.743000000003221, 33.81700000000005, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a2f880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.1, 13.1, 16, 3.964606458624173, 16.252205216325514, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62296a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 30.5, 16, 29.412752046415182, 30.36184138541968, 35], 'reward': 69.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f1b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 16.8, 17, 7.554000000002274, 24.984999999999864, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62297c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.9, 17.8, 17, 20.72133054876612, 16.969604065975243, 34], 'reward': 44.04000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.8, 40.5, 17, 45.02955323682449, 41.841277718130506, 33], 'reward': 15.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.0, 17.1, 19, 31.02500000000303, 48.06900000000034, 32], 'reward': 20.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6229b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 15.5, 19, 14.970586992809892, 59.15848111257537, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63152e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.0, 18.5, 19, 4.2998082506770725, 50.62779506014854, 31], 'reward': -76.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 14.4, 19, 1.937897531352645, 45.08205453318258, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62acee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.3, 9.2, 19, 14.578302449660747, 54.119863255911014, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f9cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.7, 8.4, 19, 10.684086951205217, 50.605422472607216, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [29.9, 25.5, 19, 3.850980273802019, 43.56873968278472, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.9, 24.7, 19, 1.8069997798133635, 52.3187538622671, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bc670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.4, 22.8, 19, 4.058230478217324, 15.692403001503234, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 19.9, 19, 8.955354478838174, 10.703147978750863, 30], 'reward': -75.03999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bcc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.9, 35.9, 20, 17.570539757736515, 51.19836330852664, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 9.5, 20, 13.993000000000947, 26.45700000000013, 29], 'reward': -17.88000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d5a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.0, 22.1, 20, 2.7420000000007576, 48.449999999999804, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d5ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.5, 6.3, 20, 5.022705489636889, 39.74307881140388, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d54f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.7, 14.9, 20, 13.576514803172342, 21.56002743328986, 28], 'reward': -25.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e21c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.8, 25.1, 20, 31.53593344747955, 36.94094264990002, 27], 'reward': 54.47999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d5d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 24.3, 20, 1.5806690097163951, 53.113867530136844, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 20.9, 20, 1.094949662786636, 43.58732068158452, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e19a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.5, 23.0, 20, 4.906000000000379, 30.547000000000402, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb0df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.4, 25.0, 21, 51.72411532060491, 50.46427608799121, 26], 'reward': 43.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 35.4, 0, 6.0235842463972205, 44.49812699024557, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.5, 7.3, 8, 39.42600000000095, 53.38000000000014, 25], 'reward': -48.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 40.6, 9, 7.9142791835001205, 36.00341194861859, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.8, 24.5, 13, 34.8286356233135, 32.787361966242656, 24], 'reward': -15.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edc7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 14.1, 13, 23.090475462272448, 19.295585809967037, 23], 'reward': -15.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edcc10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 29.7, 13, 26.64855745012513, 59.735621008394574, 22], 'reward': -19.87999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edc220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.6, 1.5, 13, 6.625433684893724, 44.48961032811887, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.7, 0.9, 13, 3.5833021692607856, 49.29371225847313, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.1, 12.5, 13, 32.498866729167915, 55.96661022235909, 21], 'reward': -1.480000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eded30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.0, 12.8, 14, 35.33925152017001, 50.74134068045956, 20], 'reward': -61.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.8, 15.0, 14, 3.8350000000003788, 35.624999999999595, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6ddf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 15.1, 14, 4.356000000002842, 45.64699999999986, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 23.5, 15, 2.7424026607556016, 34.87934493211107, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.6, 14.9, 16, 18.754930891611536, 31.43732250513083, 19], 'reward': -31.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.0, 2.4, 16, 14.45781019462693, 47.677999274644286, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.4, 13.1, 16, 23.290246979136185, 28.103767745576537, 18], 'reward': -15.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f03a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.0, 10.3, 16, 4.7480000000028415, 33.360000000000326, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 15.4, 17, 32.65642875785129, 25.47525691542302, 17], 'reward': 8.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 16.9, 17, 8.935000000001326, 49.27699999999974, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 27.2, 17, 22.430877020251184, 52.61084872532363, 16], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.9, 9.0, 17, 7.293613111528451, 26.400147196320738, 16], 'reward': -120.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 12.0, 17, 9.36992820540068, 11.864615770138803, 15], 'reward': -1.0400000000000063}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67403d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 15.5, 17, 10.440552440145172, 36.10584538092067, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ae820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 13.6, 17, 4.692000000002842, 35.981000000000186, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 36.3, 17, 37.3908362114192, 29.394048254498657, 14], 'reward': 93.72000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630fdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 29.3, 18, 4.136999999999432, 48.655000000000044, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67400a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.7, 16.1, 18, 31.388763045215832, 47.087769351070044, 13], 'reward': -89.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.1, 5.5, 19, 14.168794456506745, 49.3327091621981, 12], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f10d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.7, 13.4, 19, 7.131999999999621, 50.45699999999962, 12], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.9, 18.9, 19, 12.84834300780047, 24.080118723296874, 12], 'reward': -81.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66aeaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.0, 3.7, 19, 1.8259629450512147, 26.137452561736836, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.3, 27.9, 19, 35.93784111002127, 39.669340183662456, 11], 'reward': -7.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629de50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.0, 23.1, 20, 44.82535246933057, 57.903204127897126, 10], 'reward': -75.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.7, 23.7, 22, 9.386000000004168, 40.59499999999989, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.6, 13.1, 6, 24.558227082389884, 38.04109698783468, 9], 'reward': -84.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623de20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.3, 12.9, 7, 6.011312522387787, 11.026937890113011, 8], 'reward': -96.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623dbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.7, 5.1, 7, 7.578011930638913, 17.747098659024367, 7], 'reward': -42.83999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623dca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 7.3, 7, 4.4160000000028425, 37.28199999999984, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 3.4, 7, 12.615807233790575, 26.182794705685172, 6], 'reward': -32.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.1, 6.2, 7, 6.7001824641636, 36.84890889344565, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eebe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.8, 29.5, 7, 27.719895664061504, 13.325996824041699, 5], 'reward': 14.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eaf70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.0, 5.7, 8, 9.459000000004169, 34.5950000000001, 4], 'reward': -138.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635eb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 22.5, 8, 21.417295304884085, 48.02467260461792, 3], 'reward': 39.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635ec10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.5, 4.9, 8, 4.128903054554212, 53.454661442371375, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.3, 3.6, 9, 2.320847698105544, 32.86548016145754, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 41.7, 9, 44.333484600610895, 40.159308091391836, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 5.0, 9, 1.4494150545804763, 33.44615338739398, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.8, 5.0, 10, 1.7355825125689763, 38.473137658948566, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.7, 6.7, 10, 8.733771671669347, 49.470447225575185, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 19.1, 10, 26.0622174770358, 34.85814464337667, 2], 'reward': 10.800000000000011}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620ba60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.7, 14.6, 10, 0.2783423785647763, 32.82687107215835, 1], 'reward': 1371.96}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 10.5, 11, 15.40299175318462, 33.88347904879163, 0], 'reward': 0.9599999999999937}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.0, 10.9, 11, 8.244000000003599, 34.01400000000021, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621ccd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 16.0, 11, 7.770000000001705, 15.578000000000321, -1], 'reward': 0.8800000000000239}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620bd60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.3, 17.4, 11, 2.1470000000011367, 28.865000000000382, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.2, 22.7, 11, 1.5813117226192404, 15.03465216307032, -1], 'reward': -78.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 7.4, 12, 6.8410000000036, 15.296999999999645, -1], 'reward': -28.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.1, 22.3, 12, 29.48379319133008, 20.674651648543524, -1], 'reward': 63.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621cb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.7, 7.5, 12, 6.6179948493360925, 29.11081131601658, -1], 'reward': -150.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 16.4, 12, 12.037799412934312, 48.53361591117043, -1], 'reward': 21.200000000000017}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 26.7, 12, 31.670454096137625, 19.789251412769968, -1], 'reward': 24.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [28.6, 17.9, 12, 27.366323231300044, 35.21906905959893, -1], 'reward': -137.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62223d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.7, 27.1, 13, 28.108627049494277, 24.611907218331726, -1], 'reward': -40.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 24.4, 13, 32.844598003635426, 1.7932293030443986, -1], 'reward': 52.920000000000016}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 22.3, 13, 18.073000000000757, 20.80599999999989, -1], 'reward': 33.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 13.2, 13, 12.500830751324191, 25.01790900117757, -1], 'reward': -5.359999999999985}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.5, 19.1, 13, 4.0486389635973925, 45.19640189829088, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.2, 16.1, 13, 1.3682326631920423, 23.514248887036633, -1], 'reward': -72.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f59a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 18.4, 14, 1.898495969665401, 48.123642855296666, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.2, 17.3, 14, 23.3669743286032, 20.098536843583297, -1], 'reward': 20.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 9.2, 14, 31.786240699714426, 30.047031041657924, -1], 'reward': 0.8800000000000097}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 14.7, 14, 10.352000000000567, 47.43599999999979, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdcd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.8, 36.0, 14, 53.64400000000284, 53.02099999999992, -1], 'reward': 7.760000000000048}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 9.4, 12, 51.77642918194883, 36.39430691058346, -1], 'reward': -23.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ac040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 45.0, 19, 3.0190000000056845, 47.76900000000032, -1], 'reward': 27.039999999999964}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.3, 6.8, 19, 4.949308326255002, 52.73181313905326, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.1, 15.7, 19, 0.24998997702861203, 31.80100361556505, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cbb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.4, 26.6, 19, 27.798289906202626, 37.414983000769425, -1], 'reward': 75.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d35b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.7, 16.6, 20, 48.801054794892984, 30.983125503269527, -1], 'reward': -19.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dce50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.7, 22.5, 21, 12.766216622181403, 47.88815097525201, -1], 'reward': -68.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 5.5, 21, 26.62599999999981, 58.7269999999996, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.4, 19.3, 22, 27.64035276988735, 47.02648899113803, -1], 'reward': 31.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 6.3, 22, 5.549149530662853, 38.16101129515673, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 14.3, 22, 4.754000000005684, 50.39799999999989, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 6.7, 0, 8.236661946709049, 48.00352408374601, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606b9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.5, 6.3, 7, 17.776000000000757, 49.36099999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6073ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 14.3, 7, 41.208349920408246, 39.4343448296598, -1], 'reward': 12.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 16.7, 7, 44.723041088035785, 59.396577109162585, -1], 'reward': 28.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6073400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.0, 21.0, 8, 5.8940000000030315, 60.0, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607cf70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.9, 10.5, 9, 26.57013277065888, 58.07638633024534, -1], 'reward': -67.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.9, 13.8, 9, 33.2780974960658, 42.97493860541236, -1], 'reward': 24.440000000000012}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627e070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.5, 14.5, 10, 5.588000000002653, 49.96399999999985, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 20.4, 10, 18.19744110357692, 28.505435088002137, -1], 'reward': 32.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6087130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 10.2, 11, 15.160833631819731, 25.97223623973742, -1], 'reward': -34.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626c4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.3, 17.4, 11, 1.2519700110269292, 59.74514081629084, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626ce20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 9.7, 11, 14.422312212987594, 40.97017571521644, -1], 'reward': -56.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60878e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 8.0, 11, 17.942515470574257, 40.41099513822532, -1], 'reward': -10.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626cc70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 9.0, 11, 10.799000000004547, 49.99199999999974, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 31.5, 12, 45.852586798510245, 53.92820668689126, -1], 'reward': 53.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626cb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.4, 6.1, 12, 31.464000000003598, 51.06599999999978, -1], 'reward': -51.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f609f700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.8, 8.0, 13, 3.153656236232213, 54.85027510142165, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62776a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.0, 20.6, 13, 35.08033439063256, 50.208317825144555, -1], 'reward': -56.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 12.5, 13, 7.016960711096596, 47.86463917731546, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.9, 14.6, 13, 3.9480000000026525, 56.012999999999934, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 6.1, 14, 14.981630050397914, 48.14149841170331, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 24.5, 15, 43.87375709873955, 43.18520179947449, -1], 'reward': -18.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 10.9, 16, 15.876768390761802, 57.19585182299929, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 13.7, 17, 7.790685851395308, 43.67145907666459, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f601ad30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 16.2, 19, 49.913749712007146, 25.023808843870498, -1], 'reward': 2.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 30.9, 10, 3.564595729019743, 8.682127137420718, -1], 'reward': -45.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd86a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 15.3, 12, 6.271000000004168, 22.038000000000288, -1], 'reward': -10.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 29.4, 12, 36.23434714918369, 24.201442894774498, -1], 'reward': 56.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.6, 15.5, 12, 8.501903716054036, 46.497189814068705, -1], 'reward': -104.07999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd81c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.6, 4.6, 12, 8.927583053658239, 37.58655240268369, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd03d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.1, 5.3, 12, 8.274000000004737, 45.45299999999982, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd07f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.2, 13.0, 12, 1.808872331919209, 58.76142542461555, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.6, 23.4, 12, 42.51450707079648, 50.659109849522764, -1], 'reward': -72.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.2, 2.0, 13, 22.440135416032255, 49.24714636857031, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f823a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.7, 11.6, 13, 29.432389325975016, 28.439351302544218, -1], 'reward': -103.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.9, 6.5, 14, 21.250112913374828, 41.2090987834204, -1], 'reward': -107.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624f820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 17.5, 14, 3.549942516756534, 50.892642220471785, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f825b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 12.4, 14, 13.818416267239593, 54.97051676377725, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.2, 9.0, 14, 6.772177135454343, 39.91555142670234, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.4, 5.8, 14, 8.063319851712262, 56.95122125145516, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 10.7, 14, 15.843666786395051, 54.253664262374066, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.5, 15.2, 14, 14.034531493416184, 54.17665326845435, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.3, 13.0, 14, 10.270000000002083, 45.28899999999986, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63586d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 15.8, 15, 14.915684637423707, 59.609642768782344, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 33.9, 15, 43.472372928403864, 44.75005721357756, -1], 'reward': 24.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63582e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 2.6, 15, 23.742004685542813, 45.311062014599976, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 13.4, 16, 8.603000000002085, 54.96100000000011, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.0, 9.5, 16, 19.56461448520222, 45.14454948644428, -1], 'reward': -132.79999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63616a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 17.9, 16, 2.3794681787431653, 35.21758474801946, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e56a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 12.5, 16, 13.543265903725533, 49.41079485571808, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 25.9, 16, 48.58220766383518, 58.40445497955344, -1], 'reward': -20.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 7.3, 19, 40.03319208682321, 50.818619102795054, -1], 'reward': -12.679999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f5a340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 17.2, 19, 14.14500000000322, 42.29600000000007, -1], 'reward': -23.159999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f461f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.1, 24.8, 19, 19.380956414303412, 35.232715334433294, -1], 'reward': -30.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f460d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.7, 10.9, 19, 8.07100000000322, 39.32099999999981, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.6, 15.2, 19, 3.4081635042213554, 32.35191095069776, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 16.2, 19, 2.317813108175129, 59.648087591901664, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.8, 27.7, 19, 20.75972092893888, 23.277011302882073, -1], 'reward': -39.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.3, 11.1, 19, 0.13200539267335465, 42.23122722398629, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef41c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 38.1, 19, 41.860096458390906, 14.723173892622007, -1], 'reward': 15.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.9, 8.7, 20, 27.391550054368437, 11.31737437409576, -1], 'reward': -100.67999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee75e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.5, 6.5, 20, 13.860042335114366, 14.704056204830627, -1], 'reward': -111.79999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.6, 12.9, 23, 18.504900833218738, 17.581982435473044, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f08bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 27.2, 6, 7.086000000003789, 30.877999999999982, -1], 'reward': -15.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0ce50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 9.1, 7, 7.9293224438172825, 16.037546432062573, -1], 'reward': -17.799999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.7, 11.5, 8, 17.380502415867994, 32.69645937167225, -1], 'reward': -56.359999999999985}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f20700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 16.1, 8, 18.34081107184781, 50.2426953840061, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6aa2070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.9, 14.1, 9, 7.898000000004169, 32.849000000000224, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6aa2460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 11.2, 9, 9.247438753262472, 30.273868102821424, -1], 'reward': -69.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a977c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.9, 3.2, 9, 13.127000000000189, 43.849000000000146, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 6.6, 9, 2.7655872935902246, 55.58301724785126, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a918e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 51.1, 10, 54.413838203578294, 6.635504755444945, -1], 'reward': 148.56}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 34.6     ,  2.      , 40.024788, 19.048794, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=49.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     ,  3.3     ,  7.      , 43.249947, 21.662008, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9     , 21.8     ,  8.      , 26.61304 , 12.244728, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.7     , 17.7     ,  8.      ,  9.600092, 29.228533, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4      , 32.7      ,  8.       ,  6.2391543, 41.044003 ,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.1  , 26.2  ,  8.   , 16.147, 12.042, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.   , 31.4  ,  9.   , 11.001, 41.27 , 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.6  , 27.1  ,  9.   , 12.595, 45.193, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4      ,  4.8      , 10.       , 12.783711 ,  2.0121024,\n",
      "        37.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-102.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.   , 16.6  , 12.   , 14.497,  4.45 , 36.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.5      , 11.       , 13.       ,  6.3316927,  4.392608 ,\n",
      "        36.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-36.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.5  , 15.3  , 13.   ,  5.625, 28.882, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.4     , 11.2     , 14.      ,  2.765614, 28.969536, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.5  , 33.9  , 14.   ,  6.743, 33.817, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1      , 13.1      , 16.       ,  3.9646065, 16.252205 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     , 30.5     , 16.      , 29.412752, 30.361841, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=69.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.   , 16.8  , 17.   ,  7.554, 24.985, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9     , 17.8     , 17.      , 20.72133 , 16.969604, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=44.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.8     , 40.5     , 17.      , 45.029552, 41.841278, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.   , 17.1  , 19.   , 31.025, 48.069, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=20.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8     , 15.5     , 19.      , 14.970587, 59.15848 , 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.      , 18.5     , 19.      ,  4.299808, 50.627796, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-76.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6      , 14.4      , 19.       ,  1.9378976, 45.082054 ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.3     ,  9.2     , 19.      , 14.578302, 54.11986 , 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.7     ,  8.4     , 19.      , 10.684087, 50.605423, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[29.9      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9      , 24.7      , 19.       ,  1.8069998, 52.318752 ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4      , 22.8      , 19.       ,  4.0582304, 15.692403 ,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     , 19.9     , 19.      ,  8.955355, 10.703148, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9     , 35.9     , 20.      , 17.57054 , 51.198364, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.1  ,  9.5  , 20.   , 13.993, 26.457, 29.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.   , 22.1  , 20.   ,  2.742, 48.45 , 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.5      ,  6.3      , 20.       ,  5.0227056, 39.74308  ,\n",
      "        28.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.7     , 14.9     , 20.      , 13.576515, 21.560028, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.8     , 25.1     , 20.      , 31.535933, 36.94094 , 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=54.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     , 24.3     , 20.      ,  1.580669, 53.11387 , 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        26.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[26.5  , 23.   , 20.   ,  4.906, 30.547, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4     , 25.      , 21.      , 51.724113, 50.464275, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=43.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.       , 35.4      ,  0.       ,  6.0235844, 44.498127 ,\n",
      "        25.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.5  ,  7.3  ,  8.   , 39.426, 53.38 , 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-48.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     , 40.6     ,  9.      ,  7.914279, 36.00341 , 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.8     , 24.5     , 13.      , 34.828636, 32.78736 , 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9     , 14.1     , 13.      , 23.090475, 19.295586, 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9     , 29.7     , 13.      , 26.648558, 59.735622, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-19.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.6     ,  1.5     , 13.      ,  6.625434, 44.48961 , 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.7      ,  0.9      , 13.       ,  3.5833023, 49.293713 ,\n",
      "        21.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     , 12.5     , 13.      , 32.498867, 55.96661 , 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      , 12.8     , 14.      , 35.339252, 50.74134 , 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-61.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[26.8  , 15.   , 14.   ,  3.835, 35.625, 19.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.7  , 15.1  , 14.   ,  4.356, 45.647, 19.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4      , 23.5      , 15.       ,  2.7424026, 34.879345 ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6     , 14.9     , 16.      , 18.75493 , 31.437323, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-31.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.      ,  2.4     , 16.      , 14.45781 , 47.677998, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     , 13.1     , 16.      , 23.290247, 28.103767, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.   , 10.3  , 16.   ,  4.748, 33.36 , 17.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      , 15.4     , 17.      , 32.65643 , 25.475256, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=8.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.7  , 16.9  , 17.   ,  8.935, 49.277, 16.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 27.2     , 17.      , 22.430878, 52.610847, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     ,  9.      , 17.      ,  7.293613, 26.400146, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-120.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 12.      , 17.      ,  9.369928, 11.864615, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     , 15.5     , 17.      , 10.440553, 36.105846, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.   , 13.6  , 17.   ,  4.692, 35.981, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3     , 36.3     , 17.      , 37.390835, 29.394049, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=93.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 29.3  , 18.   ,  4.137, 48.655, 13.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     , 16.1     , 18.      , 31.388763, 47.08777 , 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-89.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.1     ,  5.5     , 19.      , 14.168795, 49.33271 , 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.7  , 13.4  , 19.   ,  7.132, 50.457, 12.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.9     , 18.9     , 19.      , 12.848343, 24.080118, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-81.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.       ,  3.7      , 19.       ,  1.8259629, 26.137453 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3     , 27.9     , 19.      , 35.93784 , 39.669342, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-7.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      , 23.1     , 20.      , 44.82535 , 57.903206, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.7  , 23.7  , 22.   ,  9.386, 40.595,  9.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.6     , 13.1     ,  6.      , 24.558228, 38.041096,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-84.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.3      , 12.9      ,  7.       ,  6.0113125, 11.0269375,\n",
      "         8.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-96.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     ,  5.1     ,  7.      ,  7.578012, 17.747099,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-42.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.   ,  7.3  ,  7.   ,  4.416, 37.282,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     ,  3.4     ,  7.      , 12.615808, 26.182795,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1      ,  6.2      ,  7.       ,  6.7001824, 36.848907 ,\n",
      "         5.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.8     , 29.5     ,  7.      , 27.719896, 13.325996,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.   ,  5.7  ,  8.   ,  9.459, 34.595,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-138.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     , 22.5     ,  8.      , 21.417295, 48.024673,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=39.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5     ,  4.9     ,  8.      ,  4.128903, 53.454662,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.3      ,  3.6      ,  9.       ,  2.3208477, 32.86548  ,\n",
      "         2.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.      , 41.7     ,  9.      , 44.333485, 40.15931 ,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.7      ,  5.       ,  9.       ,  1.4494151, 33.44615  ,\n",
      "         2.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.8      ,  5.       , 10.       ,  1.7355825, 38.473137 ,\n",
      "         2.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     ,  6.7     , 10.      ,  8.733771, 49.470448,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.4     , 19.1     , 10.      , 26.062218, 34.858143,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.7       , 14.6       , 10.        ,  0.27834237, 32.82687   ,\n",
      "         1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1371.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     , 10.5     , 11.      , 15.402991, 33.88348 ,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.   , 10.9  , 11.   ,  8.244, 34.014, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.4  , 16.   , 11.   ,  7.77 , 15.578, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.3  , 17.4  , 11.   ,  2.147, 28.865, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.2      , 22.7      , 11.       ,  1.5813117, 15.034652 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-78.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.7  ,  7.4  , 12.   ,  6.841, 15.297, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.1     , 22.3     , 12.      , 29.483793, 20.674652, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=63.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.7     ,  7.5     , 12.      ,  6.617995, 29.110811, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-150.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 16.4     , 12.      , 12.0378  , 48.533615, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=21.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 26.7     , 12.      , 31.670454, 19.789251, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[28.6     , 17.9     , 12.      , 27.366323, 35.21907 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-137.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.7     , 27.1     , 13.      , 28.108627, 24.611908, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-40.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7      , 24.4      , 13.       , 32.844597 ,  1.7932293,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=52.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.5  , 22.3  , 13.   , 18.073, 20.806, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=33.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.      , 13.2     , 13.      , 12.500831, 25.017908, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.5     , 19.1     , 13.      ,  4.048639, 45.196404, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.2      , 16.1      , 13.       ,  1.3682326, 23.51425  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-72.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1      , 18.4      , 14.       ,  1.8984959, 48.123642 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.2     , 17.3     , 14.      , 23.366974, 20.098537, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=20.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     ,  9.2     , 14.      , 31.786242, 30.047031, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.   , 14.7  , 14.   , 10.352, 47.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.8  , 36.   , 14.   , 53.644, 53.021, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.9     ,  9.4     , 12.      , 51.77643 , 36.394306, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.2  , 45.   , 19.   ,  3.019, 47.769, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=27.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3      ,  6.8      , 19.       ,  4.9493084, 52.73181  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1       , 15.7       , 19.        ,  0.24998997, 31.801004  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.4    , 26.6    , 19.     , 27.79829, 37.41498, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=75.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.7     , 16.6     , 20.      , 48.801056, 30.983126, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-19.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     , 22.5     , 21.      , 12.766216, 47.88815 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-68.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.6  ,  5.5  , 21.   , 26.626, 58.727, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     , 19.3     , 22.      , 27.640352, 47.02649 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=31.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7      ,  6.3      , 22.       ,  5.5491495, 38.16101  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.7  , 14.3  , 22.   ,  4.754, 50.398, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      ,  6.7     ,  0.      ,  8.236662, 48.003525, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.5  ,  6.3  ,  7.   , 17.776, 49.361, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 14.3     ,  7.      , 41.20835 , 39.434345, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     , 16.7     ,  7.      , 44.72304 , 59.396576, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=28.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.   , 21.   ,  8.   ,  5.894, 60.   , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.9     , 10.5     ,  9.      , 26.570133, 58.076385, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9     , 13.8     ,  9.      , 33.2781  , 42.974937, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.5  , 14.5  , 10.   ,  5.588, 49.964, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     , 20.4     , 10.      , 18.197441, 28.505436, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=32.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     , 10.2     , 11.      , 15.160833, 25.972237, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-34.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.3    , 17.4    , 11.     ,  1.25197, 59.74514, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     ,  9.7     , 11.      , 14.422312, 40.970177, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     ,  8.      , 11.      , 17.942516, 40.410995, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.5  ,  9.   , 11.   , 10.799, 49.992, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9     , 31.5     , 12.      , 45.85259 , 53.928207, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.4  ,  6.1  , 12.   , 31.464, 51.066, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.8      ,  8.       , 13.       ,  3.1536562, 54.850277 ,\n",
      "        -1.       ]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.      , 20.6     , 13.      , 35.080334, 50.208317, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1      , 12.5      , 13.       ,  7.0169606, 47.86464  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.9  , 14.6  , 13.   ,  3.948, 56.013, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.     ,  6.1    , 14.     , 14.98163, 48.1415 , -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.2     , 24.5     , 15.      , 43.873756, 43.185204, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-18.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.6     , 10.9     , 16.      , 15.876768, 57.19585 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.7      , 13.7      , 17.       ,  7.7906857, 43.67146  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2    , 16.2    , 19.     , 49.91375, 25.02381, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=2.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3      , 30.9      , 10.       ,  3.5645957,  8.682127 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.8  , 15.3  , 12.   ,  6.271, 22.038, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5     , 29.4     , 12.      , 36.23435 , 24.201443, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=56.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.6     , 15.5     , 12.      ,  8.501904, 46.49719 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-104.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     ,  4.6     , 12.      ,  8.927583, 37.58655 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.1  ,  5.3  , 12.   ,  8.274, 45.453, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2      , 13.       , 12.       ,  1.8088723, 58.761425 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.6     , 23.4     , 12.      , 42.514507, 50.65911 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-72.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.2     ,  2.      , 13.      , 22.440136, 49.247147, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     , 11.6     , 13.      , 29.43239 , 28.439352, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-103.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.9     ,  6.5     , 14.      , 21.250113, 41.2091  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-107.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4      , 17.5      , 14.       ,  3.5499425, 50.892643 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 12.4     , 14.      , 13.818417, 54.970516, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.2     ,  9.      , 14.      ,  6.772177, 39.91555 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4    ,  5.8    , 14.     ,  8.06332, 56.95122, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.      , 10.7     , 14.      , 15.843667, 54.253666, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.5     , 15.2     , 14.      , 14.034532, 54.176655, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.3  , 13.   , 14.   , 10.27 , 45.289, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 15.8     , 15.      , 14.915685, 59.609642, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.4     , 33.9     , 15.      , 43.472374, 44.750057, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     ,  2.6     , 15.      , 23.742004, 45.31106 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.   , 13.4  , 16.   ,  8.603, 54.961, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.      ,  9.5     , 16.      , 19.564615, 45.14455 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-132.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3      , 17.9      , 16.       ,  2.3794682, 35.217587 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     , 12.5     , 16.      , 13.543266, 49.410793, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2     , 25.9     , 16.      , 48.582207, 58.404453, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3    ,  7.3    , 19.     , 40.03319, 50.81862, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.5  , 17.2  , 19.   , 14.145, 42.296, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1     , 24.8     , 19.      , 19.380957, 35.232716, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.7  , 10.9  , 19.   ,  8.071, 39.321, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.6      , 15.2      , 19.       ,  3.4081635, 32.35191  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1      , 16.2      , 19.       ,  2.3178132, 59.648087 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.8     , 27.7     , 19.      , 20.759722, 23.277012, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3      , 11.1      , 19.       ,  0.1320054, 42.231228 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 38.1     , 19.      , 41.860096, 14.723174, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.9     ,  8.7     , 20.      , 27.39155 , 11.317374, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-100.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5     ,  6.5     , 20.      , 13.860043, 14.704056, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-111.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 12.9     , 23.      , 18.5049  , 17.581982, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.1  , 27.2  ,  6.   ,  7.086, 30.878, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9      ,  9.1      ,  7.       ,  7.9293222, 16.037546 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7     , 11.5     ,  8.      , 17.380503, 32.69646 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      , 16.1     ,  8.      , 18.34081 , 50.242695, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.9  , 14.1  ,  9.   ,  7.898, 32.849, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 11.2     ,  9.      ,  9.247438, 30.273869, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.9  ,  3.2  ,  9.   , 13.127, 43.849, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1      ,  6.6      ,  9.       ,  2.7655873, 55.583015 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f651d280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.2, 11.8, 0, 47.78251671426654, 35.79709224809472, 40], 'reward': -38.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 41.7, 6, 0.10651689007654852, 20.249702331225194, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653b8b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.8, 27.8, 6, 15.539000000002083, 47.317999999999806, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.1, 35.6, 7, 16.7551409431428, 28.13532297868954, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d02b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.4, 26.9, 8, 25.091724168653787, 21.49005843914733, 39], 'reward': 83.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c71f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 6.2, 8, 36.57297176742715, 26.358913361149234, 38], 'reward': -96.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.7, 26.2, 8, 16.147000000001327, 12.042000000000083, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 25.6, 9, 17.020000000001325, 48.72499999999986, 37], 'reward': 53.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630de80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 26.5, 9, 39.843348468282336, 48.75617117708643, 36], 'reward': 56.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 20.9, 12, 13.213000000001704, 51.10199999999982, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 35.2, 16, 16.166000000002462, 17.96500000000012, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 25.2, 16, 18.612000000000187, 56.221000000000274, 35], 'reward': -26.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.0, 18.1, 16, 10.359035249533493, 53.89971316739471, 34], 'reward': -57.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62293a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.6, 7.5, 16, 12.019067414042915, 41.42891401260987, 33], 'reward': -41.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.5, 25.5, 16, 29.974440933711143, 57.15824032334662, 32], 'reward': 30.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.6, 30.8, 17, 9.036209088721225, 37.03642350893827, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f1280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.3, 41.0, 17, 13.58700000000019, 36.147999999999875, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f1d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.4, 32.4, 18, 31.092000000002272, 19.717000000000187, 31], 'reward': 12.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f9d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 26.8, 18, 13.805930385081641, 50.100141967298384, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.0, 15.6, 18, 24.246215075283658, 31.772153294492927, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f9940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 28.8, 19, 7.215087962531339, 7.912185538009943, 30], 'reward': 69.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d50d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 35.9, 20, 17.570539757736515, 51.19836330852664, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bc490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 33.1, 21, 35.26993762916415, 13.47836515782541, 29], 'reward': 67.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebd310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.1, 6.2, 8, 27.926026213598828, 13.709016300840652, 28], 'reward': -55.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.7, 1.0, 8, 18.956109615061017, 25.246883895740453, 27], 'reward': -89.96}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebddf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 29.5, 8, 22.506000000001134, 48.045999999999694, 26], 'reward': 10.080000000000041}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebdc10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.3, 15.8, 8, 8.152000000000568, 58.751999999999704, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.5, 2.9, 9, 14.563365464546159, 42.924822379194055, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec29d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.7, 21.3, 9, 40.762246991000495, 51.954670389569735, 25], 'reward': 22.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eccc10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.4, 40.6, 9, 7.9142791835001205, 36.00341194861859, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.5, 23.5, 15, 2.7424026607556016, 34.87934493211107, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.1, 27.9, 16, 8.639843440076842, 48.57523737894558, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 29.3, 18, 4.136999999999432, 48.655000000000044, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ec5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.3, 22.6, 20, 7.919000000004168, 45.45400000000002, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.7, 23.7, 22, 9.386000000004168, 40.59499999999989, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 24.6, 2, 7.900999999999241, 42.098, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.0, 7.8, 8, 36.71028163884703, 44.11108229649774, 24], 'reward': 11.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.5, 10.9, 11, 8.244000000003599, 34.01400000000021, 23], 'reward': -111.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.0, 22.7, 11, 1.5813117226192404, 15.03465216307032, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620bfa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.6, 25.6, 11, 33.94565264468089, 28.981230252866336, 22], 'reward': 71.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.6, 22.2, 12, 10.349000000004168, 45.487000000000045, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fd0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.4, 14.7, 14, 10.352000000000567, 47.43599999999979, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.8, 24.7, 15, 6.222000000003032, 9.88600000000022, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.7, 2.3, 15, 24.13197886022159, 18.231425477550648, 21], 'reward': -79.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fd970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.2, 13.9, 15, 4.226430428000665, 17.636502491699108, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.6, 18.5, 16, 16.84627396998851, 44.0645892064736, 20], 'reward': -80.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61af5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.5, 24.0, 16, 7.980113688978102, 45.04592503046388, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61afa60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.3, 45.1, 16, 49.17440261160527, 41.89585384282417, 19], 'reward': 53.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c92b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.7, 13.3, 17, 17.275635712354823, 50.446336010495514, 18], 'reward': -91.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63045b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 27.8, 18, 14.448000000002084, 16.808000000000387, 17], 'reward': 48.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 24.5, 18, 20.699566217563913, 39.031028392724664, 16], 'reward': 45.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.1, 14.4, 18, 5.1060000000036, 48.91499999999956, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63049a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 9.1, 18, 11.895677774176242, 52.45351050285664, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.3, 8.6, 18, 10.957722521599575, 59.27939963324557, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.6, 38.8, 18, 16.851988408155982, 11.266445488673398, 15], 'reward': 4.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.7, 1.7, 18, 4.910000000001327, 29.41799999999991, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.8, 11.5, 18, 2.8550000000009472, 32.046999999999976, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 4.2, 19, 6.2720000000036, 11.774000000000314, 14], 'reward': -31.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631bd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.3, 22.2, 19, 4.342945186992238, 55.16373699565099, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.8, 24.6, 19, 19.52238747964391, 43.99491463026988, 13], 'reward': 5.279999999999973}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.1, 13.1, 19, 17.72670655787586, 45.650577034458784, 12], 'reward': -53.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631e490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.6, 4.3, 19, 5.304563635192457, 55.84457412557084, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.1, 18.2, 19, 0.6945295881499121, 22.02318376009886, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.4, 17.0, 19, 7.2060000000036, 33.99799999999995, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d8b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 24.3, 19, 6.162000000003031, 39.92499999999976, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631b250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.9, 8.1, 20, 7.608629745722144, 52.250173619572976, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.6, 11.8, 20, 22.86636783839674, 36.68091133559339, 11], 'reward': 26.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617aa00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.3, 35.3, 20, 23.281996831429666, 2.890688358264171, 10], 'reward': 83.72000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.9, 13.8, 21, 9.089584833132701, 32.96275865778156, 9], 'reward': -131.96000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617ad60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 13.0, 22, 17.575666416108447, 44.29708559561409, 8], 'reward': 3.519999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.9, 21.6, 22, 35.7530199120077, 37.8795817753075, 7], 'reward': 49.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 17.1, 22, 5.581000000001895, 54.38999999999993, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 23.8, 6, 6.362999999998105, 18.739000000000054, 6], 'reward': -20.399999999999977}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.1, 2.0, 7, 5.685000000003599, 21.177999999999674, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.8, 6.4, 7, 2.2750905287635375, 22.59399606045527, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 4.3, 8, 6.025610503158832, 14.04301000369016, 5], 'reward': -46.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618ca60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.3, 4.1, 8, 20.664224461505786, 17.60986410293443, 4], 'reward': -84.11999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69aeca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.3, 5.2, 9, 3.292191453040761, 38.48657322540579, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.8, 7.7, 10, 2.4760000000056843, 38.546999999999635, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [27.6, 7.1, 10, 7.389800479710109, 44.43036956959282, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 13.2, 11, 9.207707110431326, 51.692095870232805, 3], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612df40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.1, 4.6, 11, 12.40052270342308, 32.93502040161102, 3], 'reward': -81.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.8, 18.3, 12, 2.1639999999996213, 30.41600000000012, 2], 'reward': -48.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.1, 5.5, 12, 7.494000000001895, 40.19599999999995, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.1, 9.5, 12, 3.9489075321998555, 19.336140007530577, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.9, 13.8, 12, 12.974992742032462, 19.840843756232637, 1], 'reward': 1517.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.3, 2.4, 12, 12.318000000000568, 20.76700000000001, 0], 'reward': -1.1600000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613cdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.5, 4.8, 13, 4.232504417663771, 33.54930403616702, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 8.0, 13, 12.283118595928581, 23.60564970077342, -1], 'reward': -34.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bae20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 9.2, 13, 11.14631755427797, 34.5029504783691, -1], 'reward': -37.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.2, 8.8, 13, 17.178779986094668, 56.934232223962525, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.8, 32.3, 13, 12.916931577518984, 57.870008620151566, -1], 'reward': 23.12000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 26.4, 13, 31.73170622751944, 48.30365605268048, -1], 'reward': 6.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.7, 20.2, 14, 7.168000000002463, 52.08600000000031, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bd130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.4, 13.6, 14, 3.5540000000018948, 46.73000000000007, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d05e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 15.8, 14, 4.741000000005684, 58.52399999999958, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.1, 36.0, 15, 57.94347417573769, 38.93306325888152, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6143e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.4, 18.6, 15, 12.945000000000757, 55.52699999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6160520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 42.6, 15, 52.55198240358065, 14.990947849617712, -1], 'reward': 98.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.0, 39.9, 17, 42.75000000000284, 46.033999999999715, -1], 'reward': -1.5199999999999818}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.4, 13.7, 17, 7.790685851395308, 43.67145907666459, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 20.4, 19, 8.957000000002083, 58.25499999999967, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.4, 7.4, 21, 27.74452950196445, 34.659819575300034, -1], 'reward': -121.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f601acd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 6.7, 21, 17.79666156596427, 39.20757786170287, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa90d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.6, 10.0, 22, 9.203131109311077, 53.695346419900886, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f601abb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 16.8, 22, 42.37883834763513, 25.625416647294596, -1], 'reward': -15.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6295d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 3.4, 23, 24.713516437729353, 44.94824805306289, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a3f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.3, 18.9, 8, 3.2450000000056844, 28.5780000000001, -1], 'reward': -77.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 20.1, 8, 42.923998293025086, 33.19673341802101, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 8.9, 8, 6.732554237722223, 18.74464148169388, -1], 'reward': 0.6000000000000085}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a3a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.4, 21.8, 8, 14.011590516558197, 0.3771501214149673, -1], 'reward': 53.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 32.3, 11, 2.458000000002463, 17.31399999999993, -1], 'reward': -21.079999999999927}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fc0460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 19.2, 11, 14.503442801596497, 41.58600437272035, -1], 'reward': 7.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 12.3, 11, 7.183426249342787, 42.44649160516097, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fc0610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 41.2, 11, 1.3387281516444531, 7.969193618138554, -1], 'reward': 84.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 6.7, 13, 4.295999999999621, 20.713999999999867, -1], 'reward': -26.159999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6cdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.1, 7.7, 13, 2.4204350532305106, 17.458433409814997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624f910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.2, 8.9, 14, 17.719844608026392, 14.64813384478261, -1], 'reward': -27.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fe50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 18.8, 14, 3.548000000001137, 25.783000000000154, -1], 'reward': 9.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.1, 2.0, 14, 5.8266746015292155, 23.51482603436666, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 9.0, 14, 6.772177135454343, 39.91555142670234, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624f0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.8, 5.0, 14, 8.088840232326625, 33.66816720506517, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.4, 15.2, 14, 14.034531493416184, 54.17665326845435, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f780a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 10.7, 14, 12.105673422339214, 13.998172351976342, -1], 'reward': -5.200000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 11.2, 14, 5.856246825742559, 16.32588473466717, -1], 'reward': -48.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.5, 18.6, 15, 4.790086023894401, 31.376742168044537, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.0, 13.4, 15, 0.8994643375436864, 23.178313884491143, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e5700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 24.9, 16, 15.64449092611544, 46.16642882358471, -1], 'reward': 25.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.1, 10.5, 16, 8.59579660396787, 39.121601765349936, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e5310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 21.5, 16, 14.504693607479226, 35.89905413940114, -1], 'reward': -23.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.1, 35.2, 16, 11.053119449622116, 7.4592735387030515, -1], 'reward': 16.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e5ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.9, 22.0, 16, 2.2859999999996212, 26.631999999999923, -1], 'reward': 64.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e5a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [0.7, 2.5, 16, 0.8126179950438113, 27.6875830950848, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e5eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 5.6, 16, 5.657003865658888, 37.0359206315997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f393d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 33.9, 16, 39.16466078265041, 21.68215459737341, -1], 'reward': 69.04000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0cfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 22.8, 6, 8.662000000000567, 48.19100000000017, -1], 'reward': -56.920000000000016}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f08eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 19.8, 7, 19.560700399314356, 29.957886959151857, -1], 'reward': 27.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.9, 21.0, 7, 38.159864381653605, 13.640359100734102, -1], 'reward': 40.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6aa21f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.3, 41.8, 8, 10.776000000004167, 45.888999999999996, -1], 'reward': 124.92000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f204f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 10.1, 9, 3.6080000000026526, 49.60899999999955, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6aa22b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 6.1, 9, 15.699375158727346, 55.4153681735673, -1], 'reward': -21.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 7.3, 9, 13.504362375924817, 52.543556991229956, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.3, 3.2, 9, 13.127000000000189, 43.849000000000146, -1], 'reward': -87.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a970d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 6.6, 9, 2.7655872935902246, 55.58301724785126, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a27d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.9, 7.4, 11, 11.236696824677793, 35.53846095063737, -1], 'reward': -57.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a91280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.9, 7.8, 11, 15.915269008705899, 31.89087913257177, -1], 'reward': 12.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a27580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.2, 9.2, 11, 8.778765072785243, 37.46673942803067, -1], 'reward': -94.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a911c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 2.0, 11, 7.265152919770157, 51.573153789876095, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a27940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.6, 5.3, 11, 8.47667394903849, 32.48496653814409, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a275b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.0, 1.6, 11, 1.9748667239279085, 41.826067474832904, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a2f850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 24.7, 11, 27.040430389895675, 33.97885756528059, -1], 'reward': -9.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a2ff70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 18.0, 12, 6.075000000000189, 32.34300000000028, -1], 'reward': 35.84}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.4, 15.8, 12, 21.67867845250501, 28.36139349922677, -1], 'reward': 47.84}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.7, 5.3, 12, 0.19785187885751965, 47.826929991335916, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69a8d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.6, 24.3, 12, 38.894089460605734, 55.329292603894025, -1], 'reward': -35.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d87c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.1, 12.5, 13, 3.6510000000011367, 55.31800000000012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.1, 6.3, 14, 21.285182948734644, 51.30588857599382, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b3970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 28.7, 16, 0.08706828822318258, 37.976785987755264, -1], 'reward': -32.599999999999966}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a11fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.5, 8.1, 16, 3.2010000000026526, 43.56500000000035, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a11e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.9, 12.9, 16, 11.980435823953842, 43.190634534298745, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b3400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 16.2, 16, 14.09499224290181, 44.02896866893994, -1], 'reward': 13.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b3c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.0, 10.8, 16, 9.549959332854408, 36.81189017286741, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b0af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.2, 45.1, 16, 41.087083323509496, 42.652797235729466, -1], 'reward': -33.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ef0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.8, 24.3, 17, 1.9120000000045474, 48.41700000000021, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f6f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.7, 21.4, 17, 57.53248279523363, 35.617336434178206, -1], 'reward': 22.920000000000016}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.2     , 11.8     ,  0.      , 47.782516, 35.797092, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-38.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1       , 41.7       ,  6.        ,  0.10651689, 20.249702  ,\n",
      "        39.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.8  , 27.8  ,  6.   , 15.539, 47.318, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1     , 35.6     ,  7.      , 16.75514 , 28.135323, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.4     , 26.9     ,  8.      , 25.091724, 21.490059, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=83.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     ,  6.2     ,  8.      , 36.57297 , 26.358913, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-96.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.7  , 26.2  ,  8.   , 16.147, 12.042, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.2  , 25.6  ,  9.   , 17.02 , 48.725, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     , 26.5     ,  9.      , 39.84335 , 48.756172, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=56.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 20.9  , 12.   , 13.213, 51.102, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.6  , 35.2  , 16.   , 16.166, 17.965, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.7  , 25.2  , 16.   , 18.612, 56.221, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.       , 18.1      , 16.       , 10.3590355, 53.89971  ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-57.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     ,  7.5     , 16.      , 12.019068, 41.428913, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.5     , 25.5     , 16.      , 29.974442, 57.15824 , 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 30.8     , 17.      ,  9.036209, 37.036423, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.3  , 41.   , 17.   , 13.587, 36.148, 31.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.4  , 32.4  , 18.   , 31.092, 19.717, 31.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     , 26.8     , 18.      , 13.80593 , 50.100143, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.      , 15.6     , 18.      , 24.246216, 31.772154, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3      , 28.8      , 19.       ,  7.215088 ,  7.9121857,\n",
      "        30.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=69.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     , 35.9     , 20.      , 17.57054 , 51.198364, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 33.1     , 21.      , 35.26994 , 13.478365, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1     ,  6.2     ,  8.      , 27.926025, 13.709016, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7     ,  1.      ,  8.      , 18.95611 , 25.246883, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-89.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.4  , 29.5  ,  8.   , 22.506, 48.046, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.3  , 15.8  ,  8.   ,  8.152, 58.752, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     ,  2.9     ,  9.      , 14.563366, 42.924824, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.7     , 21.3     ,  9.      , 40.762245, 51.95467 , 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.4     , 40.6     ,  9.      ,  7.914279, 36.00341 , 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5      , 23.5      , 15.       ,  2.7424026, 34.879345 ,\n",
      "        24.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     , 27.9     , 16.      ,  8.639843, 48.575237, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.   , 29.3  , 18.   ,  4.137, 48.655, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.3  , 22.6  , 20.   ,  7.919, 45.454, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.7  , 23.7  , 22.   ,  9.386, 40.595, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 24.6  ,  2.   ,  7.901, 42.098, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.      ,  7.8     ,  8.      , 36.71028 , 44.111084, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=11.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.5  , 10.9  , 11.   ,  8.244, 34.014, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-111.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.       , 22.7      , 11.       ,  1.5813117, 15.034652 ,\n",
      "        22.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.6     , 25.6     , 11.      , 33.945652, 28.98123 , 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=71.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.6  , 22.2  , 12.   , 10.349, 45.487, 21.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.4  , 14.7  , 14.   , 10.352, 47.436, 21.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.8  , 24.7  , 15.   ,  6.222,  9.886, 21.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.7     ,  2.3     , 15.      , 24.131979, 18.231426, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-79.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.2      , 13.9      , 15.       ,  4.2264304, 17.636503 ,\n",
      "        20.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.6     , 18.5     , 16.      , 16.846273, 44.06459 , 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.5      , 24.       , 16.       ,  7.9801135, 45.045925 ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.3     , 45.1     , 16.      , 49.174404, 41.895855, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.7     , 13.3     , 17.      , 17.275635, 50.446335, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-91.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.   , 27.8  , 18.   , 14.448, 16.808, 17.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=48.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 24.5     , 18.      , 20.699566, 39.03103 , 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=45.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.1  , 14.4  , 18.   ,  5.106, 48.915, 15.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.6     ,  9.1     , 18.      , 11.895678, 52.45351 , 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.3     ,  8.6     , 18.      , 10.957723, 59.2794  , 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 38.8     , 18.      , 16.851988, 11.266445, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=4.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.7  ,  1.7  , 18.   ,  4.91 , 29.418, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.8  , 11.5  , 18.   ,  2.855, 32.047, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.6  ,  4.2  , 19.   ,  6.272, 11.774, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-31.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3     , 22.2     , 19.      ,  4.342945, 55.16374 , 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.8     , 24.6     , 19.      , 19.522387, 43.994915, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=5.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     , 13.1     , 19.      , 17.726707, 45.650578, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-53.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.6      ,  4.3      , 19.       ,  5.3045635, 55.844574 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1      , 18.2      , 19.       ,  0.6945296, 22.023184 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.4  , 17.   , 19.   ,  7.206, 33.998, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.3  , 24.3  , 19.   ,  6.162, 39.925, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.9      ,  8.1      , 20.       ,  7.6086297, 52.250175 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.6     , 11.8     , 20.      , 22.866367, 36.680912, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.3      , 35.3      , 20.       , 23.281998 ,  2.8906884,\n",
      "        10.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=83.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.9     , 13.8     , 21.      ,  9.089585, 32.962757,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-131.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 13.      , 22.      , 17.575666, 44.297085,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=3.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.9    , 21.6    , 22.     , 35.75302, 37.87958,  7.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=49.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.6  , 17.1  , 22.   ,  5.581, 54.39 ,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.2  , 23.8  ,  6.   ,  6.363, 18.739,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.1  ,  2.   ,  7.   ,  5.685, 21.178,  5.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8      ,  6.4      ,  7.       ,  2.2750905, 22.593996 ,\n",
      "         5.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9      ,  4.3      ,  8.       ,  6.0256104, 14.04301  ,\n",
      "         5.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3     ,  4.1     ,  8.      , 20.664225, 17.609863,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-84.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.3      ,  5.2      ,  9.       ,  3.2921915, 38.486572 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.8  ,  7.7  , 10.   ,  2.476, 38.547,  3.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[27.6      ,  7.1      , 10.       ,  7.3898005, 44.43037  ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.      , 13.2     , 11.      ,  9.207707, 51.692097,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     ,  4.6     , 11.      , 12.400522, 32.93502 ,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-81.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.8  , 18.3  , 12.   ,  2.164, 30.416,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-48.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.1  ,  5.5  , 12.   ,  7.494, 40.196,  1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.1      ,  9.5      , 12.       ,  3.9489076, 19.33614  ,\n",
      "         1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.9     , 13.8     , 12.      , 12.974993, 19.840843,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1517.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.3  ,  2.4  , 12.   , 12.318, 20.767,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.5      ,  4.8      , 13.       ,  4.2325044, 33.549305 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9     ,  8.      , 13.      , 12.283118, 23.60565 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-34.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9      ,  9.2      , 13.       , 11.1463175, 34.50295  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-37.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.2    ,  8.8    , 13.     , 17.17878, 56.93423, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.8     , 32.3     , 13.      , 12.916931, 57.87001 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4     , 26.4     , 13.      , 31.731707, 48.303658, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=6.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.7  , 20.2  , 14.   ,  7.168, 52.086, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[24.4  , 13.6  , 14.   ,  3.554, 46.73 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  , 15.8  , 14.   ,  4.741, 58.524, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     , 36.      , 15.      , 57.943474, 38.933064, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.4  , 18.6  , 15.   , 12.945, 55.527, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 42.6     , 15.      , 52.551983, 14.990948, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=98.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.   , 39.9  , 17.   , 42.75 , 46.034, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4      , 13.7      , 17.       ,  7.7906857, 43.67146  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.6  , 20.4  , 19.   ,  8.957, 58.255, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4    ,  7.4    , 21.     , 27.74453, 34.65982, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-121.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4     ,  6.7     , 21.      , 17.796661, 39.207577, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.6     , 10.      , 22.      ,  9.203131, 53.695347, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2     , 16.8     , 22.      , 42.378838, 25.625416, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.8     ,  3.4     , 23.      , 24.713516, 44.94825 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.3  , 18.9  ,  8.   ,  3.245, 28.578, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-77.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1    , 20.1    ,  8.     , 42.924  , 33.19673, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1      ,  8.9      ,  8.       ,  6.7325544, 18.744642 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.4       , 21.8       ,  8.        , 14.011591  ,  0.37715012,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.3  , 32.3  , 11.   ,  2.458, 17.314, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.9     , 19.2     , 11.      , 14.503443, 41.586006, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7      , 12.3      , 11.       ,  7.1834264, 42.44649  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.       , 41.2      , 11.       ,  1.3387282,  7.9691935,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=84.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.   ,  6.7  , 13.   ,  4.296, 20.714, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1     ,  7.7     , 13.      ,  2.420435, 17.458433, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.2     ,  8.9     , 14.      , 17.719845, 14.648134, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-27.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.4  , 18.8  , 14.   ,  3.548, 25.783, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=9.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.1      ,  2.       , 14.       ,  5.8266745, 23.514826 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     ,  9.      , 14.      ,  6.772177, 39.91555 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8      ,  5.       , 14.       ,  8.0888405, 33.668167 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.4     , 15.2     , 14.      , 14.034532, 54.176655, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 10.7     , 14.      , 12.105674, 13.998173, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.4     , 11.2     , 14.      ,  5.856247, 16.325884, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-48.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.5     , 18.6     , 15.      ,  4.790086, 31.376741, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.       , 13.4      , 15.       ,  0.8994643, 23.178314 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.      , 24.9     , 16.      , 15.644491, 46.166428, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=25.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1     , 10.5     , 16.      ,  8.595797, 39.1216  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     , 21.5     , 16.      , 14.504694, 35.899055, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1      , 35.2      , 16.       , 11.05312  ,  7.4592733,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=16.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 0.9  , 22.   , 16.   ,  2.286, 26.632, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=64.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.7     ,  2.5     , 16.      ,  0.812618, 27.687584, -1.      ]],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2     ,  5.6     , 16.      ,  5.657004, 37.03592 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 33.9     , 16.      , 39.16466 , 21.682154, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=69.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.1  , 22.8  ,  6.   ,  8.662, 48.191, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 19.8     ,  7.      , 19.5607  , 29.957888, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=27.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.9     , 21.      ,  7.      , 38.159863, 13.640359, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=40.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.3  , 41.8  ,  8.   , 10.776, 45.889, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=124.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 10.1  ,  9.   ,  3.608, 49.609, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      ,  6.1     ,  9.      , 15.699375, 55.415367, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1     ,  7.3     ,  9.      , 13.504362, 52.543556, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.3  ,  3.2  ,  9.   , 13.127, 43.849, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-87.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3      ,  6.6      ,  9.       ,  2.7655873, 55.583015 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.9     ,  7.4     , 11.      , 11.236697, 35.53846 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-57.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9     ,  7.8     , 11.      , 15.915269, 31.890879, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.2     ,  9.2     , 11.      ,  8.778765, 37.46674 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-94.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.      ,  2.      , 11.      ,  7.265153, 51.573154, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.6     ,  5.3     , 11.      ,  8.476674, 32.484966, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.       ,  1.6      , 11.       ,  1.9748667, 41.82607  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.     , 24.7    , 11.     , 27.04043, 33.97886, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-9.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.2  , 18.   , 12.   ,  6.075, 32.343, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.4     , 15.8     , 12.      , 21.678679, 28.361393, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=47.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.7       ,  5.3       , 12.        ,  0.19785188, 47.82693   ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.6     , 24.3     , 12.      , 38.89409 , 55.329292, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.1  , 12.5  , 13.   ,  3.651, 55.318, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.1     ,  6.3     , 14.      , 21.285183, 51.30589 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.3       , 28.7       , 16.        ,  0.08706829, 37.976788  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.5  ,  8.1  , 16.   ,  3.201, 43.565, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9     , 12.9     , 16.      , 11.980435, 43.190636, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 16.2     , 16.      , 14.094993, 44.02897 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.      , 10.8     , 16.      ,  9.549959, 36.81189 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.2     , 45.1     , 16.      , 41.087082, 42.652798, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-33.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.8  , 24.3  , 17.   ,  1.912, 48.417, -1.   ]], dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6668670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.5, 3.8, 2, 7.584000000002083, 41.82799999999984, 40], 'reward': -52.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 8.5, 6, 2.4409720177717364, 37.72140120808937, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.7, 15.2, 7, 4.6745403763371955, 34.62463023286752, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ec6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 20.2, 7, 18.83501497016664, 26.97389113193091, 39], 'reward': 33.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 8.6, 8, 25.26589653564874, 26.59768350357138, 38], 'reward': 2.3599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62892b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.1, 26.2, 8, 16.147000000001327, 12.042000000000083, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.8, 17.7, 9, 9.600091904012055, 29.22853197988893, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 6.9, 9, 15.079000000001326, 26.458000000000418, 37], 'reward': -44.56000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 24.9, 9, 39.48997585785834, 6.228152912624969, 36], 'reward': 29.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.4, 31.4, 9, 11.001000000001325, 41.27000000000002, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a8a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.5, 27.1, 9, 12.595000000001324, 45.19299999999999, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6087520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.7, 42.6, 13, 13.509000000001704, 43.029999999999816, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.8, 21.3, 16, 52.62499999999848, 23.7259999999999, 35], 'reward': 21.919999999999987}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb24c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 8.6, 2, 48.960000000000946, 39.04700000000024, 34], 'reward': -21.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec9b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.4, 40.6, 9, 7.9142791835001205, 36.00341194861859, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed5e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 7.7, 12, 48.628895828699385, 39.0020901197428, 33], 'reward': -28.39999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.2, 28.1, 13, 13.750000000001325, 44.10099999999972, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.6, 20.0, 16, 25.90799999999962, 42.40599999999988, 32], 'reward': 5.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.4, 15.2, 16, 2.0320000000009473, 53.97700000000002, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.3, 7.1, 16, 26.10031292211179, 44.796334450759, 31], 'reward': -20.11999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.6, 2.2, 16, 7.364501726779636, 27.604599072170164, 30], 'reward': -160.24}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e692b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 14.9, 16, 2.8250659046568334, 45.574862939619166, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 19.3, 16, 22.687014751589984, 19.02691006721323, 29], 'reward': 29.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e778e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 23.8, 16, 14.362449528596219, 4.421539689238852, 28], 'reward': -38.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e742e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.3, 17.2, 16, 18.338608685633098, 28.76618907391812, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f0f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.2, 10.3, 17, 4.7480000000028415, 33.360000000000326, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.6, 20.1, 17, 11.69991916064143, 42.226232466765524, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.4, 15.5, 17, 10.440552440145172, 36.10584538092067, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 10.8, 17, 18.773365333678473, 12.11706919956238, 27], 'reward': -53.84}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ae700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.3, 13.6, 17, 4.692000000002842, 35.981000000000186, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630f730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.9, 6.5, 18, 8.753491716261133, 35.50271714507016, 26], 'reward': -128.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630feb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.4, 51.9, 18, 45.462486840244125, 9.860234688623233, 25], 'reward': 115.76000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 6.3, 18, 36.87499999999886, 7.216000000000222, 24], 'reward': -63.480000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 31.0, 19, 4.316000000002842, 43.99400000000036, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.2, 38.2, 20, 8.429000000004358, 39.83200000000002, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.6, 13.5, 13, 40.64134724806388, 17.72835998744165, 23], 'reward': -22.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.6, 40.2, 18, 8.494000000005684, 39.13699999999983, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617ac10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.5, 17.7, 20, 4.039000000003221, 24.648000000000394, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 30.6, 7, 6.759999999998105, 34.23600000000005, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.7, 10.4, 12, 18.67500000000057, 18.36299999999999, 22], 'reward': -46.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 25.1, 12, 15.071121950079906, 38.19204249566194, 21], 'reward': 40.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dd00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.8, 6.5, 12, 32.955240687442114, 30.80176009372281, 20], 'reward': -73.03999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613cf70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.9, 22.6, 13, 30.720122104673493, 55.16786545549802, 19], 'reward': 59.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62badc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.4, 6.3, 13, 6.587785537274513, 41.80863673087923, 18], 'reward': -125.35999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 8.8, 13, 17.178779986094668, 56.934232223962525, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.6, 8.5, 13, 11.63944815500096, 36.2452282207478, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bddf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.9, 38.6, 13, 41.66979499263875, 19.26263356169104, 17], 'reward': 29.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d0b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.0, 1.9, 14, 34.03290148316131, 5.310881778061516, 16], 'reward': -89.12}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60eb790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.9, 5.9, 16, 29.168157104378636, 20.189770835956985, 15], 'reward': -62.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.2, 8.6, 16, 20.18648531257996, 31.614087935664653, 14], 'reward': -116.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61551f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.7, 23.5, 16, 24.401555236789576, 34.10161259743617, 13], 'reward': -51.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.6, 39.2, 17, 26.842885277248598, 17.046009917613944, 12], 'reward': -41.84000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.5, 17.8, 17, 1.7650358135482218, 34.98279272962623, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.7, 17.3, 17, 6.470645260641511, 49.105805109496146, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.3, 24.7, 17, 6.970000000004736, 32.702999999999655, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 5.6, 17, 20.89206885963888, 27.915892780535653, 11], 'reward': -30.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.1, 6.1, 17, 5.874465894788705, 50.74712715483066, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611fd60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 5.8, 17, 4.241665609516133, 46.82404973813961, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653bca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 8.6, 17, 11.274489728311599, 36.55086893080491, 10], 'reward': -117.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653bbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.4, 11.8, 17, 4.5670000000030315, 58.05199999999964, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653b7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 30.0, 17, 42.29373440698356, 28.624245497788714, 9], 'reward': 70.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fae50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.6, 45.0, 19, 3.0190000000056845, 47.76900000000032, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bfca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.5, 29.8, 21, 8.008000000000568, 28.15800000000016, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6073a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 10.3, 8, 30.586197624760576, 28.406111439359783, 8], 'reward': -20.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62721f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 21.0, 8, 5.8940000000030315, 60.0, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.5, 23.0, 8, 7.559978753716798, 43.36409527598523, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627efd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.6, 16.0, 10, 10.927874000075809, 19.711168322463983, 7], 'reward': -102.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626cc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 5.4, 11, 7.0578293746672385, 22.738137260004915, 6], 'reward': -52.08}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626cb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 17.4, 11, 1.2519700110269292, 59.74514081629084, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626c850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.1, 16.2, 12, 20.549347939776975, 41.43869466718898, 5], 'reward': -23.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626caf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.4, 9.0, 12, 10.799000000004547, 49.99199999999974, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 11.8, 12, 9.061662342388663, 42.80246776491761, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f609f880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.8, 26.7, 12, 47.89913628680792, 22.538966052103373, 4], 'reward': 39.20000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6030fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.2, 32.7, 14, 20.62291915275277, 42.32295074084624, 3], 'reward': 96.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.0, 8.7, 14, 0.6435345211682071, 51.508840592282276, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602a3d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 8.5, 14, 7.392701153670186, 41.026567160479004, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.9, 10.6, 14, 21.128135146158378, 58.25613529216352, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 13.3, 14, 20.285038772170843, 45.55946420972592, 2], 'reward': -63.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 2.5, 14, 26.94996264153135, 38.4074242490691, 1], 'reward': 1453.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 11.9, 14, 7.0681826151549245, 32.827733102218815, 0], 'reward': -53.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6030b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.2, 35.7, 14, 20.543111751796413, 11.181627061708227, -1], 'reward': 31.279999999999973}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 6.4, 15, 11.882000000000378, 18.857000000000212, -1], 'reward': -27.799999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60304c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 2.8, 15, 5.422686478751402, 34.68373985538616, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 8.0, 15, 4.695619267631096, 46.60017998159048, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60548b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.0, 15.4, 16, 17.700355553431702, 11.848180442165166, -1], 'reward': -66.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603ea60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.0, 7.6, 16, 10.280300090734555, 21.925393176037634, -1], 'reward': -111.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 16.6, 16, 28.528543263452526, 30.22045432104686, -1], 'reward': -15.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 4.1, 16, 20.97400000000019, 32.78900000000018, -1], 'reward': -17.479999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60542b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 10.2, 16, 16.107143775042438, 30.51267739040637, -1], 'reward': -67.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 10.5, 16, 11.120425690084295, 45.75179786629485, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.2, 7.8, 16, 8.503651991531711, 28.822429381138058, -1], 'reward': -51.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605da30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 32.7, 16, 36.600737843724126, 25.337732614323986, -1], 'reward': 67.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.3, 35.1, 16, 12.974999999999053, 50.977000000000004, -1], 'reward': 110.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 7.5, 16, 10.261712721628554, 45.09015336250203, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 4.4, 16, 5.946311883241594, 53.58878065322524, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.1, 28.1, 17, 6.66996764482878, 13.19208412183714, -1], 'reward': -5.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 22.0, 17, 2.637000000003979, 42.88299999999989, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.1, 8.9, 17, 10.049000000004735, 21.591000000000022, -1], 'reward': -53.79999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.2, 4.8, 17, 6.155472390083332, 31.90661146771775, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.5, 20.9, 17, 2.833000000000379, 50.198000000000114, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.5, 11.2, 17, 15.125347031860828, 33.03206711801185, -1], 'reward': 25.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.3, 13.7, 17, 7.790685851395308, 43.67145907666459, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6030190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.9, 7.3, 17, 10.035509179911607, 54.900791793133386, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.1, 17.1, 17, 14.785426478584437, 40.64930261875587, -1], 'reward': -109.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fa60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 15.9, 17, 22.660153848238544, 45.29255962423851, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.4, 31.0, 17, 25.678589408383825, 27.806020373360813, -1], 'reward': -5.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 16.8, 17, 32.683526444114776, 51.19113108582561, -1], 'reward': 2.0800000000000125}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 7.9, 17, 13.561999999999053, 47.92499999999998, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.3, 16.5, 17, 28.338575395485027, 36.05920037863041, -1], 'reward': -51.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.5, 24.5, 18, 8.996252715996304, 20.294689196430152, -1], 'reward': 20.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.1, 32.0, 18, 25.122892386045855, 27.401067249176933, -1], 'reward': -20.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 12.6, 18, 9.166886507788075, 55.7431415635407, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.0, 9.4, 18, 5.040172516706594, 36.55253931157359, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fecca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.7, 54.8, 18, 43.06068128027414, 3.1211194422941304, -1], 'reward': 7.399999999999977}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffae80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.3, 13.6, 22, 15.59299745857688, 26.710027801187778, -1], 'reward': -114.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 35.8, 22, 36.71329761001698, 44.29119258980063, -1], 'reward': 11.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa97c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.2, 10.0, 22, 9.203131109311077, 53.695346419900886, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.5, 2.1, 22, 20.562737082236037, 25.842601093393647, -1], 'reward': -173.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.9, 5.7, 22, 13.140967087873303, 23.828842070938997, -1], 'reward': -55.88000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 14.9, 22, 16.202858685033636, 20.12434784384, -1], 'reward': -21.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.6, 4.7, 23, 9.405879924342624, 37.007681117785545, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 14.4, 23, 16.102796047089132, 28.366503076508366, -1], 'reward': -68.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.3, 17.0, 23, 7.736745178754541, 31.205232598371953, -1], 'reward': -104.03999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6295520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.0, 3.4, 23, 24.713516437729353, 44.94824805306289, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa99a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.6, 15.7, 23, 3.8140000000056844, 31.46700000000028, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 14.2, 0, 24.309096197825347, 31.849407832383967, -1], 'reward': 23.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6295910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 19.8, 2, 6.883000000002841, 48.61899999999969, -1], 'reward': 32.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62950a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 16.3, 2, 10.90947161788938, 33.94605572497518, -1], 'reward': 23.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6295c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 11.7, 4, 9.743175101327534, 45.950791588480215, -1], 'reward': -6.0800000000000125}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6295f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.4, 24.9, 5, 28.600896551366763, 50.898033662247606, -1], 'reward': 42.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 22.3, 6, 5.1800000000022735, 46.08499999999994, -1], 'reward': 49.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62454c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.5, 10.3, 6, 12.468124780610403, 49.34969434331193, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.3, 13.9, 6, 1.478921166765243, 58.629786308647496, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62456a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.7, 13.8, 6, 13.761434992504032, 46.076217794521305, -1], 'reward': -15.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62452b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 7.2, 7, 9.275643950608966, 34.173809196691096, -1], 'reward': -101.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.5, 27.6, 7, 18.191752304472118, 41.145581829430895, -1], 'reward': -30.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.2, 15.0, 7, 16.76090822534706, 46.91245161873787, -1], 'reward': -89.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 18.4, 7, 11.31773108169035, 28.10581584224478, -1], 'reward': -28.159999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 9.9, 7, 13.037380058218293, 14.752154888172686, -1], 'reward': 3.799999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6245700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.8, 27.0, 7, 5.779027448663168, 30.291790365049213, -1], 'reward': -0.6399999999999864}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622de20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.3, 17.5, 7, 2.5760000000026526, 56.96999999999963, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 4.0, 8, 8.266647448007134, 45.6470007258497, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622dca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.3, 7.6, 8, 21.75884444181234, 29.318646289143963, -1], 'reward': -52.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 11.8, 8, 22.236976162159994, 58.786530688316994, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.2, 14.0, 8, 2.707000000005684, 25.135, -1], 'reward': -17.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624bfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.4, 7.0, 8, 2.0710000000026527, 14.740000000000302, -1], 'reward': -34.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 25.8, 8, 26.275773901087796, 3.2626647660811603, -1], 'reward': 50.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622df40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 11.6, 8, 1.5680706804406856, 14.797218704780398, -1], 'reward': -107.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 1.3, 8, 4.747145874155778, 2.683402232912264, -1], 'reward': -73.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62992b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.2, 18.7, 9, 20.271269533422426, 14.937273764258576, -1], 'reward': -63.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62993a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 4.8, 9, 8.721395529867575, 16.91365602174016, -1], 'reward': -81.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a3c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 4.4, 10, 4.1341419510890525, 22.113569397451045, -1], 'reward': -55.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.3, 13.5, 10, 6.812000000003789, 43.78500000000013, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 33.2, 10, 44.016409900038425, 48.86050801632157, -1], 'reward': -32.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fc0b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 18.9, 11, 51.47311110598325, 24.608042768195602, -1], 'reward': 15.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.5, 7.0, 17, 44.72200000000038, 43.18499999999994, -1], 'reward': -96.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 26.0, 17, 58.1752258853452, 43.739171899207975, -1], 'reward': -5.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.5, 18.4, 23, 51.85151389796997, 30.23552254228278, -1], 'reward': 35.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6997c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.4, 6.3, 18, 54.750999999999806, 37.52999999999958, -1], 'reward': -16.559999999999988}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.5  ,  3.8  ,  2.   ,  7.584, 41.828, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     ,  8.5     ,  6.      ,  2.440972, 37.7214  , 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7      , 15.2      ,  7.       ,  4.6745405, 34.62463  ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 20.2     ,  7.      , 18.835014, 26.97389 , 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=33.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     ,  8.6     ,  8.      , 25.265896, 26.597683, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=2.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.1  , 26.2  ,  8.   , 16.147, 12.042, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.8     , 17.7     ,  9.      ,  9.600092, 29.228533, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.8  ,  6.9  ,  9.   , 15.079, 26.458, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-44.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.4      , 24.9      ,  9.       , 39.489975 ,  6.2281528,\n",
      "        36.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.4  , 31.4  ,  9.   , 11.001, 41.27 , 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.5  , 27.1  ,  9.   , 12.595, 45.193, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.7  , 42.6  , 13.   , 13.509, 43.03 , 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.8  , 21.3  , 16.   , 52.625, 23.726, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=21.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.2  ,  8.6  ,  2.   , 48.96 , 39.047, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4     , 40.6     ,  9.      ,  7.914279, 36.00341 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     ,  7.7     , 12.      , 48.628895, 39.00209 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.2  , 28.1  , 13.   , 13.75 , 44.101, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.6  , 20.   , 16.   , 25.908, 42.406, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=5.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.4  , 15.2  , 16.   ,  2.032, 53.977, 31.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.3     ,  7.1     , 16.      , 26.100313, 44.796333, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.6     ,  2.2     , 16.      ,  7.364502, 27.604599, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-160.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8      , 14.9      , 16.       ,  2.8250659, 45.574863 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     , 19.3     , 16.      , 22.687016, 19.02691 , 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9    , 23.8    , 16.     , 14.36245,  4.42154, 28.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-38.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3     , 17.2     , 16.      , 18.338608, 28.76619 , 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.2  , 10.3  , 17.   ,  4.748, 33.36 , 27.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.6     , 20.1     , 17.      , 11.699919, 42.22623 , 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.4     , 15.5     , 17.      , 10.440553, 36.105846, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 10.8     , 17.      , 18.773365, 12.117069, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-53.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.3  , 13.6  , 17.   ,  4.692, 35.981, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     ,  6.5     , 18.      ,  8.753491, 35.502716, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-128.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.4     , 51.9     , 18.      , 45.462486,  9.860234, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=115.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.3  ,  6.3  , 18.   , 36.875,  7.216, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.6  , 31.   , 19.   ,  4.316, 43.994, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.2  , 38.2  , 20.   ,  8.429, 39.832, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 13.5     , 13.      , 40.641346, 17.72836 , 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-22.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.6  , 40.2  , 18.   ,  8.494, 39.137, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.5  , 17.7  , 20.   ,  4.039, 24.648, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 30.6  ,  7.   ,  6.76 , 34.236, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.7  , 10.4  , 12.   , 18.675, 18.363, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 25.1     , 12.      , 15.071122, 38.192043, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=40.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.8     ,  6.5     , 12.      , 32.955242, 30.80176 , 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9     , 22.6     , 13.      , 30.720121, 55.167866, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=59.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4      ,  6.3      , 13.       ,  6.5877857, 41.808636 ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-125.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8    ,  8.8    , 13.     , 17.17878, 56.93423, 17.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.6     ,  8.5     , 13.      , 11.639448, 36.245228, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 38.6     , 13.      , 41.669796, 19.262634, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.       ,  1.9      , 14.       , 34.0329   ,  5.3108816,\n",
      "        16.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-89.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.9     ,  5.9     , 16.      , 29.168158, 20.189772, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-62.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.2     ,  8.6     , 16.      , 20.186485, 31.614088, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-116.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.7     , 23.5     , 16.      , 24.401556, 34.101612, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.6     , 39.2     , 17.      , 26.842886, 17.04601 , 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5      , 17.8      , 17.       ,  1.7650359, 34.98279  ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.7      , 17.3      , 17.       ,  6.4706454, 49.105804 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.3  , 24.7  , 17.   ,  6.97 , 32.703, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1     ,  5.6     , 17.      , 20.892069, 27.915894, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.1     ,  6.1     , 17.      ,  5.874466, 50.747128, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.9     ,  5.8     , 17.      ,  4.241666, 46.82405 , 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3     ,  8.6     , 17.      , 11.274489, 36.55087 , 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-117.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.4  , 11.8  , 17.   ,  4.567, 58.052,  9.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     , 30.      , 17.      , 42.293736, 28.624245,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=70.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.6  , 45.   , 19.   ,  3.019, 47.769,  8.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.5  , 29.8  , 21.   ,  8.008, 28.158,  8.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.9     , 10.3     ,  8.      , 30.586197, 28.40611 ,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.6  , 21.   ,  8.   ,  5.894, 60.   ,  7.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.5     , 23.      ,  8.      ,  7.559979, 43.364094,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.6     , 16.      , 10.      , 10.927874, 19.711168,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-102.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2      ,  5.4      , 11.       ,  7.0578294, 22.738138 ,\n",
      "         6.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1    , 17.4    , 11.     ,  1.25197, 59.74514,  5.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1     , 16.2     , 12.      , 20.549349, 41.438694,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.4  ,  9.   , 12.   , 10.799, 49.992,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     , 11.8     , 12.      ,  9.061663, 42.802467,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.8     , 26.7     , 12.      , 47.899136, 22.538965,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=39.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.2     , 32.7     , 14.      , 20.62292 , 42.322952,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=96.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.        ,  8.7       , 14.        ,  0.64353454, 51.508842  ,\n",
      "         2.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.7     ,  8.5     , 14.      ,  7.392701, 41.026566,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.9     , 10.6     , 14.      , 21.128136, 58.256134,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 13.3     , 14.      , 20.285038, 45.559464,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.      ,  2.5     , 14.      , 26.949963, 38.407425,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1453.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.5      , 11.9      , 14.       ,  7.0681825, 32.827732 ,\n",
      "         0.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-53.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.2     , 35.7     , 14.      , 20.543112, 11.181627, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=31.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.1  ,  6.4  , 15.   , 11.882, 18.857, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-27.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.9      ,  2.8      , 15.       ,  5.4226866, 34.68374  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     ,  8.      , 15.      ,  4.695619, 46.60018 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.      , 15.4     , 16.      , 17.700356, 11.848181, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-66.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.      ,  7.6     , 16.      , 10.2803  , 21.925394, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-111.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.1     , 16.6     , 16.      , 28.528543, 30.220455, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  ,  4.1  , 16.   , 20.974, 32.789, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     , 10.2     , 16.      , 16.107143, 30.512678, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 10.5     , 16.      , 11.120425, 45.751797, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.2     ,  7.8     , 16.      ,  8.503652, 28.82243 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5     , 32.7     , 16.      , 36.60074 , 25.337732, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 0.3  , 35.1  , 16.   , 12.975, 50.977, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=110.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     ,  7.5     , 16.      , 10.261713, 45.090153, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  4.4     , 16.      ,  5.946312, 53.58878 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1      , 28.1      , 17.       ,  6.6699677, 13.192084 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 22.   , 17.   ,  2.637, 42.883, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.1  ,  8.9  , 17.   , 10.049, 21.591, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-53.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2      ,  4.8      , 17.       ,  6.1554723, 31.906612 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.5  , 20.9  , 17.   ,  2.833, 50.198, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.5     , 11.2     , 17.      , 15.125347, 33.032066, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=25.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3      , 13.7      , 17.       ,  7.7906857, 43.67146  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.9     ,  7.3     , 17.      , 10.035509, 54.90079 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.1     , 17.1     , 17.      , 14.785426, 40.649303, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-109.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     , 15.9     , 17.      , 22.660154, 45.29256 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.4     , 31.      , 17.      , 25.678589, 27.80602 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 16.8     , 17.      , 32.683525, 51.19113 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=2.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.   ,  7.9  , 17.   , 13.562, 47.925, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     , 16.5     , 17.      , 28.338575, 36.0592  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.5     , 24.5     , 18.      ,  8.996253, 20.29469 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=20.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1     , 32.      , 18.      , 25.122892, 27.401068, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-20.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.9     , 12.6     , 18.      ,  9.166886, 55.74314 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.       ,  9.4      , 18.       ,  5.0401726, 36.55254  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.7      , 54.8      , 18.       , 43.06068  ,  3.1211195,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3     , 13.6     , 22.      , 15.592998, 26.710028, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-114.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2    , 35.8    , 22.     , 36.7133 , 44.29119, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=11.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.2     , 10.      , 22.      ,  9.203131, 53.695347, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.5     ,  2.1     , 22.      , 20.562737, 25.842602, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-173.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.9     ,  5.7     , 22.      , 13.140967, 23.828842, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.1     , 14.9     , 22.      , 16.202858, 20.124348, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-21.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.6     ,  4.7     , 23.      ,  9.40588 , 37.007683, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9     , 14.4     , 23.      , 16.102797, 28.366503, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-68.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3      , 17.       , 23.       ,  7.7367454, 31.205233 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-104.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      ,  3.4     , 23.      , 24.713516, 44.94825 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.6  , 15.7  , 23.   ,  3.814, 31.467, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2     , 14.2     ,  0.      , 24.309095, 31.849407, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  , 19.8  ,  2.   ,  6.883, 48.619, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=32.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2      , 16.3      ,  2.       , 10.9094715, 33.946056 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4      , 11.7      ,  4.       ,  9.7431755, 45.95079  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-6.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4     , 24.9     ,  5.      , 28.600897, 50.898033, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=42.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.2  , 22.3  ,  6.   ,  5.18 , 46.085, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=49.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.5     , 10.3     ,  6.      , 12.468124, 49.349693, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3      , 13.9      ,  6.       ,  1.4789212, 58.629787 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     , 13.8     ,  6.      , 13.761435, 46.076218, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.3     ,  7.2     ,  7.      ,  9.275644, 34.17381 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-101.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5     , 27.6     ,  7.      , 18.191751, 41.14558 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2     , 15.      ,  7.      , 16.760908, 46.912453, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-89.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     , 18.4     ,  7.      , 11.317731, 28.105816, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1     ,  9.9     ,  7.      , 13.03738 , 14.752155, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=3.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8      , 27.       ,  7.       ,  5.7790275, 30.29179  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.3  , 17.5  ,  7.   ,  2.576, 56.97 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.8     ,  4.      ,  8.      ,  8.266647, 45.647   , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.3     ,  7.6     ,  8.      , 21.758844, 29.318645, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.7     , 11.8     ,  8.      , 22.236977, 58.78653 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 14.   ,  8.   ,  2.707, 25.135, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.4  ,  7.   ,  8.   ,  2.071, 14.74 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-34.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7      , 25.8      ,  8.       , 26.275774 ,  3.2626648,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3      , 11.6      ,  8.       ,  1.5680707, 14.797218 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-107.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4      ,  1.3      ,  8.       ,  4.7471457,  2.6834023,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.2     , 18.7     ,  9.      , 20.271269, 14.937274, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.2      ,  4.8      ,  9.       ,  8.7213955, 16.913656 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-81.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2     ,  4.4     , 10.      ,  4.134142, 22.11357 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.3  , 13.5  , 10.   ,  6.812, 43.785, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     , 33.2     , 10.      , 44.01641 , 48.860508, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     , 18.9     , 11.      , 51.47311 , 24.608044, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.5  ,  7.   , 17.   , 44.722, 43.185, -1.   ]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-96.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 26.      , 17.      , 58.175224, 43.73917 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.5     , 18.4     , 23.      , 51.851513, 30.235523, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ae8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.9, 41.7, 6, 0.10651689007654852, 20.249702331225194, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 27.8, 7, 15.539000000002083, 47.317999999999806, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 9.4, 7, 37.012000000002466, 49.8739999999999, 40], 'reward': -3.240000000000009}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 15.0, 8, 17.216516287020735, 39.04480176509001, 39], 'reward': -68.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.9, 17.5, 8, 8.501000000000378, 50.86099999999994, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 27.6, 8, 34.20102352005745, 26.160790388516173, 38], 'reward': 35.95999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b1d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.5, 26.2, 8, 16.147000000001327, 12.042000000000083, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622deb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 17.7, 9, 9.600091904012055, 29.22853197988893, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 31.4, 9, 11.001000000001325, 41.27000000000002, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.6, 27.1, 9, 12.595000000001324, 45.19299999999999, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.3, 5.7, 10, 26.486313956238817, 16.814498465113985, 37], 'reward': -38.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61841c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 31.9, 11, 42.069827384821025, 30.901027061293405, 36], 'reward': 4.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 42.6, 13, 13.509000000001704, 43.029999999999816, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8aa00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 27.8, 13, 13.802000000001705, 9.078999999999981, 35], 'reward': 34.56000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8a3d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.7, 15.3, 13, 5.625000000003031, 28.882000000000087, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b44f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.9, 11.2, 14, 2.7656139877731363, 28.969535971170618, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8acd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.3, 33.9, 14, 6.743000000003221, 33.81700000000005, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 16.0, 14, 12.523979441518122, 27.108079592676198, 34], 'reward': -18.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 4.7, 14, 6.264608235605703, 47.84498354266634, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 27.8, 15, 8.463621507659713, 8.67367987973308, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee7340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.3, 9.8, 15, 21.995909512504635, 30.899291212673795, 33], 'reward': 15.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4d070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 31.0, 15, 17.74622811774927, 59.5470153263454, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4df40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.7, 23.9, 15, 30.027185677906004, 28.908607210724988, 32], 'reward': -30.279999999999973}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f208b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 13.1, 16, 3.964606458624173, 16.252205216325514, 31], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 11.4, 16, 18.360516234004553, 50.06133923721617, 31], 'reward': -69.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4d370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.3, 9.7, 16, 5.91100000000341, 39.23300000000004, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f20e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.8, 7.4, 16, 23.798829670658286, 35.589555148951916, 30], 'reward': -90.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.3, 24.2, 16, 43.22830163412, 29.748422802623942, 29], 'reward': 48.20000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a27ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.5, 25.2, 16, 9.300863694491312, 24.401981858319413, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f9d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 16.8, 17, 7.554000000002274, 24.984999999999864, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bc1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.4, 9.5, 19, 29.59700000000227, 53.854999999999706, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.8, 25.5, 19, 3.850980273802019, 43.56873968278472, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.6, 24.3, 20, 1.5806690097163951, 53.113867530136844, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb0e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.9, 20.9, 20, 1.094949662786636, 43.58732068158452, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebdfa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 15.9, 7, 49.21501450847947, 9.647283737679512, 28], 'reward': 7.359999999999985}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eba520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.4, 10.7, 7, 49.08830998669168, 28.738680590646524, 27], 'reward': -29.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecc4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 40.6, 9, 7.9142791835001205, 36.00341194861859, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee36a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 28.1, 13, 13.750000000001325, 44.10099999999972, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e7b0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.1, 10.1, 16, 49.593390547391785, 41.91964575153836, 26], 'reward': 11.240000000000009}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f65514f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.3, 29.3, 18, 4.136999999999432, 48.655000000000044, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.6, 36.4, 9, 6.6860000000036, 49.974999999999646, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 22.0, 9, 25.318000000001135, 34.36900000000013, 25], 'reward': 42.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f00a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.2, 8.0, 9, 21.629733978466636, 35.41226104128438, 24], 'reward': -36.959999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 5.0, 9, 1.4494150545804763, 33.44615338739398, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 5.0, 10, 1.7355825125689763, 38.473137658948566, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f08b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.5, 9.7, 10, 4.18000000000379, 28.13800000000036, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 6.7, 10, 8.733771671669347, 49.470447225575185, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 14.6, 10, 0.2783423785647763, 32.82687107215835, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.5, 11.9, 11, 22.9503771244637, 49.37326991437243, 23], 'reward': 14.280000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.7, 22.7, 11, 1.5813117226192404, 15.03465216307032, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 9.5, 11, 11.845134103261417, 55.67675860482852, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 6.2, 12, 4.436000000005684, 47.82299999999998, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.2, 26.6, 12, 28.054264813373155, 56.80338837001148, 22], 'reward': -45.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 31.1, 12, 41.710450835139, 21.61292889772411, 21], 'reward': -6.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a80a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.0, 14.7, 14, 10.352000000000567, 47.43599999999979, 20], 'reward': -129.76}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f72e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.8, 8.4, 14, 16.436434170300373, 32.6265885588217, 19], 'reward': -73.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.6, 22.1, 14, 18.849364381961227, 43.49836579164396, 18], 'reward': -28.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdf10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.4, 19.9, 14, 1.954, 42.752, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 0.7, 14, 19.914559611172187, 44.75006173425862, 17], 'reward': -12.720000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 29.5, 14, 31.3235543456149, 45.922964099429535, 16], 'reward': -44.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61afd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.3, 5.1, 16, 8.36982912338616, 50.75836392546525, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 6.1, 16, 7.526000000000189, 55.91899999999994, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.6, 8.3, 17, 14.408347769019946, 42.02351822060705, 15], 'reward': -72.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61bae50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.3, 9.7, 17, 14.099896365300161, 57.118654638636805, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 11.6, 17, 14.83002568769232, 37.44363742347984, 14], 'reward': -41.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63046a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 27.8, 17, 29.80180052207192, 7.779250017426429, 13], 'reward': -28.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.2, 37.6, 17, 43.93354138199351, 45.18972334411226, 12], 'reward': 30.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63028b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.9, 13.3, 17, 17.275635712354823, 50.446336010495514, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.6, 18.6, 18, 5.727999999999241, 50.166999999999824, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631bc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.5, 13.2, 19, 13.761763428283526, 53.56472242654716, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 24.3, 19, 6.162000000003031, 39.92499999999976, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 51.1, 21, 7.3930000000036, 13.99899999999981, 11], 'reward': 131.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617ae80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.0, 13.8, 21, 9.089584833132701, 32.96275865778156, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.4, 18.8, 22, 24.016668241548665, 12.636862431421719, 10], 'reward': 30.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [26.8, 25.2, 23, 19.496604410274607, 10.87729240308802, 9], 'reward': -101.59999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.6, 12.2, 4, 12.937969357756103, 32.97165656377859, 8], 'reward': -80.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61744c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.0, 8.0, 6, 10.805018138378202, 38.40937177561687, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.0, 17.2, 6, 2.3806756844589403, 21.46889145919354, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.1, 8.7, 7, 3.582606222184735, 47.641675553329925, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618cf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 19.1, 7, 21.126692445321087, 54.48343964172262, 7], 'reward': -17.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 10.9, 7, 14.405000000001326, 52.00099999999981, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a5130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 6.0, 7, 9.284000000005685, 54.67300000000032, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.8, 21.3, 7, 41.067324308867605, 41.31393001217854, 6], 'reward': -5.280000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 7.4, 9, 31.144559512166644, 48.3609873298142, 5], 'reward': -23.92}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.9, 13.2, 11, 9.207707110431326, 51.692095870232805, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 12.0, 12, 26.323000000004168, 55.48300000000012, 4], 'reward': -58.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 23.5, 12, 26.939558563250735, 34.88667715559408, 3], 'reward': 43.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b50d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.3, 5.5, 12, 7.494000000001895, 40.19599999999995, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b59d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.7, 17.6, 13, 3.893000000004168, 44.93199999999993, 2], 'reward': -30.039999999999992}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.5, 6.3, 13, 6.587785537274513, 41.80863673087923, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 4.8, 13, 4.232504417663771, 33.54930403616702, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613cc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.4, 8.8, 13, 17.178779986094668, 56.934232223962525, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.6, 5.9, 13, 1.3885347982699159, 49.000602473468426, 1], 'reward': 1501.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 28.1, 13, 24.016203014951504, 24.04384408131447, 0], 'reward': 51.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.4, 15.6, 14, 16.72515213022123, 48.83009287115463, -1], 'reward': -102.39999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.6, 7.7, 14, 3.0952522666768827, 42.61722085997155, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.8, 10.0, 14, 6.897470544029573, 32.14806772076757, -1], 'reward': -75.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62baa30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.1, 32.4, 14, 36.77585016765234, 34.08522794230599, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 29.2, 14, 14.016354850951554, 14.190021222459258, -1], 'reward': 23.400000000000034}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.3, 9.4, 15, 2.0560446519675377, 22.50475705019034, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.3, 33.7, 15, 35.35453795894918, 26.934370600992786, -1], 'reward': 31.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6148850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 23.4, 16, 11.780593893557707, 26.188933250078623, -1], 'reward': 50.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 19.4, 16, 18.956086624665655, 7.776659286771734, -1], 'reward': -35.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60eb0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.6, 17.8, 17, 1.7650358135482218, 34.98279272962623, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6160730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 11.7, 17, 15.341280990291601, 11.856527014169096, -1], 'reward': -65.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61483a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.1, 17.3, 17, 6.470645260641511, 49.105805109496146, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.5, 17.6, 17, 2.210000000002084, 22.582000000000043, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.3, 25.1, 17, 5.881000000003979, 22.97400000000007, -1], 'reward': -57.72000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.1, 5.4, 18, 9.069732125741314, 27.439296654357946, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61487f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 9.9, 18, 3.08821696999055, 38.98517340898783, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653b6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 6.7, 18, 10.607182064229185, 39.05841494810471, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.6, 2.2, 18, 7.1910000000007575, 23.586999999999943, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60facd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 21.5, 19, 17.119770454354537, 48.079327843633806, -1], 'reward': -8.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 24.2, 19, 6.3934533906665205, 20.778369986226377, -1], 'reward': 16.919999999999987}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.0, 4.1, 19, 8.740389112668893, 29.399570395760243, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.3, 4.9, 19, 0.5988983662631178, 11.326875822666139, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cbd00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [27.3, 13.4, 20, 17.37600000000379, 12.13000000000039, -1], 'reward': -142.76}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 12.7, 20, 5.406951488511295, 32.341388268183316, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d3af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.2, 2.0, 21, 2.473764622054804, 24.239323575111158, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dca00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.0, 13.4, 22, 11.122997534154639, 33.64043334138552, -1], 'reward': -72.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d31c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.7, 14.3, 22, 4.754000000005684, 50.39799999999989, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dcee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.6, 5.7, 22, 7.268452340317576, 53.071764621741266, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e02b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.4, 11.8, 23, 18.712652586784664, 31.051753872319118, -1], 'reward': 7.839999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 1.8, 23, 6.1860000000036, 39.04099999999994, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 13.0, 23, 11.8027658540547, 57.81549864766548, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.6, 6.7, 0, 8.236661946709049, 48.00352408374601, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.2, 20.9, 4, 1.6080000000035999, 46.067000000000135, -1], 'reward': 31.52000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62697f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.6, 11.6, 6, 11.415774940441324, 58.13457743383645, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 14.3, 6, 10.559000000002463, 50.39300000000012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606bb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.3, 3.2, 6, 1.2522458716764886, 43.630120227926014, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606ba30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 16.4, 7, 19.020385275170135, 39.720862473492765, -1], 'reward': 0.12000000000003297}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 6.3, 7, 17.776000000000757, 49.36099999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606b3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 6.7, 7, 15.313509529722598, 44.549303583947086, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60732b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.4, 18.2, 8, 46.658958171294756, 54.13786660952019, -1], 'reward': -32.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627e5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.1, 9.0, 9, 58.20236984774, 51.13018188056587, -1], 'reward': -5.8799999999999955}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602aee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.1, 4.8, 13, 55.47480363934269, 45.45819040453883, -1], 'reward': -26.11999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.7, 13.8, 16, 45.142000000000756, 53.407999999999966, -1], 'reward': -28.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffaa00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 25.8, 18, 57.678104113210075, 23.921219853158295, -1], 'reward': 34.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 29.8, 20, 34.09499999999867, 43.90299999999972, -1], 'reward': 67.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ec3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 15.3, 22, 23.27474340815516, 31.27248684779336, -1], 'reward': -56.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.3, 9.2, 22, 5.239999999999999, 48.620999999999874, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ec790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.6, 0.8, 22, 8.615879140452133, 44.50139491412199, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ec490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.7, 23.4, 22, 28.006296510807786, 26.57114700908169, -1], 'reward': -45.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f08940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.1, 9.9, 0, 0.4610000000022737, 45.57499999999984, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f080a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 16.7, 6, 6.653000000000569, 32.00399999999967, -1], 'reward': -15.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0ca60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.2, 5.5, 6, 0.0587159901174199, 37.35202272967051, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.5, 8.9, 7, 7.830341845059009, 20.146654077979377, -1], 'reward': -8.920000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 21.8, 8, 9.872905467281841, 43.76722275735317, -1], 'reward': 45.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f006a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.5, 15.1, 8, 26.315895835282728, 48.856476173763156, -1], 'reward': -36.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f200d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 17.6, 8, 34.41944907250091, 39.00937988088686, -1], 'reward': -10.320000000000022}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 35.8, 9, 22.988948461236404, 1.400265691523451, -1], 'reward': 86.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 10.2, 10, 23.565686467323218, 20.83185417303025, -1], 'reward': -63.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a2f6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.2, 13.8, 11, 4.042000000000568, 18.955999999999957, -1], 'reward': -79.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a278b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.7, 5.3, 11, 8.47667394903849, 32.48496653814409, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a27910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.5, 30.4, 11, 29.14787956653726, 23.222047851843435, -1], 'reward': -35.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a2f5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.9, 36.9, 13, 45.75601157431803, 24.633955506391423, -1], 'reward': -24.039999999999964}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e73d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.3, 47.8, 15, 2.9140000000011366, 43.01000000000005, -1], 'reward': 144.12000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e7340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 7.7, 15, 5.205000000002653, 44.30099999999968, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 10.4, 15, 8.812432385215146, 30.954445962266135, -1], 'reward': -0.04000000000000625}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d8a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 14.0, 15, 15.123818566449986, 35.0468268931544, -1], 'reward': -8.240000000000009}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d81c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.6, 23.0, 15, 6.451293662276672, 52.865221718042164, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4da90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.2, 23.9, 15, 1.5400000000026526, 60.0, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4d340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 28.9, 15, 30.649397748490117, 59.806941864297215, -1], 'reward': 48.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4df10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.6, 11.9, 16, 3.0250000000024633, 40.89299999999982, -1], 'reward': -108.79999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e72b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.9, 24.7, 16, 33.82168592010781, 48.96651905316071, -1], 'reward': -1.8799999999999955}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a60c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.5, 22.7, 17, 24.70476654114568, 20.450697015653333, -1], 'reward': 21.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b30a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.9, 27.3, 17, 6.051322405498638, 58.71683808835638, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b0fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 6.8, 17, 4.149426621160943, 23.718239043797773, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b04c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.2, 11.5, 17, 24.353999999999242, 23.121999999999897, -1], 'reward': -25.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69efdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 3.4, 17, 8.56599999999943, 27.057000000000375, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ef220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [27.0, 7.4, 17, 14.566639091008323, 44.95700305166511, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a60190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.7, 26.4, 17, 16.168618314663718, 36.4032313914964, -1], 'reward': -69.87999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6970ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 6.9, 17, 3.3827638698439344, 49.4946878235197, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6970100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 0.8, 17, 2.174689081584621, 45.14975433464518, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b06a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.0, 23.1, 17, 18.664322278016606, 35.58923167640321, -1], 'reward': -75.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ef8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.1, 10.5, 17, 12.066827449554145, 43.854206614215, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69efaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.1, 5.2, 18, 6.265223515936995, 36.045340622987425, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6970490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.7, 3.3, 18, 0.5487423342637276, 43.80634396517768, -1], 'reward': -103.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6970be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.4, 15.1, 18, 7.180000000001326, 47.37899999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6970760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 9.6, 18, 10.2946192387394, 57.70202617382333, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e0610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.2, 3.9, 18, 1.5420000000026526, 31.248000000000108, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f6c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 33.8, 18, 8.25962863182203, 7.363942210537367, -1], 'reward': 83.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6935250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.5, 43.4, 19, 44.11171759123117, 52.685980995514115, -1], 'reward': 19.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f692f9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.6, 13.9, 19, 8.600000000004357, 57.483000000000075, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f693c130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.3, 12.2, 20, 48.246096317176814, 44.499022795509774, -1], 'reward': 0}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9       , 41.7       ,  6.        ,  0.10651689, 20.249702  ,\n",
      "        40.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.1  , 27.8  ,  7.   , 15.539, 47.318, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.9  ,  9.4  ,  7.   , 37.012, 49.874, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-3.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     , 15.      ,  8.      , 17.216516, 39.044804, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-68.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.9  , 17.5  ,  8.   ,  8.501, 50.861, 38.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7     , 27.6     ,  8.      , 34.201023, 26.16079 , 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  , 26.2  ,  8.   , 16.147, 12.042, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.8     , 17.7     ,  9.      ,  9.600092, 29.228533, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.8  , 31.4  ,  9.   , 11.001, 41.27 , 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.6  , 27.1  ,  9.   , 12.595, 45.193, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.3     ,  5.7     , 10.      , 26.486315, 16.814499, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-38.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4     , 31.9     , 11.      , 42.069828, 30.901028, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=4.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.7  , 42.6  , 13.   , 13.509, 43.03 , 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   , 27.8  , 13.   , 13.802,  9.079, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=34.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.7  , 15.3  , 13.   ,  5.625, 28.882, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 11.2     , 14.      ,  2.765614, 28.969536, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.3  , 33.9  , 14.   ,  6.743, 33.817, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3     , 16.      , 14.      , 12.523979, 27.10808 , 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-18.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2      ,  4.7      , 14.       ,  6.2646084, 47.844982 ,\n",
      "        33.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1     , 27.8     , 15.      ,  8.463621,  8.67368 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.3     ,  9.8     , 15.      , 21.995909, 30.899292, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     , 31.      , 15.      , 17.746227, 59.547016, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.7     , 23.9     , 15.      , 30.027185, 28.908607, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1      , 13.1      , 16.       ,  3.9646065, 16.252205 ,\n",
      "        31.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 11.4     , 16.      , 18.360516, 50.06134 , 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.3  ,  9.7  , 16.   ,  5.911, 39.233, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.8     ,  7.4     , 16.      , 23.79883 , 35.589554, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-90.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.3     , 24.2     , 16.      , 43.228302, 29.748423, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=48.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5     , 25.2     , 16.      ,  9.300863, 24.401981, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.8  , 16.8  , 17.   ,  7.554, 24.985, 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.4  ,  9.5  , 19.   , 29.597, 53.855, 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.8      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        28.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.6     , 24.3     , 20.      ,  1.580669, 53.11387 , 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.9      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        28.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     , 15.9     ,  7.      , 49.215015,  9.647284, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.4    , 10.7    ,  7.     , 49.08831, 28.73868, 27.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-29.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 40.6     ,  9.      ,  7.914279, 36.00341 , 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.5  , 28.1  , 13.   , 13.75 , 44.101, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.1     , 10.1     , 16.      , 49.59339 , 41.919647, 26.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=11.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.3  , 29.3  , 18.   ,  4.137, 48.655, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.6  , 36.4  ,  9.   ,  6.686, 49.975, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.1  , 22.   ,  9.   , 25.318, 34.369, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=42.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     ,  8.      ,  9.      , 21.629734, 35.412262, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-36.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8      ,  5.       ,  9.       ,  1.4494151, 33.44615  ,\n",
      "        23.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.       ,  5.       , 10.       ,  1.7355825, 38.473137 ,\n",
      "        23.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.5  ,  9.7  , 10.   ,  4.18 , 28.138, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8     ,  6.7     , 10.      ,  8.733771, 49.470448, 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.7       , 14.6       , 10.        ,  0.27834237, 32.82687   ,\n",
      "        23.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.5     , 11.9     , 11.      , 22.950377, 49.37327 , 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.7      , 22.7      , 11.       ,  1.5813117, 15.034652 ,\n",
      "        22.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     ,  9.5     , 11.      , 11.845134, 55.676758, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.   ,  6.2  , 12.   ,  4.436, 47.823, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2     , 26.6     , 12.      , 28.054264, 56.803387, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 31.1     , 12.      , 41.71045 , 21.612928, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-6.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[26.   , 14.7  , 14.   , 10.352, 47.436, 20.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-129.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     ,  8.4     , 14.      , 16.436434, 32.626587, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     , 22.1     , 14.      , 18.849365, 43.498367, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.4  , 19.9  , 14.   ,  1.954, 42.752, 17.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.2    ,  0.7    , 14.     , 19.91456, 44.75006, 17.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     , 29.5     , 14.      , 31.323555, 45.922966, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-44.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3     ,  5.1     , 16.      ,  8.369829, 50.758366, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.   ,  6.1  , 16.   ,  7.526, 55.919, 15.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     ,  8.3     , 17.      , 14.408348, 42.023518, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-72.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     ,  9.7     , 17.      , 14.099896, 57.118656, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 11.6     , 17.      , 14.830026, 37.443638, 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2    , 27.8    , 17.     , 29.8018 ,  7.77925, 13.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.2     , 37.6     , 17.      , 43.93354 , 45.189724, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 13.3     , 17.      , 17.275635, 50.446335, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.6  , 18.6  , 18.   ,  5.728, 50.167, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5     , 13.2     , 19.      , 13.761764, 53.564724, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.1  , 24.3  , 19.   ,  6.162, 39.925, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.7  , 51.1  , 21.   ,  7.393, 13.999, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=131.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.      , 13.8     , 21.      ,  9.089585, 32.962757, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     , 18.8     , 22.      , 24.016668, 12.636863, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.8     , 25.2     , 23.      , 19.496605, 10.877293,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-101.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 12.2     ,  4.      , 12.937969, 32.971657,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      ,  8.      ,  6.      , 10.805018, 38.40937 ,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.       , 17.2      ,  6.       ,  2.3806758, 21.468891 ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.1      ,  8.7      ,  7.       ,  3.5826063, 47.641674 ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 19.1     ,  7.      , 21.126692, 54.48344 ,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.3  , 10.9  ,  7.   , 14.405, 52.001,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  ,  6.   ,  7.   ,  9.284, 54.673,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.8     , 21.3     ,  7.      , 41.067326, 41.31393 ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.     ,  7.4    ,  9.     , 31.14456, 48.36099,  5.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-23.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     , 13.2     , 11.      ,  9.207707, 51.692097,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.2  , 12.   , 12.   , 26.323, 55.483,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-58.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     , 23.5     , 12.      , 26.939558, 34.886677,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=43.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.3  ,  5.5  , 12.   ,  7.494, 40.196,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.7  , 17.6  , 13.   ,  3.893, 44.932,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.5      ,  6.3      , 13.       ,  6.5877857, 41.808636 ,\n",
      "         1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.2      ,  4.8      , 13.       ,  4.2325044, 33.549305 ,\n",
      "         1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.4    ,  8.8    , 13.     , 17.17878, 56.93423,  1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.6      ,  5.9      , 13.       ,  1.3885348, 49.000603 ,\n",
      "         1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1501.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 28.1     , 13.      , 24.016203, 24.043844,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=51.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.4     , 15.6     , 14.      , 16.725153, 48.830093, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-102.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6      ,  7.7      , 14.       ,  3.0952523, 42.61722  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8      , 10.       , 14.       ,  6.8974705, 32.148067 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1     , 32.4     , 14.      , 36.77585 , 34.085228, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3      , 29.2      , 14.       , 14.016355 , 14.1900215,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.3      ,  9.4      , 15.       ,  2.0560446, 22.504757 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.3     , 33.7     , 15.      , 35.354538, 26.93437 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=31.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 23.4     , 16.      , 11.780594, 26.188932, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4      , 19.4      , 16.       , 18.956087 ,  7.7766595,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6      , 17.8      , 17.       ,  1.7650359, 34.98279  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.1     , 11.7     , 17.      , 15.341281, 11.856527, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-65.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.1      , 17.3      , 17.       ,  6.4706454, 49.105804 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.5  , 17.6  , 17.   ,  2.21 , 22.582, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.3  , 25.1  , 17.   ,  5.881, 22.974, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-57.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     ,  5.4     , 18.      ,  9.069732, 27.439297, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     ,  9.9     , 18.      ,  3.088217, 38.985172, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1      ,  6.7      , 18.       , 10.6071825, 39.058414 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.6  ,  2.2  , 18.   ,  7.191, 23.587, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4     , 21.5     , 19.      , 17.11977 , 48.079327, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9      , 24.2      , 19.       ,  6.3934536, 20.77837  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=16.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      ,  4.1     , 19.      ,  8.740389, 29.39957 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.3       ,  4.9       , 19.        ,  0.59889835, 11.326876  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[27.3  , 13.4  , 20.   , 17.376, 12.13 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-142.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7      , 12.7      , 20.       ,  5.4069514, 32.34139  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2      ,  2.       , 21.       ,  2.4737647, 24.239323 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.      , 13.4     , 22.      , 11.122997, 33.640434, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-72.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.7  , 14.3  , 22.   ,  4.754, 50.398, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     ,  5.7     , 22.      ,  7.268452, 53.071766, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     , 11.8     , 23.      , 18.712652, 31.051754, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.   ,  1.8  , 23.   ,  6.186, 39.041, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7     , 13.      , 23.      , 11.802766, 57.8155  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     ,  6.7     ,  0.      ,  8.236662, 48.003525, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.2  , 20.9  ,  4.   ,  1.608, 46.067, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=31.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 11.6     ,  6.      , 11.415775, 58.13458 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.4  , 14.3  ,  6.   , 10.559, 50.393, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.3      ,  3.2      ,  6.       ,  1.2522459, 43.63012  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7     , 16.4     ,  7.      , 19.020386, 39.720863, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.3  ,  6.3  ,  7.   , 17.776, 49.361, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  6.7     ,  7.      , 15.31351 , 44.549305, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4     , 18.2     ,  8.      , 46.65896 , 54.137867, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.1    ,  9.     ,  9.     , 58.20237, 51.13018, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     ,  4.8     , 13.      , 55.474804, 45.45819 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.7  , 13.8  , 16.   , 45.142, 53.408, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1     , 25.8     , 18.      , 57.678104, 23.92122 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=34.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.1  , 29.8  , 20.   , 34.095, 43.903, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 15.3     , 22.      , 23.274744, 31.272488, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.3  ,  9.2  , 22.   ,  5.24 , 48.621, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.6     ,  0.8     , 22.      ,  8.615879, 44.501396, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7     , 23.4     , 22.      , 28.006296, 26.571148, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[24.1  ,  9.9  ,  0.   ,  0.461, 45.575, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.1  , 16.7  ,  6.   ,  6.653, 32.004, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.2       ,  5.5       ,  6.        ,  0.05871599, 37.352024  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5     ,  8.9     ,  7.      ,  7.830342, 20.146654, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 21.8     ,  8.      ,  9.872906, 43.767223, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=45.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.5     , 15.1     ,  8.      , 26.315895, 48.856476, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-36.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8    , 17.6    ,  8.     , 34.41945, 39.00938, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1      , 35.8      ,  9.       , 22.988949 ,  1.4002657,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=86.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.2     , 10.2     , 10.      , 23.565687, 20.831854, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.2  , 13.8  , 11.   ,  4.042, 18.956, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-79.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.7     ,  5.3     , 11.      ,  8.476674, 32.484966, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5     , 30.4     , 11.      , 29.147879, 23.222048, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.9     , 36.9     , 13.      , 45.756012, 24.633955, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-24.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.3  , 47.8  , 15.   ,  2.914, 43.01 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=144.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.3  ,  7.7  , 15.   ,  5.205, 44.301, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 10.4     , 15.      ,  8.812432, 30.954447, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     , 14.      , 15.      , 15.123818, 35.046825, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6      , 23.       , 15.       ,  6.4512935, 52.865223 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.2 , 23.9 , 15.  ,  1.54, 60.  , -1.  ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     , 28.9     , 15.      , 30.649397, 59.806942, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=48.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.6  , 11.9  , 16.   ,  3.025, 40.893, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-108.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.9     , 24.7     , 16.      , 33.821686, 48.96652 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.5     , 22.7     , 17.      , 24.704767, 20.450697, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=21.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9      , 27.3      , 17.       ,  6.0513225, 58.71684  ,\n",
      "        -1.       ]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1      ,  6.8      , 17.       ,  4.1494265, 23.718239 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 11.5  , 17.   , 24.354, 23.122, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.2  ,  3.4  , 17.   ,  8.566, 27.057, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[27.      ,  7.4     , 17.      , 14.566639, 44.957005, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.7     , 26.4     , 17.      , 16.16862 , 36.403233, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4      ,  6.9      , 17.       ,  3.3827639, 49.494686 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8     ,  0.8     , 17.      ,  2.174689, 45.149754, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      , 23.1     , 17.      , 18.664322, 35.589233, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1     , 10.5     , 17.      , 12.066828, 43.854206, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1      ,  5.2      , 18.       ,  6.2652235, 36.04534  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7       ,  3.3       , 18.        ,  0.54874235, 43.806343  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-103.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.4  , 15.1  , 18.   ,  7.18 , 47.379, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     ,  9.6     , 18.      , 10.29462 , 57.702026, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.2  ,  3.9  , 18.   ,  1.542, 31.248, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 33.8     , 18.      ,  8.259628,  7.363942, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=83.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5     , 43.4     , 19.      , 44.111717, 52.68598 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=19.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.6  , 13.9  , 19.   ,  8.6  , 57.483, -1.   ]], dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ae490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.6, 20.3, 5, 13.491988683063822, 32.5624276645147, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.3, 8.5, 6, 2.4409720177717364, 37.72140120808937, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63583d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.6, 8.1, 7, 30.307248377046832, 9.518916950897141, 40], 'reward': -73.35999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62893a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.6, 26.2, 8, 16.147000000001327, 12.042000000000083, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ad4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.8, 17.7, 9, 9.600091904012055, 29.22853197988893, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623db50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 31.4, 9, 11.001000000001325, 41.27000000000002, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.9, 27.1, 9, 12.595000000001324, 45.19299999999999, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62221c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [29.6, 16.2, 10, 13.737392133327909, 48.169266317657616, 39], 'reward': -149.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624f580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.3, 11.7, 10, 8.62981637578337, 35.69282830699598, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61c9400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 21.3, 10, 3.8277655810327356, 27.234600753952403, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 11.1, 10, 13.784131778403822, 49.46973811184348, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61741c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 2.9, 11, 15.155713161788594, 46.837880453650946, 38], 'reward': -5.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61678e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.0, 9.4, 11, 14.494000000001705, 57.78899999999988, 37], 'reward': -78.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618cee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.3, 18.4, 11, 7.203164535153548, 37.9354166498884, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6160c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.1, 28.2, 12, 32.924261360674166, 35.65931114524885, 36], 'reward': 75.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.8, 9.0, 12, 7.632744241841025, 57.14468542013553, 35], 'reward': -139.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61435b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 4.1, 12, 17.768258303658097, 50.7063172111772, 34], 'reward': -64.39999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613caf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 22.4, 12, 30.10507698998974, 55.757623460220216, 33], 'reward': -6.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [28.3, 14.2, 12, 5.242856438813538, 45.210399609535266, 32], 'reward': -147.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6160940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.1, 22.8, 12, 35.27245801992316, 56.86596401955208, 31], 'reward': -16.119999999999976}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60acb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.9, 20.9, 12, 13.213000000001704, 51.10199999999982, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 5.2, 14, 24.700000000002273, 54.517999999999866, 30], 'reward': -67.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605de80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 22.8, 14, 34.13483433498063, 41.84636947909125, 29], 'reward': -10.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82f70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 13.8, 14, 6.096000000003221, 44.2699999999998, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e7be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.5, 27.8, 15, 8.463621507659713, 8.67367987973308, 28], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a58d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 7.4, 15, 17.169000000004168, 57.25299999999985, 28], 'reward': -82.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4d0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.4, 4.2, 15, 24.343519240529968, 56.50188414349354, 27], 'reward': -43.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f20460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.3, 4.7, 15, 2.662796353819019, 37.38485278688044, 26], 'reward': -150.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 18.3, 15, 18.266566587294925, 43.544993907619144, 25], 'reward': -46.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4d7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.1, 14.1, 16, 25.315603355157542, 48.77829541360185, 24], 'reward': 3.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a276d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 5.0, 16, 17.369000000002462, 47.77200000000024, 23], 'reward': -32.959999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 9.7, 16, 5.91100000000341, 39.23300000000004, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.8, 26.6, 16, 45.0138546000905, 39.449625288844004, 22], 'reward': 66.07999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.5, 41.0, 17, 13.58700000000019, 36.147999999999875, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.2, 30.8, 17, 9.036209088721225, 37.03642350893827, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6229e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 23.4, 18, 23.418667627265965, 56.86641744265342, 21], 'reward': 34.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63156d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 5.5, 19, 18.94049339665377, 45.916755815150054, 20], 'reward': -98.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f94c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.8, 14.4, 19, 1.937897531352645, 45.08205453318258, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6229dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.8, 15.5, 19, 14.970586992809892, 59.15848111257537, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.1, 8.5, 19, 12.240190197601311, 39.06578978400367, 19], 'reward': 12.920000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.5, 18.5, 19, 4.2998082506770725, 50.62779506014854, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.8, 8.4, 19, 10.684086951205217, 50.605422472607216, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62acac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.8, 9.2, 19, 14.578302449660747, 54.119863255911014, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bc700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 23.9, 19, 38.90108278673601, 39.60107184549784, 18], 'reward': 28.200000000000017}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 25.5, 19, 3.850980273802019, 43.56873968278472, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e2640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.5, 24.7, 19, 1.8069997798133635, 52.3187538622671, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.1, 39.3, 20, 8.363896599348827, 43.753778725841585, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 24.3, 20, 1.5806690097163951, 53.113867530136844, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb0850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 20.9, 20, 1.094949662786636, 43.58732068158452, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e1cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.6, 27.7, 21, 0.4697131324205088, 49.6066487378464, 17], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 15.5, 0, 14.882813430340828, 26.590714016443204, 17], 'reward': -67.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb6d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.1, 3.0, 2, 11.25473646566337, 18.187612647563622, 16], 'reward': -65.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb6310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.0, 2.9, 6, 5.482883423433576, 22.869961634539393, 15], 'reward': -24.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb67f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 9.2, 6, 5.092000000004548, 43.13299999999988, 14], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb6e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 2.5, 6, 7.771000000003221, 14.175999999999878, 14], 'reward': -39.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eba700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 23.5, 7, 32.20119390899255, 31.952465239411442, 13], 'reward': 5.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebd1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.9, 24.7, 8, 7.918525946784081, 50.27705866167909, 12], 'reward': -8.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eba0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 11.8, 8, 7.17937638668108, 53.829088760093775, 11], 'reward': -39.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebdf40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.3, 15.8, 8, 8.152000000000568, 58.751999999999704, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebd580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.5, 7.4, 8, 4.4550000000005685, 29.599999999999884, 10], 'reward': -108.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec21f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.5, 18.1, 8, 0.33100000000132634, 22.36000000000009, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 23.2, 8, 28.45454160449089, 33.96385347563494, 9], 'reward': 49.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.2, 2.9, 9, 14.563365464546159, 42.924822379194055, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.9, 38.6, 9, 3.0539110335721737, 53.028310710166735, 8], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 19.5, 9, 22.388392076972814, 56.809384984900575, 8], 'reward': 29.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec9bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 4.1, 9, 2.570436829787198, 59.55160901330712, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eccaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 24.7, 9, 28.58093860242918, 44.82430169013563, 7], 'reward': -13.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec92e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.8, 14.6, 9, 3.286248095436692, 38.38435630413236, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecca90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 40.6, 9, 7.9142791835001205, 36.00341194861859, 6], 'reward': 0.03999999999996362}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec97c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 22.4, 10, 16.053389565001353, 49.21357116073922, 5], 'reward': 24.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.5, 15.9, 10, 23.197360606371596, 33.73850363082886, 4], 'reward': -0.11999999999997613}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecff70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 24.6, 11, 15.685259452313689, 46.89103358682864, 3], 'reward': -51.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecf490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.9, 18.0, 11, 1.726999999999621, 34.19399999999988, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.7, 19.3, 11, 22.234647550520613, 40.34321183881795, 2], 'reward': -58.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecf160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.6, 16.3, 11, 11.296470932130593, 30.421298845137667, 1], 'reward': 1432.48}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecf5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.4, 4.9, 11, 2.3550000000036, 41.55699999999985, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecff40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.4, 4.2, 11, 4.726856037882229, 53.792973203940846, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 17.5, 11, 18.272146289731737, 31.116882824184422, 0], 'reward': -41.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.6, 10.5, 11, 28.6518058024167, 34.32947525639574, -1], 'reward': 15.920000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ecf700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 6.6, 12, 6.215999999999242, 43.3749999999997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edca00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 16.9, 12, 5.156000000000379, 22.69899999999989, -1], 'reward': -84.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed4760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 10.5, 12, 23.248999999998674, 28.672000000000157, -1], 'reward': -29.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed5b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 5.2, 12, 3.8328973253790424, 40.497296556322574, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.1, 14.2, 13, 19.85352094630234, 56.92220276442115, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed5ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.5, 9.6, 13, 4.064466080794417, 4.297864412570231, -1], 'reward': -115.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed50d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.3, 15.9, 13, 20.3684161612031, 13.278164372100695, -1], 'reward': -59.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ed59d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 35.3, 13, 49.57523742253712, 34.861514027406386, -1], 'reward': 53.12000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6740460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 27.9, 17, 8.639843440076842, 48.57523737894558, -1], 'reward': -35.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.6, 10.8, 17, 11.597007850642193, 54.59538428688107, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 43.3, 17, 45.34588106046224, 39.58508447204531, -1], 'reward': 100.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.7, 29.3, 18, 4.136999999999432, 48.655000000000044, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fed00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.2, 36.4, 9, 6.6860000000036, 49.974999999999646, -1], 'reward': 60.72000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eadc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.8, 3.6, 9, 2.320847698105544, 32.86548016145754, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635ea30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.6, 25.8, 9, 29.353512541961425, 46.42960266568287, -1], 'reward': 24.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.6, 9.7, 10, 4.18000000000379, 28.13800000000036, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.1, 14.0, 10, 41.64730561398055, 42.954571139903535, -1], 'reward': 23.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.2, 9.5, 11, 11.845134103261417, 55.67675860482852, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 18.1, 13, 24.859000000001323, 49.553000000000196, -1], 'reward': 36.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 9.5, 13, 17.188610522826714, 47.560267175955985, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.4, 18.6, 13, 0.2676724783503621, 54.73096684876428, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.2, 19.6, 13, 25.2161773339825, 27.83951818545267, -1], 'reward': -88.23999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.1, 28.4, 13, 7.449000000001326, 53.212000000000344, -1], 'reward': 56.20000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.5, 26.6, 13, 7.8044516592294695, 25.682540951065338, -1], 'reward': 74.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 30.3, 13, 37.75048243641097, 26.303656612092283, -1], 'reward': 75.20000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 14.7, 14, 10.352000000000567, 47.43599999999979, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63040a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.1, 24.7, 15, 6.222000000003032, 9.88600000000022, -1], 'reward': -64.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 8.7, 15, 7.347433567838012, 31.61239230007902, -1], 'reward': -75.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 19.6, 15, 22.6764645371011, 30.718865746406614, -1], 'reward': 24.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fd430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.4, 1.3, 15, 11.11881248765553, 25.350073919573443, -1], 'reward': -80.16000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a8250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 19.6, 15, 10.14335831100631, 4.721586046252273, -1], 'reward': 26.67999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdd30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 11.3, 15, 15.927142732085786, 13.21844946662825, -1], 'reward': 13.719999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a8460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.9, 13.9, 15, 4.226430428000665, 17.636502491699108, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.6, 18.9, 16, 17.32675301275891, 28.404600662482075, -1], 'reward': -52.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.3, 15.5, 16, 23.88113764351563, 38.315926286365155, -1], 'reward': -47.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61afac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.3, 39.3, 16, 34.32467107830041, 50.64648474189261, -1], 'reward': -32.67999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61af640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.4, 13.3, 17, 17.275635712354823, 50.446336010495514, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61bea00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.3, 32.4, 17, 10.69299999999981, 30.14899999999984, -1], 'reward': 94.84000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.5, 15.5, 17, 14.203102644186433, 35.862904953862504, -1], 'reward': -15.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.3, 14.4, 17, 5.1060000000036, 48.91499999999956, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.0, 23.8, 17, 4.62970295890791, 20.432978596050532, -1], 'reward': -5.439999999999969}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.0, 5.7, 18, 5.4590000000036, 42.11400000000024, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.8, 33.4, 18, 23.270267876715565, 56.98262311005284, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631b2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 1.7, 18, 4.910000000001327, 29.41799999999991, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.9, 11.5, 18, 2.8550000000009472, 32.046999999999976, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61be130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 12.4, 18, 6.5200000000036, 35.187999999999626, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.6, 38.9, 19, 41.75729897038951, 15.284267920803451, -1], 'reward': 59.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 15.4, 20, 28.969966569606825, 8.419716760756131, -1], 'reward': 24.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630dfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 17.7, 20, 4.039000000003221, 24.648000000000394, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 35.4, 7, 5.0670000000030315, 35.67499999999977, -1], 'reward': 58.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a57c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.1, 2.8, 7, 5.751999999999241, 35.48399999999965, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.0, 10.0, 8, 14.516660298923199, 52.80300417232819, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61925e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.9, 18.2, 8, 9.644723575060627, 24.28021474334912, -1], 'reward': 18.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ade20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.5, 14.1, 8, 10.432879276525648, 45.311624523849396, -1], 'reward': -26.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62adb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.7, 0.9, 8, 13.21900000000322, 45.173000000000044, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ae5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.3, 21.3, 8, 5.010937811024361, 59.52116440113012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ad970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.9, 21.0, 8, 0.4213554665351449, 25.726687751319602, -1], 'reward': 54.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ae130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.4, 1.4, 9, 6.954778597737881, 45.059256374689, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.5, 5.2, 9, 3.292191453040761, 38.48657322540579, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.0, 10.7, 9, 11.6073880871141, 53.13754794234947, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6938100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.9, 15.9, 10, 2.002000000001516, 47.94200000000036, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.9, 7.7, 10, 2.4760000000056843, 38.546999999999635, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 7.1, 10, 7.389800479710109, 44.43036956959282, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619f7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 13.2, 11, 9.207707110431326, 51.692095870232805, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 6.6, 11, 8.858457245244116, 24.94188650235271, -1], 'reward': -14.919999999999987}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612dca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.5, 5.7, 11, 2.154568900143198, 39.4968255537382, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619f160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.4, 4.6, 11, 12.40052270342308, 32.93502040161102, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61331c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 23.5, 12, 28.152961752331596, 26.677676623754248, -1], 'reward': 26.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 16.4, 12, 10.042000000000757, 35.09200000000019, -1], 'reward': 28.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.8, 9.5, 12, 3.9489075321998555, 19.336140007530577, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62b5310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 31.3, 12, 25.17740112627912, 54.91598654717494, -1], 'reward': 55.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.7, 6.3, 13, 6.587785537274513, 41.80863673087923, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ba5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.9, 8.8, 13, 17.178779986094668, 56.934232223962525, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.2, 8.5, 13, 11.63944815500096, 36.2452282207478, -1], 'reward': -137.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bd5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.8, 26.6, 13, 22.014578947687582, 15.544177124031584, -1], 'reward': 59.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d07c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.8, 15.6, 14, 30.16430232371533, 31.91222505797672, -1], 'reward': 30.88000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bdf70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 4.2, 14, 19.74500000000057, 33.30900000000038, -1], 'reward': -40.959999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.7, 20.2, 14, 7.168000000002463, 52.08600000000031, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bdfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 16.8, 14, 20.766587906155607, 39.07175969854896, -1], 'reward': -63.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613c100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.6, 13.6, 14, 3.5540000000018948, 46.73000000000007, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c58e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.0, 27.6, 14, 34.44549622222046, 52.03949750676699, -1], 'reward': -6.8799999999999955}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.7, 15.8, 14, 4.741000000005684, 58.52399999999958, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6160b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.9, 18.6, 15, 12.945000000000757, 55.52699999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 4.4, 16, 15.500230591423035, 49.47306256057078, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611f850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 28.2, 17, 3.429000000005684, 38.322000000000266, -1], 'reward': 50.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611ff70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.2, 5.8, 17, 4.241665609516133, 46.82404973813961, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 6.1, 17, 5.874465894788705, 50.74712715483066, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 8.9, 17, 12.346000000000188, 49.94999999999997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a8b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.8, 8.6, 17, 11.274489728311599, 36.55086893080491, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653b970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 14.9, 17, 19.577033452765303, 45.82954087333113, -1], 'reward': 22.519999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f611fa30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.0, 19.6, 17, 7.1699999999981054, 28.067000000000085, -1], 'reward': 8.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f610e790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 11.8, 17, 4.5670000000030315, 58.05199999999964, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a85e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.8, 11.3, 17, 4.636637551604974, 11.055696805485402, -1], 'reward': -10.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ebfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.7, 5.4, 18, 9.069732125741314, 27.439296654357946, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a87c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.6, 28.5, 18, 29.33451803981053, 24.17822666856817, -1], 'reward': 73.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ac640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.5, 15.7, 19, 0.24998997702861203, 31.80100361556505, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dcbb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.6, 8.7, 22, 38.48966307307886, 17.15770843752321, -1], 'reward': -44.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6269910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.3, 19.9, 6, 45.67307393531625, 16.453703628958614, -1], 'reward': -26.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60737f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 30.0, 8, 16.384000000000757, 25.77400000000017, -1], 'reward': 62.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6073cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.9, 12.9, 8, 19.821814310302948, 39.07965586837393, -1], 'reward': -39.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60737c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.9, 21.0, 8, 5.8940000000030315, 60.0, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 14.4, 8, 17.397774649992236, 40.34161619674512, -1], 'reward': -70.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60730a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.1, 16.2, 8, 17.540492262851497, 33.68654497332218, -1], 'reward': -91.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e00d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 23.9, 8, 29.988453727783, 24.36224684931151, -1], 'reward': 15.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 23.0, 8, 7.559978753716798, 43.36409527598523, -1], 'reward': 26.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6073340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.7, 2.1, 8, 4.57434171427992, 47.35755548930594, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 11.7, 8, 11.262052862355556, 42.65432802727178, -1], 'reward': -25.799999999999983}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 18.6, 9, 24.65019705646101, 34.994120064825225, -1], 'reward': 37.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.3, 16.0, 9, 16.986230990523794, 42.974316730617105, -1], 'reward': -86.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6272a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 11.5, 9, 27.29499999999981, 39.90900000000002, -1], 'reward': -15.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627e550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.4, 14.6, 10, 9.366489534398541, 45.55632831829444, -1], 'reward': -78.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627eb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 32.2, 10, 36.52193090482896, 56.26030402358482, -1], 'reward': -0.32000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627efa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 14.5, 10, 5.588000000002653, 49.96399999999985, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626ca00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.5, 9.0, 12, 10.799000000004547, 49.99199999999974, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626cdf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 23.5, 12, 18.79871899783717, 37.54433828015019, -1], 'reward': 41.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626c1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 11.8, 12, 9.061662342388663, 42.80246776491761, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f609f6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.4, 23.5, 12, 8.355, 54.62099999999999, -1], 'reward': 11.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.4, 11.1, 12, 7.997000000000189, 43.45799999999987, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f609f0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.8, 11.1, 12, 2.0770000000022737, 56.58600000000029, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62772b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 42.1, 12, 29.199885018377202, 28.46948623158769, -1], 'reward': 74.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.2, 28.2, 13, 11.853000000004545, 46.16600000000034, -1], 'reward': 61.68000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602acd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.9, 27.7, 13, 28.65114927951237, 34.34009748012372, -1], 'reward': 14.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62774c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.8, 8.0, 13, 3.153656236232213, 54.85027510142165, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62859a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.6, 14.6, 13, 3.9480000000026525, 56.012999999999934, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62852e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.5, 12.5, 13, 7.016960711096596, 47.86463917731546, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.9, 16.5, 13, 17.626000000001515, 53.86600000000021, -1], 'reward': -75.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.0, 13.9, 13, 22.880426719617944, 41.371509746656535, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.2, 8.5, 13, 8.214994349986075, 44.22185060989747, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.7, 23.9, 13, 43.85813007600878, 48.72549410445133, -1], 'reward': -3.0799999999999557}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 44.1, 13, 10.64438765882013, 16.642057286321364, -1], 'reward': 126.15999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 7.2, 14, 4.792361261442191, 18.856117309241952, -1], 'reward': -25.92}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62925b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.0, 19.6, 14, 6.672000000002653, 27.261999999999713, -1], 'reward': -12.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60300d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 14.3, 14, 10.66903226162701, 51.26937314070045, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6289790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.5, 8.0, 14, 11.951423588483044, 15.078790502406031, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62857c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.3, 24.4, 15, 25.76987834569475, 31.23238183273209, -1], 'reward': -66.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.7, 18.1, 15, 8.329783083398734, 31.27173191110715, -1], 'reward': -89.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.3, 2.8, 15, 5.422686478751402, 34.68373985538616, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 22.3, 15, 4.88228230915746, 23.063402990907424, -1], 'reward': -32.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 8.0, 15, 4.695619267631096, 46.60017998159048, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 15.7, 15, 20.96890892400073, 26.463218856315535, -1], 'reward': 10.800000000000011}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 26.9, 16, 1.7800010348864213, 8.461437604854957, -1], 'reward': -11.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608ffd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 6.6, 16, 13.247023489736238, 10.860769989629492, -1], 'reward': -46.2}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 22.6, 16, 24.729244199197403, 35.236351271981405, -1], 'reward': -5.8799999999999955}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605dfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [27.5, 17.4, 16, 21.90858295766533, 47.44856114977631, -1], 'reward': -131.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d3d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 12.7, 16, 37.51752885526868, 47.625664884157615, -1], 'reward': 10.040000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.3, 13.7, 17, 7.790685851395308, 43.67145907666459, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.6, 33.1, 17, 49.47043419467321, 26.83528913855089, -1], 'reward': 20.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [30.2, 22.6, 9, 15.404936680605106, 24.843702768312273, -1], 'reward': -133.03999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.1, 30.1, 9, 34.751750215209356, 33.77519851860972, -1], 'reward': -26.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.6, 13.5, 10, 6.812000000003789, 43.78500000000013, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 15.9, 11, 36.454898764678525, 13.47165389250001, -1], 'reward': 14.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.4, 19.5, 12, 16.45586053440593, 43.943555486618656, -1], 'reward': -55.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 5.8, 12, 6.547379612672773, 52.39811635633872, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.9, 10.5, 12, 10.686477194285144, 35.1161852244524, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd80d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.4, 22.9, 12, 38.460895756790904, 44.75626749067631, -1], 'reward': 43.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6cc70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.3, 21.0, 12, 4.2680000000009475, 44.04300000000001, -1], 'reward': -43.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 4.6, 12, 8.927583053658239, 37.58655240268369, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.0, 5.3, 12, 8.274000000004737, 45.45299999999982, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 5.0, 12, 19.26700000000038, 34.09000000000013, -1], 'reward': -75.8}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 6.6, 12, 26.880000000001324, 28.134000000000018, -1], 'reward': -48.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 16.2, 13, 37.13357727212786, 39.17766234610977, -1], 'reward': -63.079999999999956}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 2.0, 13, 22.440135416032255, 49.24714636857031, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6167040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.0, 25.0, 13, 4.734000000003789, 54.42099999999974, -1], 'reward': -28.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.7, 27.6, 13, 4.902000000000569, 31.64599999999979, -1], 'reward': 1.9600000000000364}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 6.8, 13, 14.78699999999943, 46.25700000000028, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61dd730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 15.1, 13, 11.645492539322968, 48.86764871513695, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f822e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.8, 7.7, 13, 2.4204350532305106, 17.458433409814997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.3, 8.9, 13, 15.295227520848677, 28.163638184857057, -1], 'reward': 12.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6ceb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 3.3, 13, 19.53361820281583, 32.513104910713075, -1], 'reward': -50.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ddfd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.1, 32.6, 13, 36.76445999405379, 18.658755794235706, -1], 'reward': 22.039999999999964}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 7.8, 15, 27.874800035575497, 24.77481351887645, -1], 'reward': -28.08}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.2, 2.6, 15, 23.742004685542813, 45.311062014599976, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.9, 20.3, 16, 6.1697933083871135, 31.528598336299456, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8a100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.5, 21.7, 17, 4.59000000000379, 21.83199999999983, -1], 'reward': 18.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 18.1, 17, 12.928518268395774, 40.51239580751816, -1], 'reward': 25.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625fdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.3, 16.3, 17, 32.17400000000246, 53.508000000000045, -1], 'reward': -24.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 23.0, 17, 17.607745848501736, 31.499516361139978, -1], 'reward': 29.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.1, 8.0, 17, 2.4738646510867874, 42.45796508777647, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.5, 39.0, 17, 42.86125146681667, 51.075732118544174, -1], 'reward': 73.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 24.8, 17, 27.3653043408266, 32.13064640634638, -1], 'reward': -50.52000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 12.9, 18, 5.482533497719135, 29.1806900875728, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62581c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.5, 20.2, 18, 33.013520274434484, 10.277929236382347, -1], 'reward': -54.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f5a640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 13.8, 19, 42.40422969500227, 17.686252969383922, -1], 'reward': 22.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0cfa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.6, 44.1, 7, 2.8170000000028423, 49.677999999999706, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a91970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.3, 31.3, 9, 45.07118513948163, 51.91255660402931, -1], 'reward': 30.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4de20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.6, 18.9, 15, 45.13988960086748, 44.678754851791695, -1], 'reward': -45.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e0550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.1, 14.9, 17, 47.590588750013296, 46.89869214474912, -1], 'reward': -75.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6935370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.2, 12.2, 20, 48.246096317176814, 44.499022795509774, -1], 'reward': -37.119999999999976}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6     , 20.3     ,  5.      , 13.491989, 32.562428, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3     ,  8.5     ,  6.      ,  2.440972, 37.7214  , 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     ,  8.1     ,  7.      , 30.30725 ,  9.518917, 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.6  , 26.2  ,  8.   , 16.147, 12.042, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.8     , 17.7     ,  9.      ,  9.600092, 29.228533, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.8  , 31.4  ,  9.   , 11.001, 41.27 , 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.9  , 27.1  ,  9.   , 12.595, 45.193, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[29.6     , 16.2     , 10.      , 13.737392, 48.169266, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-149.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.3     , 11.7     , 10.      ,  8.629816, 35.69283 , 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3      , 21.3      , 10.       ,  3.8277655, 27.2346   ,\n",
      "        38.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.      , 11.1     , 10.      , 13.784132, 49.469738, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.2     ,  2.9     , 11.      , 15.155713, 46.83788 , 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.   ,  9.4  , 11.   , 14.494, 57.789, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-78.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3      , 18.4      , 11.       ,  7.2031646, 37.935417 ,\n",
      "        36.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.1     , 28.2     , 12.      , 32.924263, 35.65931 , 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=75.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.8      ,  9.       , 12.       ,  7.6327443, 57.144684 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-139.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4     ,  4.1     , 12.      , 17.76826 , 50.706318, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-64.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 22.4     , 12.      , 30.105078, 55.75762 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-6.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[28.3      , 14.2      , 12.       ,  5.2428565, 45.2104   ,\n",
      "        32.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-147.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.1     , 22.8     , 12.      , 35.272457, 56.865963, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-16.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.9  , 20.9  , 12.   , 13.213, 51.102, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.4  ,  5.2  , 14.   , 24.7  , 54.518, 30.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     , 22.8     , 14.      , 34.134834, 41.84637 , 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.6  , 13.8  , 14.   ,  6.096, 44.27 , 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5     , 27.8     , 15.      ,  8.463621,  8.67368 , 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.6  ,  7.4  , 15.   , 17.169, 57.253, 28.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-82.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     ,  4.2     , 15.      , 24.34352 , 56.501884, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.3      ,  4.7      , 15.       ,  2.6627963, 37.384853 ,\n",
      "        26.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-150.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 18.3     , 15.      , 18.266567, 43.544994, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     , 14.1     , 16.      , 25.315603, 48.778294, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=3.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.2  ,  5.   , 16.   , 17.369, 47.772, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.9  ,  9.7  , 16.   ,  5.911, 39.233, 22.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.8     , 26.6     , 16.      , 45.013855, 39.449627, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=66.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.5  , 41.   , 17.   , 13.587, 36.148, 21.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2     , 30.8     , 17.      ,  9.036209, 37.036423, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      , 23.4     , 18.      , 23.418667, 56.866417, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=34.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     ,  5.5     , 19.      , 18.940493, 45.916756, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-98.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.8      , 14.4      , 19.       ,  1.9378976, 45.082054 ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.8     , 15.5     , 19.      , 14.970587, 59.15848 , 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.1      ,  8.5      , 19.       , 12.2401905, 39.06579  ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.5     , 18.5     , 19.      ,  4.299808, 50.627796, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     ,  8.4     , 19.      , 10.684087, 50.605423, 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.8     ,  9.2     , 19.      , 14.578302, 54.11986 , 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1    , 23.9    , 19.     , 38.90108, 39.60107, 18.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=28.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        17.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.5      , 24.7      , 19.       ,  1.8069998, 52.318752 ,\n",
      "        17.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1     , 39.3     , 20.      ,  8.363896, 43.75378 , 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 24.3     , 20.      ,  1.580669, 53.11387 , 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        17.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.6       , 27.7       , 21.        ,  0.46971312, 49.606647  ,\n",
      "        17.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2     , 15.5     ,  0.      , 14.882813, 26.590714, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1     ,  3.      ,  2.      , 11.254737, 18.187613, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-65.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.       ,  2.9      ,  6.       ,  5.4828835, 22.86996  ,\n",
      "        15.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-24.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.7  ,  9.2  ,  6.   ,  5.092, 43.133, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.   ,  2.5  ,  6.   ,  7.771, 14.176, 14.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2     , 23.5     ,  7.      , 32.201195, 31.952465, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=5.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9     , 24.7     ,  8.      ,  7.918526, 50.277058, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4      , 11.8      ,  8.       ,  7.1793766, 53.82909  ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.3  , 15.8  ,  8.   ,  8.152, 58.752, 10.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.5  ,  7.4  ,  8.   ,  4.455, 29.6  , 10.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-108.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.5  , 18.1  ,  8.   ,  0.331, 22.36 ,  9.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 23.2     ,  8.      , 28.454542, 33.963852,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=49.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2     ,  2.9     ,  9.      , 14.563366, 42.924824,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9     , 38.6     ,  9.      ,  3.053911, 53.02831 ,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 19.5     ,  9.      , 22.388391, 56.809383,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7      ,  4.1      ,  9.       ,  2.5704367, 59.55161  ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6     , 24.7     ,  9.      , 28.580938, 44.824303,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-13.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.8      , 14.6      ,  9.       ,  3.2862482, 38.384357 ,\n",
      "         6.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     , 40.6     ,  9.      ,  7.914279, 36.00341 ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9    , 22.4    , 10.     , 16.05339, 49.21357,  5.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.5     , 15.9     , 10.      , 23.197361, 33.738503,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     , 24.6     , 11.      , 15.68526 , 46.891033,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.9  , 18.   , 11.   ,  1.727, 34.194,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7     , 19.3     , 11.      , 22.234648, 40.343212,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-58.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 16.3     , 11.      , 11.296471, 30.421299,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1432.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.4  ,  4.9  , 11.   ,  2.355, 41.557,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.4     ,  4.2     , 11.      ,  4.726856, 53.792973,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4     , 17.5     , 11.      , 18.272146, 31.116882,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.6     , 10.5     , 11.      , 28.651806, 34.329475, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.7  ,  6.6  , 12.   ,  6.216, 43.375, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.4  , 16.9  , 12.   ,  5.156, 22.699, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-84.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.3  , 10.5  , 12.   , 23.249, 28.672, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-29.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.6      ,  5.2      , 12.       ,  3.8328974, 40.497295 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.1     , 14.2     , 13.      , 19.853521, 56.922203, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.5      ,  9.6      , 13.       ,  4.064466 ,  4.2978644,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-115.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.3     , 15.9     , 13.      , 20.368416, 13.278164, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-59.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8     , 35.3     , 13.      , 49.575237, 34.861515, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=53.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.3     , 27.9     , 17.      ,  8.639843, 48.575237, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 10.8     , 17.      , 11.597008, 54.595383, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 43.3     , 17.      , 45.345882, 39.585083, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=100.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.7  , 29.3  , 18.   ,  4.137, 48.655, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.2  , 36.4  ,  9.   ,  6.686, 49.975, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=60.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.8      ,  3.6      ,  9.       ,  2.3208477, 32.86548  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.6     , 25.8     ,  9.      , 29.353512, 46.429604, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.6  ,  9.7  , 10.   ,  4.18 , 28.138, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.1     , 14.      , 10.      , 41.647305, 42.95457 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.2     ,  9.5     , 11.      , 11.845134, 55.676758, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.2  , 18.1  , 13.   , 24.859, 49.553, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=36.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4    ,  9.5    , 13.     , 17.18861, 47.56027, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4       , 18.6       , 13.        ,  0.26767248, 54.73097   ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.2     , 19.6     , 13.      , 25.216177, 27.839518, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-88.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.1  , 28.4  , 13.   ,  7.449, 53.212, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=56.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.5      , 26.6      , 13.       ,  7.8044515, 25.68254  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=74.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2     , 30.3     , 13.      , 37.75048 , 26.303658, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=75.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.   , 14.7  , 14.   , 10.352, 47.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.1  , 24.7  , 15.   ,  6.222,  9.886, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-64.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2      ,  8.7      , 15.       ,  7.3474336, 31.612392 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 19.6     , 15.      , 22.676464, 30.718866, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.4     ,  1.3     , 15.      , 11.118813, 25.350075, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 19.6     , 15.      , 10.143358,  4.721586, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3     , 11.3     , 15.      , 15.927143, 13.21845 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.9      , 13.9      , 15.       ,  4.2264304, 17.636503 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.6     , 18.9     , 16.      , 17.326754, 28.4046  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3     , 15.5     , 16.      , 23.881138, 38.315926, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3     , 39.3     , 16.      , 34.324673, 50.646484, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4     , 13.3     , 17.      , 17.275635, 50.446335, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.3  , 32.4  , 17.   , 10.693, 30.149, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=94.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.5     , 15.5     , 17.      , 14.203103, 35.862904, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.3  , 14.4  , 17.   ,  5.106, 48.915, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 23.8     , 17.      ,  4.629703, 20.432978, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.   ,  5.7  , 18.   ,  5.459, 42.114, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     , 33.4     , 18.      , 23.270267, 56.982624, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.   ,  1.7  , 18.   ,  4.91 , 29.418, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.9  , 11.5  , 18.   ,  2.855, 32.047, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.6  , 12.4  , 18.   ,  6.52 , 35.188, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 38.9     , 19.      , 41.757298, 15.284268, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=59.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     , 15.4     , 20.      , 28.969967,  8.419717, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=24.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  , 17.7  , 20.   ,  4.039, 24.648, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   , 35.4  ,  7.   ,  5.067, 35.675, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=58.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.1  ,  2.8  ,  7.   ,  5.752, 35.484, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 10.      ,  8.      , 14.516661, 52.803005, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9     , 18.2     ,  8.      ,  9.644724, 24.280214, -1.      ]],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=18.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.5     , 14.1     ,  8.      , 10.432879, 45.311623, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.7  ,  0.9  ,  8.   , 13.219, 45.173, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.3      , 21.3      ,  8.       ,  5.0109377, 59.521164 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.9       , 21.        ,  8.        ,  0.42135546, 25.726688  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=54.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.4      ,  1.4      ,  9.       ,  6.9547787, 45.059258 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.5      ,  5.2      ,  9.       ,  3.2921915, 38.486572 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.       , 10.7      ,  9.       , 11.6073885, 53.137547 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.9  , 15.9  , 10.   ,  2.002, 47.942, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.9  ,  7.7  , 10.   ,  2.476, 38.547, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4      ,  7.1      , 10.       ,  7.3898005, 44.43037  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     , 13.2     , 11.      ,  9.207707, 51.692097, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     ,  6.6     , 11.      ,  8.858458, 24.941887, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-14.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.5     ,  5.7     , 11.      ,  2.154569, 39.496826, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     ,  4.6     , 11.      , 12.400522, 32.93502 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2     , 23.5     , 12.      , 28.152962, 26.677677, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.6  , 16.4  , 12.   , 10.042, 35.092, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=28.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.8      ,  9.5      , 12.       ,  3.9489076, 19.33614  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     , 31.3     , 12.      , 25.1774  , 54.915985, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=55.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.7      ,  6.3      , 13.       ,  6.5877857, 41.808636 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9    ,  8.8    , 13.     , 17.17878, 56.93423, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.2     ,  8.5     , 13.      , 11.639448, 36.245228, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-137.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.8     , 26.6     , 13.      , 22.01458 , 15.544177, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=59.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.8     , 15.6     , 14.      , 30.164303, 31.912226, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   ,  4.2  , 14.   , 19.745, 33.309, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-40.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.7  , 20.2  , 14.   ,  7.168, 52.086, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2     , 16.8     , 14.      , 20.766588, 39.07176 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.6  , 13.6  , 14.   ,  3.554, 46.73 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.      , 27.6     , 14.      , 34.445496, 52.039497, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-6.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.7  , 15.8  , 14.   ,  4.741, 58.524, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.9  , 18.6  , 15.   , 12.945, 55.527, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     ,  4.4     , 16.      , 15.500231, 49.473064, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.8  , 28.2  , 17.   ,  3.429, 38.322, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.2     ,  5.8     , 17.      ,  4.241666, 46.82405 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7     ,  6.1     , 17.      ,  5.874466, 50.747128, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.7  ,  8.9  , 17.   , 12.346, 49.95 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.8     ,  8.6     , 17.      , 11.274489, 36.55087 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     , 14.9     , 17.      , 19.577034, 45.82954 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.   , 19.6  , 17.   ,  7.17 , 28.067, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=8.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.6  , 11.8  , 17.   ,  4.567, 58.052, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.8      , 11.3      , 17.       ,  4.6366377, 11.0556965,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.7     ,  5.4     , 18.      ,  9.069732, 27.439297, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.6     , 28.5     , 18.      , 29.334518, 24.178226, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=73.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.5       , 15.7       , 19.        ,  0.24998997, 31.801004  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     ,  8.7     , 22.      , 38.489662, 17.15771 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-44.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.3     , 19.9     ,  6.      , 45.673073, 16.453703, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.9  , 30.   ,  8.   , 16.384, 25.774, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=62.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.9     , 12.9     ,  8.      , 19.821814, 39.079655, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.9  , 21.   ,  8.   ,  5.894, 60.   , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     , 14.4     ,  8.      , 17.397774, 40.341618, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-70.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.1     , 16.2     ,  8.      , 17.540493, 33.686546, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-91.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9     , 23.9     ,  8.      , 29.988453, 24.362247, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=15.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9     , 23.      ,  8.      ,  7.559979, 43.364094, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     ,  2.1     ,  8.      ,  4.574342, 47.357555, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     , 11.7     ,  8.      , 11.262053, 42.654327, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2     , 18.6     ,  9.      , 24.650198, 34.99412 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=37.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.3     , 16.      ,  9.      , 16.98623 , 42.974316, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-86.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.7  , 11.5  ,  9.   , 27.295, 39.909, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.4     , 14.6     , 10.      ,  9.366489, 45.556328, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-78.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2     , 32.2     , 10.      , 36.52193 , 56.260303, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.   , 14.5  , 10.   ,  5.588, 49.964, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.5  ,  9.   , 12.   , 10.799, 49.992, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9    , 23.5    , 12.     , 18.79872, 37.54434, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=41.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 11.8     , 12.      ,  9.061663, 42.802467, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.4  , 23.5  , 12.   ,  8.355, 54.621, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=11.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.4  , 11.1  , 12.   ,  7.997, 43.458, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.8  , 11.1  , 12.   ,  2.077, 56.586, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8     , 42.1     , 12.      , 29.199884, 28.469486, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=74.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.2  , 28.2  , 13.   , 11.853, 46.166, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=61.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.9    , 27.7    , 13.     , 28.65115, 34.3401 , -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.8      ,  8.       , 13.       ,  3.1536562, 54.850277 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.6  , 14.6  , 13.   ,  3.948, 56.013, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5      , 12.5      , 13.       ,  7.0169606, 47.86464  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.9  , 16.5  , 13.   , 17.626, 53.866, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 13.9     , 13.      , 22.880426, 41.37151 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.2     ,  8.5     , 13.      ,  8.214994, 44.22185 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.7     , 23.9     , 13.      , 43.85813 , 48.725494, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-3.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.2     , 44.1     , 13.      , 10.644387, 16.642057, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=126.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2      ,  7.2      , 14.       ,  4.7923613, 18.856117 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.   , 19.6  , 14.   ,  6.672, 27.262, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4     , 14.3     , 14.      , 10.669032, 51.269375, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     ,  8.      , 14.      , 11.951424, 15.078791, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.3     , 24.4     , 15.      , 25.769878, 31.232382, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-66.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.7     , 18.1     , 15.      ,  8.329783, 31.271732, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-89.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.3      ,  2.8      , 15.       ,  5.4226866, 34.68374  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.2      , 22.3      , 15.       ,  4.8822823, 23.063402 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8     ,  8.      , 15.      ,  4.695619, 46.60018 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     , 15.7     , 15.      , 20.968908, 26.463219, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4     , 26.9     , 16.      ,  1.780001,  8.461437, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     ,  6.6     , 16.      , 13.247024, 10.86077 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 22.6     , 16.      , 24.729244, 35.23635 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[27.5     , 17.4     , 16.      , 21.908583, 47.448563, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-131.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.5     , 12.7     , 16.      , 37.51753 , 47.625664, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.3      , 13.7      , 17.       ,  7.7906857, 43.67146  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6     , 33.1     , 17.      , 49.470436, 26.835289, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=20.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[30.2     , 22.6     ,  9.      , 15.404937, 24.843702, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-133.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1    , 30.1    ,  9.     , 34.75175, 33.7752 , -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-26.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.6  , 13.5  , 10.   ,  6.812, 43.785, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 15.9     , 11.      , 36.4549  , 13.471654, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4     , 19.5     , 12.      , 16.45586 , 43.943554, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3      ,  5.8      , 12.       ,  6.5473795, 52.398117 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 10.5     , 12.      , 10.686478, 35.116184, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     , 22.9     , 12.      , 38.460896, 44.756268, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=43.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.3  , 21.   , 12.   ,  4.268, 44.043, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     ,  4.6     , 12.      ,  8.927583, 37.58655 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.   ,  5.3  , 12.   ,  8.274, 45.453, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.5  ,  5.   , 12.   , 19.267, 34.09 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.3  ,  6.6  , 12.   , 26.88 , 28.134, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-48.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.9     , 16.2     , 13.      , 37.133575, 39.17766 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1     ,  2.      , 13.      , 22.440136, 49.247147, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.   , 25.   , 13.   ,  4.734, 54.421, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.7  , 27.6  , 13.   ,  4.902, 31.646, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.2  ,  6.8  , 13.   , 14.787, 46.257, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2     , 15.1     , 13.      , 11.645493, 48.86765 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.8     ,  7.7     , 13.      ,  2.420435, 17.458433, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.3     ,  8.9     , 13.      , 15.295227, 28.16364 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      ,  3.3     , 13.      , 19.533619, 32.513103, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-50.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1     , 32.6     , 13.      , 36.76446 , 18.658756, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     ,  7.8     , 15.      , 27.8748  , 24.774813, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2     ,  2.6     , 15.      , 23.742004, 45.31106 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.9     , 20.3     , 16.      ,  6.169793, 31.528599, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.5  , 21.7  , 17.   ,  4.59 , 21.832, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=18.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     , 18.1     , 17.      , 12.928518, 40.512394, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=25.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.3  , 16.3  , 17.   , 32.174, 53.508, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-24.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     , 23.      , 17.      , 17.607746, 31.499516, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.1      ,  8.       , 17.       ,  2.4738646, 42.457966 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.5     , 39.      , 17.      , 42.86125 , 51.075733, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=73.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1     , 24.8     , 17.      , 27.365305, 32.130646, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-50.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6      , 12.9      , 18.       ,  5.4825335, 29.18069  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.5     , 20.2     , 18.      , 33.01352 , 10.277929, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-54.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2     , 13.8     , 19.      , 42.40423 , 17.686253, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.6  , 44.1  ,  7.   ,  2.817, 49.678, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3     , 31.3     ,  9.      , 45.071186, 51.912556, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     , 18.9     , 15.      , 45.13989 , 44.678757, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1     , 14.9     , 17.      , 47.590588, 46.898693, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67a8940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.5, 41.7, 6, 0.10651689007654852, 20.249702331225194, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f65513d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.6, 8.5, 6, 2.4409720177717364, 37.72140120808937, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f653bbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.0, 6.0, 7, 19.131720964803655, 34.88622943688156, 40], 'reward': -8.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.6, 15.2, 7, 4.6745403763371955, 34.62463023286752, 39], 'reward': -77.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.1, 7.3, 8, 10.578974928303072, 46.31767260562961, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631ef70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.4, 28.7, 8, 32.08920123014499, 45.655716450583746, 38], 'reward': 82.32000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bafd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.0, 17.5, 8, 8.501000000000378, 50.86099999999994, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6285e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.2, 26.2, 8, 16.147000000001327, 12.042000000000083, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62060a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.4, 17.3, 9, 14.695180855824894, 58.96893354115201, 37], 'reward': -90.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.3, 11.7, 9, 8.62981637578337, 35.69282830699598, 36], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.5, 8.1, 9, 5.976392245046368, 44.553790585861904, 36], 'reward': -86.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.5, 16.2, 9, 13.737392133327909, 48.169266317657616, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.9, 20.3, 9, 6.561069024183675, 14.805875263112963, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61af040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.9, 21.3, 10, 3.8277655810327356, 27.234600753952403, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63041f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.3, 9.8, 10, 12.13935073453639, 45.19214927317786, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62060d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 13.1, 10, 21.09238458105887, 31.505253076140768, 35], 'reward': -9.759999999999991}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.4, 11.1, 10, 13.784131778403822, 49.46973811184348, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61262e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 20.4, 11, 3.237657110999983, 39.84112069796166, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.1, 11.2, 11, 4.697752254595368, 33.41920663193296, 34], 'reward': -46.43999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fd90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.5, 18.4, 11, 7.203164535153548, 37.9354166498884, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f619fee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 3.6, 11, 8.561226646312052, 28.722734028396417, 33], 'reward': -51.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f618c340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 12.4, 11, 25.775149644982946, 33.927478521142916, 32], 'reward': -5.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.1, 16.6, 12, 14.497000000001705, 4.449999999999936, 31], 'reward': -117.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6030070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.6, 2.2, 13, 14.336445530039192, 20.373615302500355, 30], 'reward': -92.24}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 22.1, 13, 2.8637013738149584, 12.093044484469825, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.7, 19.2, 13, 12.948000000001704, 50.62399999999985, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 33.8, 13, 38.992898401168866, 49.15662232386858, 29], 'reward': 67.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.5, 16.0, 14, 34.67087490133703, 30.312069316176867, 28], 'reward': -88.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.5, 13.8, 14, 6.096000000003221, 44.2699999999998, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e7940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.5, 27.8, 15, 8.463621507659713, 8.67367987973308, 27], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f20670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 5.4, 15, 35.7425440574244, 29.496402862778538, 27], 'reward': -14.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fe0fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.7, 35.2, 16, 16.166000000002462, 17.96500000000012, 26], 'reward': 26.279999999999973}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.0, 13.1, 16, 3.964606458624173, 16.252205216325514, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69b0790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 9.7, 16, 5.91100000000341, 39.23300000000004, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f202b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.1, 21.9, 16, 29.774880451431617, 20.938460924436967, 25], 'reward': -12.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6107280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.9, 16.8, 17, 7.554000000002274, 24.984999999999864, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62332b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.2, 11.9, 18, 5.160000000002274, 2.283000000000058, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6233400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 5.8, 18, 24.48835264347628, 28.820325231481963, 24], 'reward': -25.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f1250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.8, 5.8, 18, 7.901999999999811, 46.637, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f17c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.0, 29.7, 18, 18.144603818674383, 2.9607825114283663, 23], 'reward': -27.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f97f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.3, 42.4, 18, 33.885040472543835, 51.96169552956094, 22], 'reward': -2.3600000000000136}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 30.2, 19, 26.593603977906835, 16.957745261240078, 21], 'reward': 58.56000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62acbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.9, 18.5, 19, 4.2998082506770725, 50.62779506014854, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bccd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 9.7, 19, 25.666276985099103, 23.29760285597818, 20], 'reward': -47.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.5, 25.5, 19, 3.850980273802019, 43.56873968278472, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ac490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.9, 24.7, 19, 1.8069997798133635, 52.3187538622671, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62aca60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.7, 8.4, 19, 10.684086951205217, 50.605422472607216, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6315ee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.4, 17.1, 19, 33.446687932895934, 33.73418228623737, 19], 'reward': -43.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62acc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 24.3, 20, 1.5806690097163951, 53.113867530136844, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d5d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 20.9, 20, 1.094949662786636, 43.58732068158452, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2130>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.1, 24.1, 22, 48.28600000000303, 42.66300000000001, 18], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.7, 35.4, 0, 6.0235842463972205, 44.49812699024557, 18], 'reward': -47.87999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb2730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.2, 18.5, 5, 26.484164947085276, 31.277118195167002, 17], 'reward': -37.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5eb62b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.8, 11.7, 7, 17.67741336806246, 25.01420695590451, 16], 'reward': -97.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebad00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 20.0, 7, 37.60637566504142, 37.715966359931954, 15], 'reward': 12.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebda60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.0, 30.5, 8, 55.78623559889482, 44.17742872619779, 14], 'reward': 16.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec5b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.1, 40.6, 9, 7.9142791835001205, 36.00341194861859, 13], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edc460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.5, 7.2, 13, 47.32493507365287, 32.82177815844756, 13], 'reward': -41.55999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 28.1, 13, 13.750000000001325, 44.10099999999972, 12], 'reward': 38.23999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.7, 26.5, 13, 34.09236485315357, 56.98551302923487, 11], 'reward': -8.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.6, 18.6, 14, 25.7818795659403, 32.38617794745667, 10], 'reward': 7.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.5, 23.7, 14, 20.735558516692965, 24.902407087707836, 9], 'reward': -70.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee32e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.3, 33.5, 14, 25.024727760724204, 43.12601613676134, 8], 'reward': -17.239999999999952}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 5.0, 14, 10.331671172058485, 39.24099707526594, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.2, 12.0, 14, 2.7274550342166206, 35.94449127742476, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edea60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.2, 4.5, 14, 0.3285955711138504, 29.465399586671918, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.0, 11.6, 14, 13.624210963578733, 38.26421998456077, 7], 'reward': -92.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edec70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.9, 15.0, 14, 3.8350000000003788, 35.624999999999595, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 11.0, 14, 17.101532288910647, 40.117867457475946, 6], 'reward': -32.119999999999976}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 26.0, 14, 31.052010617727213, 52.603088890453364, 5], 'reward': 30.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.1, 15.1, 14, 4.356000000002842, 45.64699999999986, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.6, 8.5, 14, 3.6810000000036, 32.538000000000046, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.1, 23.5, 15, 2.7424026607556016, 34.87934493211107, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.0, 9.7, 15, 3.829483404988631, 42.957989109389764, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.4, 15.2, 15, 2.0320000000009473, 53.97700000000002, 4], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.8, 29.0, 16, 18.246476533575986, 10.783482989608874, 4], 'reward': -75.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.6, 17.2, 16, 18.338608685633098, 28.76618907391812, 3], 'reward': -64.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e771f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.5, 2.4, 16, 14.45781019462693, 47.677999274644286, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f1148c2c7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 10.3, 16, 4.7480000000028415, 33.360000000000326, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f11d926a100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.7, 10.5, 16, 20.148489282249052, 53.110376052267874, 2], 'reward': -100.35999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.3, 8.8, 16, 7.623000000003979, 49.350999999999765, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.7, 10.8, 16, 13.065753781800113, 40.44119820690583, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e77dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.2, 10.7, 16, 7.089000000004358, 35.80199999999991, 1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e778b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.7, 28.0, 16, 33.318709011036205, 48.05711375469541, 1], 'reward': 1442.04}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f06a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 19.0, 17, 2.4730000000036, 49.02599999999984, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630fb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.6, 16.9, 17, 8.935000000001326, 49.27699999999974, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67f09d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.0, 13.6, 17, 4.692000000002842, 35.981000000000186, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6551c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 21.0, 18, 45.26145601278233, 45.41545172061268, 0], 'reward': -11.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.7, 29.3, 18, 4.136999999999432, 48.655000000000044, -1], 'reward': -19.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630ffd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 14.4, 18, 30.968726966203903, 50.04349602799714, -1], 'reward': -42.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e777c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.0, 31.6, 18, 43.488972322420466, 56.91809743015528, -1], 'reward': -28.079999999999984}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.8, 22.6, 20, 7.919000000004168, 45.45400000000002, -1], 'reward': -28.32000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629df10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.0, 14.4, 20, 5.957000000002842, 60.0, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f1040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.1, 10.2, 21, 11.160454102142761, 32.654192511393006, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.4, 32.5, 21, 37.411142571611194, 27.76358409325095, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63543d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 29.3, 21, 29.923337033443474, 31.32965686947444, -1], 'reward': 60.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.9, 37.0, 22, 12.098508902598095, 53.6396882710389, -1], 'reward': 57.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63542b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 39.8, 23, 45.825194608392025, 51.60468586615904, -1], 'reward': 86.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e5b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.4, 20.8, 8, 31.458661980970536, 33.66105480528254, -1], 'reward': 50.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 3.3, 9, 19.99083326951274, 42.29618504488487, -1], 'reward': -67.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.3, 3.6, 9, 2.320847698105544, 32.86548016145754, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 27.5, 9, 40.352462209676176, 36.99907083179539, -1], 'reward': 34.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62154f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.0, 22.2, 12, 10.349000000004168, 45.487000000000045, -1], 'reward': -10.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.7, 12.5, 12, 18.4902488936268, 34.52229844678314, -1], 'reward': -5.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.2, 19.5, 12, 21.8653791353185, 52.285524198876544, -1], 'reward': -74.96000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6222ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 51.1, 12, 55.89225226932968, 55.33580713093936, -1], 'reward': 46.56000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9f40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.1, 40.0, 8, 22.29900000000284, 18.12199999999999, -1], 'reward': 25.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69ae4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.5, 22.3, 9, 6.379000000001705, 8.58700000000022, -1], 'reward': 13.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61332e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 20.8, 11, 23.556185307194625, 3.4592788676273525, -1], 'reward': 44.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62baa90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [27.1, 14.8, 13, 18.727334593599338, 30.447693697779016, -1], 'reward': -136.92000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f613cbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 12.5, 13, 20.399089394761116, 29.071861121957216, -1], 'reward': -37.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c57f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.8, 7.7, 14, 3.0952522666768827, 42.61722085997155, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c58b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.0, 17.0, 14, 5.942454352712082, 25.926871129837917, -1], 'reward': -102.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62bdc70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.0, 25.2, 14, 20.705257203042212, 33.319901494217035, -1], 'reward': 5.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c5610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.4, 32.4, 14, 36.77585016765234, 34.08522794230599, -1], 'reward': -14.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c77c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.6, 15.8, 14, 4.741000000005684, 58.52399999999958, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62c7d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.0, 36.0, 15, 57.94347417573769, 38.93306325888152, -1], 'reward': 13.199999999999989}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.7, 0.9, 8, 50.785946595882486, 41.45208284360527, -1], 'reward': -42.68}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602a4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 6.1, 12, 31.464000000003598, 51.06599999999978, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6292a90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.6, 44.1, 13, 10.64438765882013, 16.642057286321364, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec1f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.7, 11.6, 17, 53.04466870037973, 52.61457794143659, -1], 'reward': 32.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.7, 18.2, 13, 36.408839065669056, 45.86901365568162, -1], 'reward': 39.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd8040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.7, 16.4, 13, 41.15782628152405, 44.676656558004204, -1], 'reward': -27.079999999999984}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.7, 17.5, 14, 3.549942516756534, 50.892642220471785, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fa90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.1, 8.6, 14, 52.49680153949672, 43.16782878615239, -1], 'reward': -41.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.3, 36.8, 17, 25.967247287078226, 22.331458534906655, -1], 'reward': 95.32000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625f490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 7.0, 17, 15.979068359504721, 33.16099636965663, -1], 'reward': -55.8}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f625faf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.4, 6.1, 17, 10.022054322328465, 51.866193539269375, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 16.1, 17, 3.538999999999621, 42.1479999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8a5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.1, 8.0, 17, 2.4738646510867874, 42.45796508777647, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62585b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.9, 22.6, 17, 27.388810808067856, 24.54586904212694, -1], 'reward': -22.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.3, 29.9, 17, 22.658999479749937, 34.47980834367663, -1], 'reward': -69.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.5, 12.9, 18, 5.482533497719135, 29.1806900875728, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.6, 6.2, 18, 14.992831364970167, 28.183470794526997, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f9b100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 3.7, 18, 21.394205622184323, 48.68807679576751, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.2, 8.7, 18, 11.052871350482878, 27.377977846095725, -1], 'reward': -123.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2fa90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 13.3, 18, 14.008553287724666, 0.6647423669583414, -1], 'reward': -49.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6258610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.5, 16.8, 18, 8.743847042752279, 28.21628310217631, -1], 'reward': -51.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f39c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.4, 14.5, 18, 3.6160000000020838, 47.98400000000023, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.0, 14.2, 18, 12.111094992676046, 44.39862012217112, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2fb80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.1, 2.6, 18, 4.775679457911068, 27.036169253856887, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6640>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [24.8, 14.7, 18, 9.85075443300669, 55.4241820198175, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e69d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.2, 6.2, 18, 0.5429980665941949, 22.602137105706237, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f2f220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.5, 12.0, 18, 5.60400000000379, 42.435999999999865, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f46520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 14.0, 19, 9.278955005014613, 45.39590399480989, -1], 'reward': 20.319999999999993}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e69a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 4.9, 19, 6.198576146655095, 37.25758810310518, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f468b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.3, 45.4, 19, 17.624425058316163, 9.255167029538285, -1], 'reward': 88.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.9, 4.1, 19, 5.786000000002084, 21.53899999999994, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.4, 10.9, 19, 8.07100000000322, 39.32099999999981, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ef4670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [26.5, 11.1, 19, 0.13200539267335465, 42.23122722398629, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e63d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.8, 8.3, 20, 4.094219352178799, 30.69028876542724, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f461c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.9, 10.4, 20, 5.198000000000758, 29.734000000000012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f5a490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.6, 12.2, 20, 7.209601883729763, 35.14452498064337, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee7fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.1, 6.5, 20, 13.860042335114366, 14.704056204830627, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.6, 12.9, 23, 18.504900833218738, 17.581982435473044, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f085e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.8, 20.9, 6, 5.661000000004169, 28.369000000000106, -1], 'reward': 54.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0ceb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.6, 5.5, 6, 0.0587159901174199, 37.35202272967051, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f0c940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.0, 6.8, 7, 19.130499459875814, 37.39638096027652, -1], 'reward': -134.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f205e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.3, 21.0, 8, 27.67985236745058, 24.462736309319165, -1], 'reward': -36.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a97fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.9, 14.1, 9, 7.898000000004169, 32.849000000000224, -1], 'reward': -63.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a972b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.4, 37.7, 9, 36.08160270770051, 9.85520527783543, -1], 'reward': 111.12}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 22.3, 12, 8.99800000000341, 6.358999999999821, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 15.6, 13, 6.941231391181622, 13.06440492682225, -1], 'reward': -79.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.3, 5.4, 13, 8.964099412268617, 9.37405653480609, -1], 'reward': 1.6400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a347f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 11.3, 13, 16.17164333949809, 22.48483789512727, -1], 'reward': -16.19999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d8df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.7, 23.4, 13, 6.872000000000568, 56.16400000000019, -1], 'reward': -45.47999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 12.5, 14, 3.6510000000011367, 55.31800000000012, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34fd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 11.3, 14, 5.736436346009032, 53.936670822447226, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d89a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 7.6, 14, 19.303358333770042, 44.14932592296135, -1], 'reward': -42.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a342b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 36.0, 14, 20.194916789985115, 18.109998701059567, -1], 'reward': -1.759999999999991}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.3, 4.3, 14, 0.7323020911561509, 36.05753979462301, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 15.5, 14, 27.925443919507764, 28.8104752454377, -1], 'reward': -50.359999999999985}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d8790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.8, 19.9, 14, 0.25384618269275094, 24.658558156685565, -1], 'reward': -30.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.9, 5.1, 15, 1.1438122195951896, 20.454045193281356, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4daf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.9, 21.6, 15, 15.193043933192795, 7.557889575963486, -1], 'reward': 29.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a4dc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.7, 27.9, 16, 34.867659177666894, 36.966722734564485, -1], 'reward': 23.32000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a60b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 40.0, 17, 44.08287456488788, 1.1091489674127502, -1], 'reward': 96.03999999999996}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f693c160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 16.4, 19, 29.855236051305383, 19.908415014883012, -1], 'reward': 0.12000000000003297}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ecca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.9, 33.2, 21, 42.83326157760398, 9.775552832155277, -1], 'reward': -8.680000000000007}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.5       , 41.7       ,  6.        ,  0.10651689, 20.249702  ,\n",
      "        40.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6     ,  8.5     ,  6.      ,  2.440972, 37.7214  , 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.      ,  6.      ,  7.      , 19.131721, 34.88623 , 40.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.6      , 15.2      ,  7.       ,  4.6745405, 34.62463  ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-77.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.1     ,  7.3     ,  8.      , 10.578975, 46.317673, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.4     , 28.7     ,  8.      , 32.089203, 45.655716, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=82.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.   , 17.5  ,  8.   ,  8.501, 50.861, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.2  , 26.2  ,  8.   , 16.147, 12.042, 37.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4     , 17.3     ,  9.      , 14.695181, 58.968933, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-90.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     , 11.7     ,  9.      ,  8.629816, 35.69283 , 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.5      ,  8.1      ,  9.       ,  5.9763923, 44.55379  ,\n",
      "        36.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-86.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.5     , 16.2     ,  9.      , 13.737392, 48.169266, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9     , 20.3     ,  9.      ,  6.561069, 14.805875, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9      , 21.3      , 10.       ,  3.8277655, 27.2346   ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     ,  9.8     , 10.      , 12.139351, 45.19215 , 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 13.1     , 10.      , 21.092384, 31.505253, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-9.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.4     , 11.1     , 10.      , 13.784132, 49.469738, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 20.4     , 11.      ,  3.237657, 39.84112 , 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1      , 11.2      , 11.       ,  4.6977525, 33.41921  ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.5      , 18.4      , 11.       ,  7.2031646, 37.935417 ,\n",
      "        33.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     ,  3.6     , 11.      ,  8.561227, 28.722734, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6    , 12.4    , 11.     , 25.77515, 33.92748, 32.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.1  , 16.6  , 12.   , 14.497,  4.45 , 31.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-117.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     ,  2.2     , 13.      , 14.336446, 20.373615, 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-92.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.3      , 22.1      , 13.       ,  2.8637013, 12.093044 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.7  , 19.2  , 13.   , 12.948, 50.624, 29.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      , 33.8     , 13.      , 38.992897, 49.156624, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=67.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.5     , 16.      , 14.      , 34.670876, 30.312069, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-88.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.5  , 13.8  , 14.   ,  6.096, 44.27 , 27.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.5     , 27.8     , 15.      ,  8.463621,  8.67368 , 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     ,  5.4     , 15.      , 35.742542, 29.496403, 27.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-14.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.7  , 35.2  , 16.   , 16.166, 17.965, 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.       , 13.1      , 16.       ,  3.9646065, 16.252205 ,\n",
      "        25.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.1  ,  9.7  , 16.   ,  5.911, 39.233, 25.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1     , 21.9     , 16.      , 29.774881, 20.938461, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.9  , 16.8  , 17.   ,  7.554, 24.985, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.2  , 11.9  , 18.   ,  5.16 ,  2.283, 24.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.5     ,  5.8     , 18.      , 24.488352, 28.820326, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[22.8  ,  5.8  , 18.   ,  7.902, 46.637, 23.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.       , 29.7      , 18.       , 18.144604 ,  2.9607825,\n",
      "        23.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-27.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.3     , 42.4     , 18.      , 33.88504 , 51.961697, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6     , 30.2     , 19.      , 26.593603, 16.957745, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=58.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.9     , 18.5     , 19.      ,  4.299808, 50.627796, 20.      ]],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  9.7     , 19.      , 25.666277, 23.297604, 20.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5      , 25.5      , 19.       ,  3.8509803, 43.56874  ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.9      , 24.7      , 19.       ,  1.8069998, 52.318752 ,\n",
      "        19.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.7     ,  8.4     , 19.      , 10.684087, 50.605423, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.4    , 17.1    , 19.     , 33.44669, 33.73418, 19.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4     , 24.3     , 20.      ,  1.580669, 53.11387 , 18.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4      , 20.9      , 20.       ,  1.0949497, 43.587322 ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.1  , 24.1  , 22.   , 48.286, 42.663, 18.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.7      , 35.4      ,  0.       ,  6.0235844, 44.498127 ,\n",
      "        18.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.2     , 18.5     ,  5.      , 26.484165, 31.277119, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-37.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.8     , 11.7     ,  7.      , 17.677414, 25.014208, 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-97.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 20.      ,  7.      , 37.606377, 37.715965, 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=12.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 30.5     ,  8.      , 55.786236, 44.17743 , 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=16.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     , 40.6     ,  9.      ,  7.914279, 36.00341 , 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.5     ,  7.2     , 13.      , 47.324936, 32.821777, 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.6  , 28.1  , 13.   , 13.75 , 44.101, 12.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=38.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.7     , 26.5     , 13.      , 34.092365, 56.98551 , 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-8.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.6     , 18.6     , 14.      , 25.78188 , 32.386177, 10.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=7.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.5     , 23.7     , 14.      , 20.73556 , 24.902407,  9.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-70.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.3     , 33.5     , 14.      , 25.024727, 43.126015,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  5.      , 14.      , 10.331671, 39.240997,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.2      , 12.       , 14.       ,  2.7274551, 35.944492 ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.2       ,  4.5       , 14.        ,  0.32859558, 29.465399  ,\n",
      "         7.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.      , 11.6     , 14.      , 13.624211, 38.26422 ,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-92.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.9  , 15.   , 14.   ,  3.835, 35.625,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.9     , 11.      , 14.      , 17.101532, 40.117867,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-32.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7     , 26.      , 14.      , 31.052011, 52.60309 ,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.1  , 15.1  , 14.   ,  4.356, 45.647,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[25.6  ,  8.5  , 14.   ,  3.681, 32.538,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.1      , 23.5      , 15.       ,  2.7424026, 34.879345 ,\n",
      "         4.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.       ,  9.7      , 15.       ,  3.8294835, 42.95799  ,\n",
      "         4.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.4  , 15.2  , 15.   ,  2.032, 53.977,  4.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.8     , 29.      , 16.      , 18.246477, 10.783483,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-75.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 17.2     , 16.      , 18.338608, 28.76619 ,  3.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-64.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.5     ,  2.4     , 16.      , 14.45781 , 47.677998,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.4  , 10.3  , 16.   ,  4.748, 33.36 ,  2.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.7     , 10.5     , 16.      , 20.148489, 53.110374,  2.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-100.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.3  ,  8.8  , 16.   ,  7.623, 49.351,  1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.7     , 10.8     , 16.      , 13.065754, 40.441196,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.2  , 10.7  , 16.   ,  7.089, 35.802,  1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.7     , 28.      , 16.      , 33.31871 , 48.057114,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1442.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.8  , 19.   , 17.   ,  2.473, 49.026,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.6  , 16.9  , 17.   ,  8.935, 49.277,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[24.   , 13.6  , 17.   ,  4.692, 35.981,  0.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 21.      , 18.      , 45.261456, 45.41545 ,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.7  , 29.3  , 18.   ,  4.137, 48.655, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-19.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 14.4     , 18.      , 30.968727, 50.043495, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-42.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.     , 31.6    , 18.     , 43.48897, 56.9181 , -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.8  , 22.6  , 20.   ,  7.919, 45.454, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-28.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.   , 14.4  , 20.   ,  5.957, 60.   , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.1     , 10.2     , 21.      , 11.160454, 32.654194, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.4     , 32.5     , 21.      , 37.411144, 27.763584, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.9     , 29.3     , 21.      , 29.923338, 31.329657, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=60.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9     , 37.      , 22.      , 12.098509, 53.639687, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=57.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.      , 39.8     , 23.      , 45.825195, 51.604687, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=86.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.4     , 20.8     ,  8.      , 31.458662, 33.661057, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=50.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  3.3     ,  9.      , 19.990833, 42.296185, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-67.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3      ,  3.6      ,  9.       ,  2.3208477, 32.86548  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     , 27.5     ,  9.      , 40.352463, 36.99907 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=34.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.   , 22.2  , 12.   , 10.349, 45.487, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.7     , 12.5     , 12.      , 18.49025 , 34.522297, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2     , 19.5     , 12.      , 21.86538 , 52.285522, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-74.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2     , 51.1     , 12.      , 55.892254, 55.335808, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=46.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.1  , 40.   ,  8.   , 22.299, 18.122, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=25.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.5  , 22.3  ,  9.   ,  6.379,  8.587, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3      , 20.8      , 11.       , 23.556185 ,  3.4592788,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=44.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[27.1     , 14.8     , 13.      , 18.727335, 30.447693, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-136.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.4     , 12.5     , 13.      , 20.39909 , 29.071861, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-37.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.8      ,  7.7      , 14.       ,  3.0952523, 42.61722  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.       , 17.       , 14.       ,  5.9424543, 25.92687  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-102.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.      , 25.2     , 14.      , 20.705257, 33.3199  , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=5.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4     , 32.4     , 14.      , 36.77585 , 34.085228, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-14.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[24.6  , 15.8  , 14.   ,  4.741, 58.524, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.      , 36.      , 15.      , 57.943474, 38.933064, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.7     ,  0.9     ,  8.      , 50.785946, 41.452084, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-42.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  ,  6.1  , 12.   , 31.464, 51.066, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 44.1     , 13.      , 10.644387, 16.642057, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.7    , 11.6    , 17.     , 53.04467, 52.61458, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=32.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.7     , 18.2     , 13.      , 36.40884 , 45.869015, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=39.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.7     , 16.4     , 13.      , 41.157825, 44.676655, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-27.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.7      , 17.5      , 14.       ,  3.5499425, 50.892643 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.1     ,  8.6     , 14.      , 52.496803, 43.167828, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-41.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3     , 36.8     , 17.      , 25.967247, 22.331459, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=95.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     ,  7.      , 17.      , 15.979069, 33.160995, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-55.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.4     ,  6.1     , 17.      , 10.022055, 51.86619 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.5  , 16.1  , 17.   ,  3.539, 42.148, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.1      ,  8.       , 17.       ,  2.4738646, 42.457966 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 22.6     , 17.      , 27.388811, 24.54587 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-22.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.3    , 29.9    , 17.     , 22.659  , 34.47981, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.5      , 12.9      , 18.       ,  5.4825335, 29.18069  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.6     ,  6.2     , 18.      , 14.992831, 28.183472, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.4     ,  3.7     , 18.      , 21.394205, 48.688076, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.2     ,  8.7     , 18.      , 11.052872, 27.377977, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-123.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6       , 13.3       , 18.        , 14.0085535 ,  0.66474235,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-49.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5     , 16.8     , 18.      ,  8.743847, 28.216284, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.4  , 14.5  , 18.   ,  3.616, 47.984, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.      , 14.2     , 18.      , 12.111095, 44.39862 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.1      ,  2.6      , 18.       ,  4.7756796, 27.03617  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.8     , 14.7     , 18.      ,  9.850755, 55.424183, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2      ,  6.2      , 18.       ,  0.5429981, 22.602137 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.5  , 12.   , 18.   ,  5.604, 42.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     , 14.      , 19.      ,  9.278955, 45.395905, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=20.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.8     ,  4.9     , 19.      ,  6.198576, 37.257587, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.3     , 45.4     , 19.      , 17.624426,  9.255167, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=88.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.9  ,  4.1  , 19.   ,  5.786, 21.539, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[23.4  , 10.9  , 19.   ,  8.071, 39.321, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[26.5      , 11.1      , 19.       ,  0.1320054, 42.231228 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.8     ,  8.3     , 20.      ,  4.094219, 30.690289, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.9  , 10.4  , 20.   ,  5.198, 29.734, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.6     , 12.2     , 20.      ,  7.209602, 35.144524, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.1     ,  6.5     , 20.      , 13.860043, 14.704056, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6     , 12.9     , 23.      , 18.5049  , 17.581982, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.8  , 20.9  ,  6.   ,  5.661, 28.369, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=54.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.6       ,  5.5       ,  6.        ,  0.05871599, 37.352024  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.      ,  6.8     ,  7.      , 19.130499, 37.39638 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-134.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     , 21.      ,  8.      , 27.679852, 24.462736, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-36.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[15.9  , 14.1  ,  9.   ,  7.898, 32.849, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.4     , 37.7     ,  9.      , 36.081604,  9.855206, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=111.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.2  , 22.3  , 12.   ,  8.998,  6.359, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1      , 15.6      , 13.       ,  6.9412313, 13.0644045,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-79.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.3     ,  5.4     , 13.      ,  8.9641  ,  9.374057, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7     , 11.3     , 13.      , 16.171644, 22.484838, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-16.2>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.7  , 23.4  , 13.   ,  6.872, 56.164, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.2  , 12.5  , 14.   ,  3.651, 55.318, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4      , 11.3      , 14.       ,  5.7364364, 53.936672 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     ,  7.6     , 14.      , 19.303358, 44.149326, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-42.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2     , 36.      , 14.      , 20.194918, 18.109999, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.3       ,  4.3       , 14.        ,  0.73230207, 36.05754   ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     , 15.5     , 14.      , 27.925444, 28.810474, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-50.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.8       , 19.9       , 14.        ,  0.25384617, 24.658558  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.9      ,  5.1      , 15.       ,  1.1438122, 20.454044 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.9      , 21.6      , 15.       , 15.193044 ,  7.5578895,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=29.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.7     , 27.9     , 16.      , 34.86766 , 36.966724, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.7     , 40.      , 17.      , 44.082874,  1.109149, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=96.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7     , 16.4     , 19.      , 29.855236, 19.908415, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631bca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 27.3, 8, 21.71799999999943, 40.23100000000031, 40], 'reward': -4.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ad9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 17.5, 8, 8.501000000000378, 50.86099999999994, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f627e670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 26.2, 8, 16.147000000001327, 12.042000000000083, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624fa30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.9, 17.3, 9, 14.695180855824894, 58.96893354115201, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.5, 20.3, 9, 6.561069024183675, 14.805875263112963, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61e6a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.3, 8.1, 9, 5.976392245046368, 44.553790585861904, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.0, 11.7, 9, 8.62981637578337, 35.69282830699598, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61af700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 16.2, 9, 13.737392133327909, 48.169266317657616, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61f6b20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 17.6, 10, 24.626000000001323, 32.30499999999982, 39], 'reward': -13.039999999999992}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61a83a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.3, 11.1, 10, 13.784131778403822, 49.46973811184348, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6133bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 20.4, 11, 3.237657110999983, 39.84112069796166, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 11.2, 11, 4.697752254595368, 33.41920663193296, 38], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.3, 7.5, 11, 18.044666652471918, 35.376473045927554, 38], 'reward': -59.640000000000015}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61672b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.6, 4.8, 12, 12.70328449280226, 36.91135433998117, 37], 'reward': -43.11999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6199fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.8, 17.7, 12, 30.496401708576705, 36.92861414855969, 36], 'reward': 30.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6155520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.1, 9.0, 12, 7.632744241841025, 57.14468542013553, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.9, 14.2, 12, 5.242856438813538, 45.210399609535266, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 12.3, 12, 3.189320390178912, 34.09010116234906, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.7, 20.9, 12, 13.213000000001704, 51.10199999999982, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60ac580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.4, 20.1, 13, 13.210213898261479, 28.418819013014602, 35], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6030880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.4, 17.8, 13, 27.779793366372445, 39.25187439059199, 35], 'reward': -88.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec2b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.5, 19.2, 13, 12.948000000001704, 50.62399999999985, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f607c430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.5, 22.1, 13, 2.8637013738149584, 12.093044484469825, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0c10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 9.4, 13, 3.810549387997334, 36.68840946203857, 34], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fd0d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.2, 5.2, 13, 6.9089487189364664, 28.452463810280022, 34], 'reward': -113.91999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6087dc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.2, 5.9, 13, 7.337046150150713, 32.47807111613416, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 15.3, 13, 5.625000000003031, 28.882000000000087, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb5460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.0, 18.7, 13, 11.864399137818344, 58.10903030984113, 33], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.2, 25.4, 13, 23.167145664772676, 50.706995778074266, 33], 'reward': 45.920000000000016}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.6, 26.5, 13, 11.903932501482346, 26.363864698702038, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.3, 23.9, 13, 31.366000000000945, 25.24099999999993, 32], 'reward': -0.36000000000001364}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f52310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.3, 11.2, 14, 2.7656139877731363, 28.969535971170618, 31], 'reward': -95.4}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.9, 22.8, 14, 34.13483433498063, 41.84636947909125, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f6c5e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.0, 9.8, 14, 16.727556241596805, 32.741741878218576, 30], 'reward': -2.6400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60484f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.5, 19.2, 14, 9.677788682803186, 55.21742455973801, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 13.8, 14, 6.096000000003221, 44.2699999999998, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.2, 20.3, 14, 3.7519463502408126, 30.6671268968651, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f8aee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 25.5, 14, 1.0439290130213585, 17.180867774669608, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb1c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.7, 4.7, 14, 6.264608235605703, 47.84498354266634, 29], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb52e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.9, 17.4, 14, 24.05966107922324, 40.47959471407327, 29], 'reward': -52.43999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82a00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.6, 13.5, 14, 7.613061938265394, 33.030824266585746, 28], 'reward': -1.6800000000000068}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fc07c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 14.1, 14, 18.95900000000417, 32.886000000000266, 27], 'reward': 23.36}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f398e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.0, 18.5, 14, 17.08600000000417, 46.47999999999961, 26], 'reward': 18.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee72b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.5, 44.8, 14, 39.00620377116019, 16.186696632153662, 25], 'reward': 78.76000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f08e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.2, 29.1, 15, 23.171046088289263, 3.4781610962471703, 24], 'reward': 30.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a60400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.0, 13.1, 16, 3.964606458624173, 16.252205216325514, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 43.5, 16, 57.21482132608604, 20.783567177629326, 23], 'reward': 103.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebaf70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.9, 21.4, 7, 35.148938488529744, 36.35954509942776, 22], 'reward': 21.56000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebdaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.4, 16.1, 8, 38.99026130324236, 41.87395384633462, 21], 'reward': -39.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebae50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 11.8, 8, 7.17937638668108, 53.829088760093775, 20], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ebaca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.2, 15.8, 8, 8.152000000000568, 58.751999999999704, 20], 'reward': -93.6}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 2.9, 9, 14.563365464546159, 42.924822379194055, 19], 'reward': -107.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec9190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 7.8, 9, 19.028000000002084, 40.962999999999894, 18], 'reward': -2.9199999999999875}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ec2730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.7, 57.0, 9, 56.19256048513415, 11.970940971635287, 17], 'reward': 109.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 15.0, 22, 44.05867830354449, 31.104388253226546, 16], 'reward': -5.039999999999992}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.6, 20.7, 6, 8.744208783218479, 32.5092333464284, 15], 'reward': -33.039999999999964}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d9d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.2, 11.0, 6, 15.923036124794288, 35.39546782298618, 14], 'reward': 13.440000000000012}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.9, 13.6, 7, 16.533861603158254, 36.56890751985232, 13], 'reward': -51.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eae80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.5, 9.1, 7, 15.129967638018051, 43.18143288429463, 12], 'reward': -69.48000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.4, 7.3, 7, 4.4160000000028425, 37.28199999999984, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623dee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.1, 6.0, 7, 8.701000000001706, 41.59100000000004, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eaaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.0, 27.9, 7, 6.497999999999242, 48.02400000000011, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.4, 29.5, 7, 27.719895664061504, 13.325996824041699, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ea7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.6, 6.2, 7, 6.7001824641636, 36.84890889344565, 11], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eaf10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 23.8, 7, 5.2640000000036, 17.40200000000039, 11], 'reward': 45.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62ee460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.1, 19.4, 7, 27.689071666817703, 7.3784915870222365, 10], 'reward': 27.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f04f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.8, 19.9, 9, 4.96500000000379, 2.320999999999895, 9], 'reward': -64.16000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 39.3, 9, 8.875769256940565, 50.245933153892494, 8], 'reward': 64.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fedc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.8, 11.5, 10, 13.829225588238472, 50.13839715156357, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.6, 5.0, 10, 1.4494150545804763, 33.44615338739398, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0280>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.0, 5.0, 10, 1.7355825125689763, 38.473137658948566, 7], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.1, 7.4, 10, 9.081701011367619, 33.13541414507219, 7], 'reward': -113.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.4, 9.7, 10, 4.18000000000379, 28.13800000000036, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f0970>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.8, 6.7, 10, 8.733771671669347, 49.470447225575185, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.8, 9.8, 10, 2.8667163980259236, 40.75219464115571, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [3.3, 10.5, 10, 12.850608953984898, 42.37527984525556, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.3, 14.6, 10, 0.2783423785647763, 32.82687107215835, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.9, 10.9, 11, 8.244000000003599, 34.01400000000021, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.7, 16.0, 11, 7.770000000001705, 15.578000000000321, 6], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620bb50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 13.7, 11, 9.051939219392205, 56.53124835572259, 6], 'reward': -22.799999999999983}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.5, 22.7, 11, 1.5813117226192404, 15.03465216307032, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.2, 9.5, 11, 11.845134103261417, 55.67675860482852, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621cd30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.6, 6.2, 12, 4.436000000005684, 47.82299999999998, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215b50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.8, 36.1, 12, 7.017145926633231, 19.70701540926764, 5], 'reward': 103.28000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6215bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.7, 35.3, 12, 25.285394413853183, 5.322814967174704, 4], 'reward': 13.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6309220>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.4, 12.8, 12, 5.223335861144907, 8.896797460528841, 3], 'reward': -22.960000000000008}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.2, 4.5, 12, 2.470431639689015, 7.266497639946243, 2], 'reward': -34.55999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63099d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.6, 15.1, 13, 2.85520921605377, 4.2425170975464805, 1], 'reward': 1462.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fd550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [22.0, 19.1, 14, 4.0486389635973925, 45.19640189829088, 0], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f5460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.2, 10.6, 14, 0.13237494727767718, 14.4404110191062, 0], 'reward': -103.43999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdfa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.5, 27.2, 14, 29.813860273576104, 49.60545673136252, -1], 'reward': -45.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62f7c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.9, 14.7, 14, 10.352000000000567, 47.43599999999979, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 5.1, 16, 8.36982912338616, 50.75836392546525, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61ba0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.6, 21.5, 16, 36.582160315085645, 39.877775109876616, -1], 'reward': -10.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.7, 13.3, 17, 17.275635712354823, 50.446336010495514, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 18.6, 17, 5.727999999999241, 50.166999999999824, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6302100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.3, 14.4, 18, 5.1060000000036, 48.91499999999956, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.9, 13.2, 19, 13.761763428283526, 53.56472242654716, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.0, 17.7, 19, 7.310000000001705, 32.47100000000013, -1], 'reward': -72.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61d2880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.6, 18.2, 19, 0.6945295881499121, 22.02318376009886, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f616c370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.1, 5.7, 19, 11.479328315094865, 25.82963682430796, -1], 'reward': -16.439999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63234c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.4, 12.3, 20, 9.908000000005686, 23.615999999999996, -1], 'reward': -38.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f631ee80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.3, 21.9, 20, 28.1294315295866, 30.720997456933382, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f630d310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.2, 13.4, 20, 16.383922165943353, 24.243648350451856, -1], 'reward': -12.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63234f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.6, 17.7, 20, 4.039000000003221, 24.648000000000394, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617a430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.9, 4.7, 20, 10.944164191361924, 18.482384219459306, -1], 'reward': -59.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6323790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.1, 25.6, 20, 23.837890621076976, 16.993700041396217, -1], 'reward': -47.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63235e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.5, 10.2, 21, 17.417478489187527, 38.01667520884504, -1], 'reward': -140.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63236d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.5, 14.3, 21, 19.981788598649363, 46.39979423741606, -1], 'reward': -66.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f617ab50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.9, 46.6, 21, 53.54269439323301, 52.20574502016325, -1], 'reward': 41.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.8, 13.9, 18, 20.480716880103138, 52.34900899242892, -1], 'reward': -96.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.3, 18.0, 19, 9.605402037849087, 28.169794243522908, -1], 'reward': -73.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.8, 2.2, 19, 7.1910000000007575, 23.586999999999943, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.0, 16.9, 19, 4.498208371977535, 28.73099223355376, -1], 'reward': -54.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.0, 12.2, 19, 19.80177700321348, 12.309821773514571, -1], 'reward': -49.359999999999985}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.4, 4.1, 19, 8.740389112668893, 29.399570395760243, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60cb7f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.7, 7.1, 19, 19.307835594510912, 22.908239362321115, -1], 'reward': -2.4399999999999977}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60fa670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 19.6, 19, 14.299000000000188, 43.45799999999972, -1], 'reward': 32.120000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d6af0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.6, 15.7, 19, 0.24998997702861203, 31.80100361556505, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d62e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [8.8, 6.8, 19, 4.949308326255002, 52.73181313905326, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b4a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [14.1, 24.5, 19, 7.103123079976211, 35.63676993995987, -1], 'reward': -17.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bfaf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.7, 4.6, 20, 3.1804562204991895, 41.84682724639756, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60bf820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.5, 15.1, 20, 21.789061890998845, 33.65488006068379, -1], 'reward': -16.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60d3190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.9, 23.2, 20, 6.2699923311619274, 59.188015610788284, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60b49a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [21.8, 24.8, 21, 16.158624245174416, 26.248956316990295, -1], 'reward': -68.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dcd60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 2.0, 21, 2.473764622054804, 24.239323575111158, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dc8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.0, 6.3, 22, 5.549149530662853, 38.16101129515673, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60e0e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.4, 14.3, 22, 4.754000000005684, 50.39799999999989, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62dcfa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.1, 26.3, 23, 33.7764978822178, 34.256844589012196, -1], 'reward': 35.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606b4c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 15.8, 6, 38.42327509095845, 43.85607775754054, -1], 'reward': -10.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f606b040>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.3, 6.3, 7, 17.776000000000757, 49.36099999999999, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62e0eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.8, 22.6, 7, 47.557615263668055, 30.851865832796356, -1], 'reward': -7.920000000000044}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f8e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.9, 37.2, 16, 9.872, 26.535000000000224, -1], 'reward': 85.71999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.6, 22.0, 17, 2.637000000003979, 42.88299999999989, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f460>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 1.4, 17, 17.8430158676212, 27.547611559144844, -1], 'reward': -49.24}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.6, 2.7, 17, 23.470553712555972, 30.27665933896226, -1], 'reward': -15.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fdc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.8, 20.9, 17, 2.833000000000379, 50.198000000000114, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fd30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.8, 13.7, 17, 7.790685851395308, 43.67145907666459, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6004cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.1, 12.3, 17, 20.14671685972473, 3.94095260907011, -1], 'reward': -97.32000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fa00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.2, 24.3, 17, 19.03444266780062, 41.83143929742145, -1], 'reward': -52.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.8, 20.3, 17, 18.111297289257656, 54.25737198986859, -1], 'reward': -56.079999999999984}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec2e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [2.2, 30.3, 17, 3.9060000000013266, 29.985000000000348, -1], 'reward': 82.0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6048d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.1, 3.7, 17, 11.1370000000036, 23.833999999999833, -1], 'reward': -43.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.1, 27.7, 17, 30.099079502728532, 20.177781000899362, -1], 'reward': 26.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.6, 9.4, 18, 5.040172516706594, 36.55253931157359, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.4, 16.4, 18, 9.995951915593643, 48.308615491793546, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [25.4, 7.5, 18, 5.945340670790753, 29.360152284969054, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60136a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [12.0, 21.9, 18, 39.85780953686236, 22.387321643601286, -1], 'reward': -11.519999999999982}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fa9370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 34.7, 19, 2.0740000000041685, 30.62700000000009, -1], 'reward': 41.67999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6013400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.6, 23.7, 19, 26.499375859704173, 9.88460536027257, -1], 'reward': 10.56000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffadc0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.0, 11.4, 19, 12.715000000003222, 5.767000000000319, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f601a250>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.1, 20.0, 19, 40.08787666941011, 15.677731447084698, -1], 'reward': -45.47999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b0a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.7, 28.5, 7, 4.446000000000568, 4.956999999999884, -1], 'reward': -35.960000000000036}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d6d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.9, 21.3, 8, 9.188400646982362, 41.029019069253565, -1], 'reward': -39.96000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [4.4, 4.0, 8, 8.266647448007134, 45.6470007258497, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f622d6a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [0.5, 17.5, 8, 2.5760000000026526, 56.96999999999963, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a38e0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.9, 2.1, 8, 3.408852310040646, 54.21348943687376, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a3340>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.1, 9.0, 8, 4.345000000000947, 42.6949999999999, -1], 'reward': -33.08000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f624b7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.1, 10.0, 8, 34.01034969280204, 50.54794293881506, -1], 'reward': -131.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6299760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.3, 23.6, 9, 22.812842714024047, 32.82399854055033, -1], 'reward': -14.920000000000044}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fb52b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.3, 30.2, 9, 51.64415202592886, 41.611484270442425, -1], 'reward': 60.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f82700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 35.8, 14, 21.48299999999981, 18.14399999999968, -1], 'reward': 66.96000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f63586a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.4, 3.4, 15, 14.607335820103245, 20.09719272952102, -1], 'reward': -25.840000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.6, 6.0, 15, 11.667000000004357, 15.446000000000028, -1], 'reward': -46.08}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f78cd0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.0, 18.6, 15, 4.790086023894401, 31.376742168044537, -1], 'reward': -1.6800000000000068}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.3, 4.7, 15, 3.7289959645504593, 41.433381290657685, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361400>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.5, 2.6, 15, 23.742004685542813, 45.311062014599976, -1], 'reward': -158.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6361eb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.3, 3.7, 15, 8.621220926902565, 59.849562578599574, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6358a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 40.9, 15, 59.459808700169575, 38.06364975792884, -1], 'reward': 78.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f00d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.3, 3.4, 7, 50.43535564992045, 52.087598070434, -1], 'reward': -93.16}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6aa2160>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.1, 8.8, 8, 32.599999999999426, 36.82300000000019, -1], 'reward': -88.12}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5f209d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.8, 3.9, 8, 31.313303593947424, 29.3322233192868, -1], 'reward': -40.55999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a91610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [1.3, 2.1, 9, 28.513494131696707, 28.197414805522698, -1], 'reward': -2.120000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.0, 6.1, 12, 13.547999999999432, 41.32599999999993, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [29.7, 11.6, 12, 20.135389302857924, 54.139217733147426, -1], 'reward': -164.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a53d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.1, 5.3, 12, 0.19785187885751965, 47.826929991335916, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.3, 34.3, 12, 3.7565416482341267, 13.502737770234063, -1], 'reward': -7.879999999999939}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39b80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [0.9, 7.7, 13, 12.344658881468842, 14.109233377672828, -1], 'reward': 18.520000000000003}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [16.8, 10.3, 13, 9.69350635389996, 20.489547516594868, -1], 'reward': -81.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39730>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.2, 20.3, 13, 16.795574240050676, 33.75393904167336, -1], 'reward': 22.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.5, 29.9, 13, 35.62021769690502, 25.43796218608344, -1], 'reward': 17.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69d8e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.8, 6.2, 14, 36.38439412640613, 23.43150438961637, -1], 'reward': -12.799999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34df0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.8, 18.9, 14, 4.769000000001137, 27.55400000000002, -1], 'reward': -80.96000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.1, 20.2, 14, 13.711239053691223, 12.890403853589255, -1], 'reward': 43.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39ac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.3, 4.3, 14, 0.7323020911561509, 36.05753979462301, -1], 'reward': -137.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a34a60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.4, 26.2, 14, 26.27573999785843, 23.876723079831244, -1], 'reward': 40.31999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a39d60>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [27.0, 17.1, 14, 1.4430000000026526, 56.724999999999646, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a391f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [25.0, 9.8, 14, 14.191715197905115, 33.90139808021388, -1], 'reward': -138.64}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.2, 26.3, 14, 24.924607392348037, 46.785851732522, -1], 'reward': 14.800000000000011}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e7310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.2, 4.5, 14, 6.721781865431259, 57.06018856899615, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a42d90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.4, 19.3, 15, 25.05978475795552, 42.78711083753398, -1], 'reward': -63.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e7760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.4, 7.7, 15, 5.205000000002653, 44.30099999999968, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a346a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.1, 33.1, 15, 40.93369236034197, 58.34471606469225, -1], 'reward': -17.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69e0be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.8, 24.3, 17, 1.9120000000045474, 48.41700000000021, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69976d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 58.7, 18, 7.501000000000947, 18.36800000000019, -1], 'reward': 124.60000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6997ca0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.6, 20.8, 18, 10.205907313571162, 39.16518360991017, -1], 'reward': 35.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6997be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.5, 8.6, 18, 14.109232484495216, 38.90489168417147, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f69f6c40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 9.5, 18, 10.751000000004737, 32.953999999999986, -1], 'reward': -36.24000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6928a30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 9.3, 18, 12.114600381435075, 28.761760984896274, -1], 'reward': -30.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f692f850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [10.8, 29.0, 19, 23.114752081387135, 14.228306200265177, -1], 'reward': 19.360000000000014}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f692f700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [16.1, 43.4, 19, 44.11171759123117, 52.685980995514115, -1], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f692fa90>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.2, 7.9, 19, 24.896371772510676, 12.215460826502358, -1], 'reward': -10.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f66ecd30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [24.5, 19.4, 21, 0.7420000000026528, 47.6070000000002, -1], 'reward': -104.51999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6949610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.3, 58.3, 22, 38.88697992110753, 0.8335398106603549, -1], 'reward': 130.12000000000006}\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.], dtype=float32)>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.5  , 27.3  ,  8.   , 21.718, 40.231, 40.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-4.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.8  , 17.5  ,  8.   ,  8.501, 50.861, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.   , 26.2  ,  8.   , 16.147, 12.042, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.9     , 17.3     ,  9.      , 14.695181, 58.968933, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.5     , 20.3     ,  9.      ,  6.561069, 14.805875, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.3      ,  8.1      ,  9.       ,  5.9763923, 44.55379  ,\n",
      "        39.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 11.7     ,  9.      ,  8.629816, 35.69283 , 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.2     , 16.2     ,  9.      , 13.737392, 48.169266, 39.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.2  , 17.6  , 10.   , 24.626, 32.305, 39.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-13.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.3     , 11.1     , 10.      , 13.784132, 49.469738, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1     , 20.4     , 11.      ,  3.237657, 39.84112 , 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1      , 11.2      , 11.       ,  4.6977525, 33.41921  ,\n",
      "        38.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.3     ,  7.5     , 11.      , 18.044666, 35.376472, 38.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-59.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.6     ,  4.8     , 12.      , 12.703284, 36.911354, 37.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.8     , 17.7     , 12.      , 30.4964  , 36.928616, 36.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.1      ,  9.       , 12.       ,  7.6327443, 57.144684 ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.9      , 14.2      , 12.       ,  5.2428565, 45.2104   ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.       , 12.3      , 12.       ,  3.1893203, 34.0901   ,\n",
      "        35.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.7  , 20.9  , 12.   , 13.213, 51.102, 35.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.4     , 20.1     , 13.      , 13.210214, 28.41882 , 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.4     , 17.8     , 13.      , 27.779793, 39.251873, 35.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-88.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.5  , 19.2  , 13.   , 12.948, 50.624, 34.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.5      , 22.1      , 13.       ,  2.8637013, 12.093044 ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3      ,  9.4      , 13.       ,  3.8105495, 36.688408 ,\n",
      "        34.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2     ,  5.2     , 13.      ,  6.908949, 28.452463, 34.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-113.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     ,  5.9     , 13.      ,  7.337046, 32.47807 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.   , 15.3  , 13.   ,  5.625, 28.882, 33.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 18.7     , 13.      , 11.864399, 58.10903 , 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.2     , 25.4     , 13.      , 23.167145, 50.706997, 33.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=45.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.6     , 26.5     , 13.      , 11.903933, 26.363865, 32.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.3  , 23.9  , 13.   , 31.366, 25.241, 32.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-0.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3     , 11.2     , 14.      ,  2.765614, 28.969536, 31.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-95.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.9     , 22.8     , 14.      , 34.134834, 41.84637 , 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.      ,  9.8     , 14.      , 16.727556, 32.74174 , 30.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.5     , 19.2     , 14.      ,  9.677789, 55.217426, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.4  , 13.8  , 14.   ,  6.096, 44.27 , 29.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.2      , 20.3      , 14.       ,  3.7519464, 30.667128 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     , 25.5     , 14.      ,  1.043929, 17.180868, 29.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7      ,  4.7      , 14.       ,  6.2646084, 47.844982 ,\n",
      "        29.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9     , 17.4     , 14.      , 24.059662, 40.479595, 29.      ]],\n",
      "      dtype=float32)>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.6     , 13.5     , 14.      ,  7.613062, 33.030823, 28.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 3.2  , 14.1  , 14.   , 18.959, 32.886, 27.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=23.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.   , 18.5  , 14.   , 17.086, 46.48 , 26.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=18.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.5     , 44.8     , 14.      , 39.006203, 16.186697, 25.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=78.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.2     , 29.1     , 15.      , 23.171045,  3.478161, 24.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=30.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.       , 13.1      , 16.       ,  3.9646065, 16.252205 ,\n",
      "        23.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 43.5     , 16.      , 57.21482 , 20.783567, 23.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=103.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.9     , 21.4     ,  7.      , 35.148937, 36.359547, 22.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=21.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.4     , 16.1     ,  8.      , 38.99026 , 41.873955, 21.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.       , 11.8      ,  8.       ,  7.1793766, 53.82909  ,\n",
      "        20.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[21.2  , 15.8  ,  8.   ,  8.152, 58.752, 20.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-93.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.1     ,  2.9     ,  9.      , 14.563366, 42.924824, 19.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-107.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.1  ,  7.8  ,  9.   , 19.028, 40.963, 18.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.7     , 57.      ,  9.      , 56.192562, 11.970941, 17.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=109.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     , 15.      , 22.      , 44.058678, 31.10439 , 16.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-5.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6     , 20.7     ,  6.      ,  8.744208, 32.50923 , 15.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-33.04>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.2     , 11.      ,  6.      , 15.923037, 35.39547 , 14.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.9     , 13.6     ,  7.      , 16.533861, 36.56891 , 13.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-51.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.5     ,  9.1     ,  7.      , 15.129968, 43.181435, 12.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-69.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.4  ,  7.3  ,  7.   ,  4.416, 37.282, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.1  ,  6.   ,  7.   ,  8.701, 41.591, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.   , 27.9  ,  7.   ,  6.498, 48.024, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     , 29.5     ,  7.      , 27.719896, 13.325996, 11.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.6      ,  6.2      ,  7.       ,  6.7001824, 36.848907 ,\n",
      "        11.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  , 23.8  ,  7.   ,  5.264, 17.402, 11.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=45.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.1      , 19.4      ,  7.       , 27.689072 ,  7.3784914,\n",
      "        10.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=27.4>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.8  , 19.9  ,  9.   ,  4.965,  2.321,  9.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-64.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 39.3     ,  9.      ,  8.87577 , 50.245934,  8.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=64.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.8     , 11.5     , 10.      , 13.829226, 50.138397,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.6      ,  5.       , 10.       ,  1.4494151, 33.44615  ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.       ,  5.       , 10.       ,  1.7355825, 38.473137 ,\n",
      "         7.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1     ,  7.4     , 10.      ,  9.081701, 33.135414,  7.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-113.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.4  ,  9.7  , 10.   ,  4.18 , 28.138,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     ,  6.7     , 10.      ,  8.733771, 49.470448,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.8      ,  9.8      , 10.       ,  2.8667164, 40.752193 ,\n",
      "         6.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.3     , 10.5     , 10.      , 12.850609, 42.37528 ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.3       , 14.6       , 10.        ,  0.27834237, 32.82687   ,\n",
      "         6.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.9  , 10.9  , 11.   ,  8.244, 34.014,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 1.7  , 16.   , 11.   ,  7.77 , 15.578,  6.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.8     , 13.7     , 11.      ,  9.051939, 56.53125 ,  6.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-22.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5      , 22.7      , 11.       ,  1.5813117, 15.034652 ,\n",
      "         5.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.2     ,  9.5     , 11.      , 11.845134, 55.676758,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.6  ,  6.2  , 12.   ,  4.436, 47.823,  5.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.8     , 36.1     , 12.      ,  7.017146, 19.707016,  5.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=103.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.7     , 35.3     , 12.      , 25.285395,  5.322815,  4.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=13.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.4      , 12.8      , 12.       ,  5.2233357,  8.896797 ,\n",
      "         3.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-22.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.2      ,  4.5      , 12.       ,  2.4704316,  7.2664976,\n",
      "         2.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-34.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.6     , 15.1     , 13.      ,  2.855209,  4.242517,  1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=1462.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.      , 19.1     , 14.      ,  4.048639, 45.196404,  0.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.2       , 10.6       , 14.        ,  0.13237494, 14.440411  ,\n",
      "         0.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-103.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.5     , 27.2     , 14.      , 29.81386 , 49.605457, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.9  , 14.7  , 14.   , 10.352, 47.436, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1     ,  5.1     , 16.      ,  8.369829, 50.758366, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.6     , 21.5     , 16.      , 36.58216 , 39.877773, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.7     , 13.3     , 17.      , 17.275635, 50.446335, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.1  , 18.6  , 17.   ,  5.728, 50.167, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.3  , 14.4  , 18.   ,  5.106, 48.915, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9     , 13.2     , 19.      , 13.761764, 53.564724, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[19.   , 17.7  , 19.   ,  7.31 , 32.471, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-72.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6      , 18.2      , 19.       ,  0.6945296, 22.023184 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.1     ,  5.7     , 19.      , 11.479328, 25.829638, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-16.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[11.4  , 12.3  , 20.   ,  9.908, 23.616, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-38.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.3     , 21.9     , 20.      , 28.12943 , 30.720997, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.2     , 13.4     , 20.      , 16.383923, 24.243649, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 5.6  , 17.7  , 20.   ,  4.039, 24.648, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.9     ,  4.7     , 20.      , 10.944164, 18.482384, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-59.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1    , 25.6    , 20.     , 23.83789, 16.9937 , -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-47.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.5     , 10.2     , 21.      , 17.417479, 38.016674, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-140.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.5     , 14.3     , 21.      , 19.981789, 46.399796, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-66.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9     , 46.6     , 21.      , 53.542694, 52.205746, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=41.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.8     , 13.9     , 18.      , 20.480717, 52.34901 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-96.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.3     , 18.      , 19.      ,  9.605402, 28.169794, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-73.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 6.8  ,  2.2  , 19.   ,  7.191, 23.587, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.       , 16.9      , 19.       ,  4.4982085, 28.730991 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-54.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.      , 12.2     , 19.      , 19.801777, 12.309822, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-49.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.4     ,  4.1     , 19.      ,  8.740389, 29.39957 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.7     ,  7.1     , 19.      , 19.307837, 22.90824 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.44>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.5  , 19.6  , 19.   , 14.299, 43.458, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=32.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 2.6       , 15.7       , 19.        ,  0.24998997, 31.801004  ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8      ,  6.8      , 19.       ,  4.9493084, 52.73181  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.1     , 24.5     , 19.      ,  7.103123, 35.63677 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.7      ,  4.6      , 20.       ,  3.1804562, 41.84683  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.5     , 15.1     , 20.      , 21.789062, 33.65488 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-16.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[14.9      , 23.2      , 20.       ,  6.2699924, 59.188015 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.8     , 24.8     , 21.      , 16.158625, 26.248957, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-68.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8      ,  2.       , 21.       ,  2.4737647, 24.239323 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.       ,  6.3      , 22.       ,  5.5491495, 38.16101  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[12.4  , 14.3  , 22.   ,  4.754, 50.398, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.1     , 26.3     , 23.      , 33.776497, 34.256844, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 15.8     ,  6.      , 38.423275, 43.85608 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.3  ,  6.3  ,  7.   , 17.776, 49.361, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.8     , 22.6     ,  7.      , 47.557613, 30.851866, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-7.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.9  , 37.2  , 16.   ,  9.872, 26.535, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=85.72>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 4.6  , 22.   , 17.   ,  2.637, 42.883, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.9     ,  1.4     , 17.      , 17.843016, 27.547611, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-49.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.6     ,  2.7     , 17.      , 23.470554, 30.276659, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-15.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.8  , 20.9  , 17.   ,  2.833, 50.198, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.8      , 13.7      , 17.       ,  7.7906857, 43.67146  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[20.1      , 12.3      , 17.       , 20.146717 ,  3.9409525,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-97.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.2     , 24.3     , 17.      , 19.034443, 41.83144 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-52.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.8     , 20.3     , 17.      , 18.111298, 54.257374, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-56.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 2.2  , 30.3  , 17.   ,  3.906, 29.985, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=82.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 8.1  ,  3.7  , 17.   , 11.137, 23.834, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-43.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.1    , 27.7    , 17.     , 30.09908, 20.17778, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=26.76>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[21.6      ,  9.4      , 18.       ,  5.0401726, 36.55254  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.4     , 16.4     , 18.      ,  9.995952, 48.308617, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.4      ,  7.5      , 18.       ,  5.9453406, 29.360153 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.      , 21.9     , 18.      , 39.85781 , 22.387321, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-11.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[10.2  , 34.7  , 19.   ,  2.074, 30.627, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=41.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.6     , 23.7     , 19.      , 26.499376,  9.884605, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=10.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[14.   , 11.4  , 19.   , 12.715,  5.767, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1      , 20.       , 19.       , 40.087875 , 15.6777315,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-45.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[18.7  , 28.5  ,  7.   ,  4.446,  4.957, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-35.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9    , 21.3    ,  8.     ,  9.1884 , 41.02902, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-39.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.4     ,  4.      ,  8.      ,  8.266647, 45.647   , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 0.5  , 17.5  ,  8.   ,  2.576, 56.97 , -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.9      ,  2.1      ,  8.       ,  3.4088523, 54.21349  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.1  ,  9.   ,  8.   ,  4.345, 42.695, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-33.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.1     , 10.      ,  8.      , 34.01035 , 50.547943, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-131.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[13.3     , 23.6     ,  9.      , 22.812843, 32.823997, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-14.92>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.3     , 30.2     ,  9.      , 51.644154, 41.611485, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=60.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 7.   , 35.8  , 14.   , 21.483, 18.144, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=66.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.4     ,  3.4     , 15.      , 14.607336, 20.097193, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-25.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.6  ,  6.   , 15.   , 11.667, 15.446, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-46.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 9.      , 18.6     , 15.      ,  4.790086, 31.376741, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-1.68>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.3     ,  4.7     , 15.      ,  3.728996, 41.43338 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[24.5     ,  2.6     , 15.      , 23.742004, 45.31106 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-158.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[23.3     ,  3.7     , 15.      ,  8.621221, 59.849564, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.7    , 40.9    , 15.     , 59.45981, 38.06365, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=78.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[15.3     ,  3.4     ,  7.      , 50.435356, 52.087597, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-93.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[17.1  ,  8.8  ,  8.   , 32.6  , 36.823, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-88.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 7.8     ,  3.9     ,  8.      , 31.313303, 29.332224, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-40.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 1.3     ,  2.1     ,  9.      , 28.513494, 28.197414, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-2.12>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.   ,  6.1  , 12.   , 13.548, 41.326, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[29.7     , 11.6     , 12.      , 20.13539 , 54.139217, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-164.84>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[19.1       ,  5.3       , 12.        ,  0.19785188, 47.82693   ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.3      , 34.3      , 12.       ,  3.7565417, 13.502738 ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-7.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 0.9     ,  7.7     , 13.      , 12.344659, 14.109233, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=18.52>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.8     , 10.3     , 13.      ,  9.693506, 20.489548, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-81.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.2     , 20.3     , 13.      , 16.795574, 33.75394 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=22.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[11.5     , 29.9     , 13.      , 35.620216, 25.437962, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=17.48>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.8     ,  6.2     , 14.      , 36.384396, 23.431505, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-12.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[20.8  , 18.9  , 14.   ,  4.769, 27.554, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-80.96>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 3.1     , 20.2     , 14.      , 13.711239, 12.890404, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=43.56>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[22.3       ,  4.3       , 14.        ,  0.73230207, 36.05754   ,\n",
      "        -1.        ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-137.88>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 6.4     , 26.2     , 14.      , 26.27574 , 23.876722, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=40.32>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[27.   , 17.1  , 14.   ,  1.443, 56.725, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[25.      ,  9.8     , 14.      , 14.191715, 33.901398, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-138.64>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.2     , 26.3     , 14.      , 24.924608, 46.78585 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=14.8>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[17.2      ,  4.5      , 14.       ,  6.7217817, 57.06019  ,\n",
      "        -1.       ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.4     , 19.3     , 15.      , 25.059784, 42.78711 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-63.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[13.4  ,  7.7  , 15.   ,  5.205, 44.301, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[18.1     , 33.1     , 15.      , 40.933693, 58.344715, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-17.16>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[16.8  , 24.3  , 17.   ,  1.912, 48.417, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.3  , 58.7  , 18.   ,  7.501, 18.368, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=124.6>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 4.6     , 20.8     , 18.      , 10.205907, 39.165184, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=35.28>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[12.5     ,  8.6     , 18.      , 14.109233, 38.904892, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[ 9.8  ,  9.5  , 18.   , 10.751, 32.954, -1.   ]], dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-36.24>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 8.8    ,  9.3    , 18.     , 12.1146 , 28.76176, -1.     ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-30.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[10.8     , 29.      , 19.      , 23.114752, 14.228306, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=19.36>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[16.1     , 43.4     , 19.      , 44.111717, 52.68598 , -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
      "array([[ 5.2     ,  7.9     , 19.      , 24.896372, 12.215461, -1.      ]],\n",
      "      dtype=float32)>)\n",
      "TimeStep(step_type=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, reward=<tf.Tensor: shape=(), dtype=float32, numpy=-10.08>, discount=<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[24.5  , 19.4  , 21.   ,  0.742, 47.607, -1.   ]], dtype=float32)>)\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 42.6, 13, 13.509000000001704, 43.029999999999816, 40], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5edee50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.7, 18.9, 13, 34.24667893472807, 16.850373074715222, 40], 'reward': 14.920000000000016}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.3, 15.0, 14, 3.8350000000003788, 35.624999999999595, 39], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d580>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.2, 8.2, 15, 17.27204029323255, 27.402716765888577, 39], 'reward': -90.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [15.2, 19.0, 15, 8.400546416613096, 45.47208636205106, 38], 'reward': -42.56}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ede9a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.2, 15.8, 15, 14.089000000000189, 47.81500000000002, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [5.8, 42.5, 15, 39.80046326286461, 26.41204092963106, 37], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ee3fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 5.5, 15, 8.537000000000758, 27.667999999999953, 37], 'reward': -74.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.8, 20.0, 15, 16.86139076761053, 6.802369792638611, 36], 'reward': 24.560000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6dee0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 12.3, 15, 6.21700000000398, 18.942999999999977, 35], 'reward': 8.760000000000005}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e69e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.6, 6.3, 15, 11.537445391068726, 35.060552311128134, 34], 'reward': -72.32}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71700>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 15.0, 15, 22.366544475346306, 36.40290992617992, 33], 'reward': 17.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e6d0d0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.3, 9.7, 15, 3.829483404988631, 42.957989109389764, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.7, 15.2, 15, 2.0320000000009473, 53.97700000000002, 32], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e71be0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [17.6, 31.5, 15, 36.380789034970846, 35.61175169719044, 32], 'reward': -18.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5e74190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.9, 26.5, 16, 39.43174501363266, 54.099920996741346, 31], 'reward': 17.480000000000018}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f67403a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.2, 27.9, 16, 8.639843440076842, 48.57523737894558, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6a9a7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [19.2, 29.3, 18, 4.136999999999432, 48.655000000000044, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f629d520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [10.1, 22.6, 20, 7.919000000004168, 45.45400000000002, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6354820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [13.7, 23.7, 22, 9.386000000004168, 40.59499999999989, 30], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d3a0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 24.6, 2, 7.900999999999241, 42.098, 30], 'reward': 12.079999999999956}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.2, 16.0, 4, 21.34583832906863, 29.09958334563404, 29], 'reward': -79.36000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f623d760>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.4, 12.5, 5, 10.216432719034186, 5.27462380816309, 28], 'reward': -85.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62eec40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [5.6, 9.7, 8, 15.114188004258656, 2.5814794869077566, 27], 'reward': -7.039999999999992}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61feac0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.4, 17.8, 8, 4.705610380016296, 31.74629435713131, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206520>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [11.6, 39.3, 9, 8.875769256940565, 50.245933153892494, 26], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61fe550>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.2, 5.2, 10, 12.235832526130482, 18.16071041980053, 26], 'reward': -107.11999999999998}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6206430>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.2, 9.7, 10, 4.18000000000379, 28.13800000000036, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f635e850>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.7, 14.6, 10, 0.2783423785647763, 32.82687107215835, 25], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f620b940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.4, 19.6, 11, 26.621423415313497, 21.75990528026845, 25], 'reward': 5.599999999999994}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c940>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [12.8, 10.9, 11, 8.244000000003599, 34.01400000000021, 24], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f621c880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.0, 17.8, 11, 43.137242638913726, 30.94930375924359, 24], 'reward': 29.75999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62fdb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [23.0, 14.7, 14, 10.352000000000567, 47.43599999999979, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61af4f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [0.6, 39.8, 17, 4.790000000001516, 18.32300000000009, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6304f10>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [6.1, 15.1, 17, 52.171730800307316, 17.19925391607439, 23], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6314e80>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [11.7, 30.7, 19, 12.63082506252973, 2.0660515536207953, 23], 'reward': 18.680000000000007}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6323e50>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.0, 13.8, 21, 9.089584833132701, 32.96275865778156, 22], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174e20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.5, 33.1, 22, 20.81599999999905, 50.18599999999958, 22], 'reward': -19.879999999999995}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [1.9, 17.1, 22, 5.581000000001895, 54.38999999999993, 21], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.4, 19.1, 23, 23.455796410998243, 36.27716354034723, 21], 'reward': -77.59999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6174820>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [18.8, 40.9, 23, 40.74858564507099, 21.159079599749074, 20], 'reward': 3.0399999999999636}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6184d00>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [7.1, 30.6, 7, 6.759999999998105, 34.23600000000005, 19], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62a9100>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [20.9, 25.1, 8, 49.46800000000038, 26.908999999999917, 19], 'reward': -61.80000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6126c70>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.5, 52.4, 12, 4.777000000003221, 45.357999999999755, 18], 'reward': 137.08000000000004}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f612d610>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.1, 37.1, 12, 39.76500446770713, 38.75296221379696, 17], 'reward': 90.83999999999997}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61432b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.3, 12.1, 15, 27.696751843007597, 24.115142094748514, 16], 'reward': -17.72}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f62d08b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [17.0, 18.6, 15, 12.945000000000757, 55.52699999999999, 15], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f61438b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [4.7, 22.5, 16, 47.42197141862049, 13.030442725421366, 15], 'reward': 40.04000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6277fa0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.8, 12.3, 12, 27.064000000002274, 16.502000000000194, 14], 'reward': -27.28}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f626c670>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.0, 10.1, 12, 30.720695506029877, 16.434099131352216, 13], 'reward': -15.280000000000001}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f609fb20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [21.4, 24.5, 12, 31.25816709747211, 32.86198176212744, 12], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f602a880>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [22.1, 31.6, 13, 42.784284749976536, 30.491990286144826, 12], 'reward': -49.160000000000025}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054d30>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [29.1, 12.5, 16, 28.49900000000284, 39.866000000000106, 11], 'reward': -157.88}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60481f0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [14.1, 10.9, 16, 15.876768390761802, 57.19585182299929, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605d070>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [18.1, 3.5, 16, 15.433023880499903, 53.59181960034229, 10], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054490>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.9, 12.7, 16, 19.87602128844457, 40.809080586749836, 10], 'reward': -13.080000000000013}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605db20>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [15.3, 8.7, 16, 12.769314466165781, 43.48021178338007, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f60482b0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [20.1, 17.4, 16, 21.90858295766533, 47.44856114977631, 9], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f603e370>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [6.5, 12.7, 16, 10.972000000004547, 30.62200000000007, 9], 'reward': -3.5600000000000023}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f6054bb0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.3, 19.7, 16, 23.334144561210497, 31.94866618275442, 8], 'reward': 13.400000000000006}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fc40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [19.4, 16.3, 16, 7.463000000003221, 22.714000000000105, 7], 'reward': -79.76000000000002}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f310>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [8.8, 7.0, 16, 8.332000000000189, 24.313000000000308, 6], 'reward': -37.44}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f605ddf0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [2.6, 22.0, 17, 2.637000000003979, 42.88299999999989, 5], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec790>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [7.7, 20.7, 17, 26.67600000000417, 28.581000000000245, 5], 'reward': 13.880000000000024}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608ff40>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [9.3, 26.9, 17, 39.03666075189873, 14.54263881140405, 4], 'reward': 22.839999999999975}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608fbe0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [13.5, 36.7, 17, 27.151000000004167, 38.36099999999981, 3], 'reward': 25.639999999999986}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5ffa910>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=()), 'observation': [9.4, 20.9, 17, 2.833000000000379, 50.198000000000114, 2], 'reward': 0}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f5fec190>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [23.9, 15.9, 17, 22.660153848238544, 45.29255962423851, 2], 'reward': -111.63999999999999}\n",
      "{'state': <RideSimulator.State.State object at 0x7f10f608f7c0>, 'action': PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=()), 'observation': [3.9, 30.6, 17, 49.09906103587759, 53.74483213354705, 1], 'reward': 1571.4}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-0d283db5eb49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Collect a few steps using collect_policy and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollect_steps_per_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-37fa2cc9a1db>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(num_iterations, policy, replay_buffer)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m#convert states directly to tf timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;31m#create time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/collections/__init__.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;34m'Return a nicely formatted representation string'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr_fmt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_asdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     return \"<tf.Tensor: shape=%s, dtype=%s, numpy=%s>\" % (\n\u001b[0;32m--> 909\u001b[0;31m         self.shape, self.dtype.name, numpy_text(self, is_repr=True))\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy_text\u001b[0;34m(tensor, is_repr)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_numpy_compatible\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_repr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    925\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print(' Average Return = {0}'.format( avg_return))\n",
    "returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())tf.constant(0, dtype=tf.int32)\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "            saver.save('policy_%d' % step)\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickup distribution [ 871.  763.  848.  764. 1021.  929.  610.]\n",
      "pikcup acceptance rates [0.5670572916666666, 0.5762839879154078, 0.5909407665505226, 0.5801063022019742, 0.5761851015801355, 0.5835427135678392, 0.595703125]\n",
      "pikcup acceptance rates_agent [0.6028645833333334, 0.6155589123867069, 0.6202090592334495, 0.5801063022019742, 0.5654627539503386, 0.439070351758794, 0.2421875]\n",
      "trip distribution [ 448. 1223. 1405. 2278. 1542. 1396. 1708.]\n",
      "trip acceptance rates [0.5959821428571429, 0.5641864268192968, 0.6, 0.5790166812993854, 0.5739299610894941, 0.5859598853868195, 0.5761124121779859]\n",
      "trip acceptance rates_agent [0.4486607142857143, 0.45952575633687653, 0.5281138790035588, 0.5588235294117647, 0.5687418936446174, 0.5429799426934098, 0.5450819672131147]\n",
      "Accuracy: 54.66%\n",
      "accept freq: 53.44%\n",
      "dataset accept freq: 58.06%\n"
     ]
    }
   ],
   "source": [
    "#test against data from pickme dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "week_6 = pd.read_csv(\"Eval_data.csv\")\n",
    "tot = 0\n",
    "tot_accept = 0\n",
    "dataset_accept = 0\n",
    "num = 10000\n",
    "\n",
    "pickup_accepted_trips = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_accepted_trips_agent = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_trip_counts = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_acceptance_rates = []\n",
    "pickup_acceptance_rates_agent = []\n",
    "\n",
    "trip_accepted_trips = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_accepted_trips_agent = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_counts = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_acceptance_rates = []\n",
    "trip_acceptance_rates_agent = []\n",
    "    \n",
    "for i in range(num):\n",
    "    #load relevant fields from dataset\n",
    "    data_point = week_6.iloc[i][['distance_to_pickup','trip_distance','day_time','accepted_trip_count','drop_latitude', 'drop_longitude', 'action']].tolist()\n",
    "    #observation_ts = ts.transition(np.array(data_point[:-1], dtype=np.float32), reward=0.0, discount=1.0)\n",
    "    #print(np.array(data_point[:-1],dtype=np.float32))\n",
    "    \n",
    "    \n",
    "    #group by pickup distances, trip distances\n",
    "    pickup_trip_counts[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "    trip_counts[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "    \n",
    "    #scale drop location\n",
    "    data_point[-3] = (data_point[-3] - 6.8) * 40\n",
    "    data_point[-2] = (data_point[-2] - 79.85) * 40\n",
    "    \n",
    "    observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([data_point[:-1]], dtype=np.float32), dtype=tf.float32))\n",
    "    policy_step = eval_policy.action(observation_ts)\n",
    "    policy_state = policy_step.state\n",
    "    #print(policy_step.action.numpy()[0])\n",
    "    if policy_step.action.numpy()[0] == 1:\n",
    "        tot_accept += 1\n",
    "        pickup_accepted_trips_agent[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "        trip_accepted_trips_agent[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "        \n",
    "    if data_point[-1] == 1:\n",
    "        dataset_accept += 1\n",
    "        pickup_accepted_trips[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "        trip_accepted_trips[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "        \n",
    "    if policy_step.action.numpy()[0] == data_point[-1]:\n",
    "        tot += 1\n",
    "\n",
    "for j in range(len(pickup_distance_brackets)+1):\n",
    "    pickup_acceptance_rates.append(float(pickup_accepted_trips[j])/float(pickup_trip_counts[j]))\n",
    "    pickup_acceptance_rates_agent.append(float(pickup_accepted_trips_agent[j])/float(pickup_trip_counts[j]))\n",
    "print (\"pickup distribution\", pickup_accepted_trips)\n",
    "print(\"pikcup acceptance rates\", pickup_acceptance_rates)\n",
    "print(\"pikcup acceptance rates_agent\", pickup_acceptance_rates_agent)\n",
    "\n",
    "for k in range(len(trip_distance_brackets)+1):\n",
    "    trip_acceptance_rates.append(float(trip_accepted_trips[k])/float(trip_counts[k]))\n",
    "    trip_acceptance_rates_agent.append(float(trip_accepted_trips_agent[k])/float(trip_counts[k]))\n",
    "print (\"trip distribution\", trip_counts)\n",
    "print(\"trip acceptance rates\", trip_acceptance_rates)\n",
    "print(\"trip acceptance rates_agent\", trip_acceptance_rates_agent)\n",
    "    \n",
    "print(f'Accuracy: {tot/num * 100}%')\n",
    "print(f'accept freq: {tot_accept/num * 100}%')\n",
    "print(f'dataset accept freq: {dataset_accept/num * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#draw graphs\n",
    "\"\"\"\n",
    "plt.plot([0]+pickup_distance_brackets, trip_acceptance_rates)\n",
    "plt.xlabel('pickup distance (km)')\n",
    "plt.ylabel('acceptance rate')\n",
    "legned()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Make some fake data.\n",
    "a = b = [0]+pickup_distance_brackets\n",
    "c = pickup_acceptance_rates\n",
    "d = pickup_acceptance_rates_agent\n",
    "\n",
    "# Create plots with pre-defined labels.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(a, c, 'k--', label='Dataset trip acceptance rates')\n",
    "ax.plot(a, d, 'k:', label='Agent trip acceptance rates')\n",
    "#ax.plot(a, c + d, 'k', label='Total message length')\n",
    "plt.xlabel('pickup distance (km)')\n",
    "plt.ylabel('acceptance rate')\n",
    "legend = ax.legend(loc='best', shadow=True, fontsize='medium')\n",
    "\n",
    "# Put a nicer background color on the legend.\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0-1', '1-2', '2-3', '3-4', '4-6', '6-10', '10+']\n",
    "men_means = [0.5270572916666666, 0.5762839879154078, 0.6209407665505226, 0.5801063022019742, 0.5761851015801355, 0.5835427135678392, 0.595703125]\n",
    "women_means = [0.51046875, 0.5805740181268882, 0.6181881533101046, 0.5536142748671223, 0.5333182844243793, 0.5501256281407035, 0.578828125]\n",
    "#men_means = pickup_acceptance_rates\n",
    "#women_means = pickup_acceptance_rates_agent\n",
    "\n",
    "labels1 = ['0-5', '5-10', '10-15', '15-25', '25-35', '50-35', '50+']\n",
    "men_means1 = [0.5959821428571429, 0.5641864268192968, 0.6, 0.5790166812993854, 0.5739299610894941, 0.5859598853868195, 0.5761124121779859]\n",
    "women_means1 = [0.37857142857142856, 0.4463614063777596, 0.51423487544483983, 0.546268656716418, 0.5578469520103762, 0.5914040114613181, 0.6959016393442623]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "rects1 = ax[0].bar(x - width/2, men_means, width, label='Dataset acceptance rates')\n",
    "rects2 = ax[0].bar(x + width/2, women_means, width, label='Agent acceptance rates')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax[0].set_ylabel('acceptance rate', fontsize=20)\n",
    "ax[0].set_xlabel('pickup distance (km)', fontsize=20)\n",
    "ax[0].set_title('acceptance rate variation with pickup distance', fontsize=20)\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(labels)\n",
    "#ax[0].legend(fontsize='medium')\n",
    "\n",
    "rects1 = ax[1].bar(x - width/2, men_means1, width, label='Dataset acceptance rates')\n",
    "rects2 = ax[1].bar(x + width/2, women_means1, width, label='Agent acceptance rates')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax[1].set_ylabel('acceptance rate', fontsize=20)\n",
    "ax[1].set_xlabel('trip distance (km)', fontsize=20)\n",
    "ax[1].set_title('acceptance rate variation with trip distance', fontsize=20)\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(labels1, fontsize=16)\n",
    "handles, labels = ax[1].get_legend_handles_labels()    \n",
    "ax[1].legend(handles, labels, loc=\"center\", bbox_to_anchor=(-0.15,-0.2),prop={'size': 18},ncol=2)\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "#autolabel(rects1)\n",
    "#autolabel(rects2)\n",
    "\n",
    "#fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('file', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize progress\n",
    "iterations = range(0, num_iterations +1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.2        39.5        13.         51.2430783  19.32878666 33.        ]]\n",
      "70.63999999999999\n",
      "[[ 4.3         6.5         6.         57.07621934 17.05308593 32.        ]]\n",
      "-8.439999999999998\n",
      "[[ 3.         22.2        13.         45.50722993  0.6880211  31.        ]]\n",
      "50.640000000000015\n",
      "driver reward  112.84\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.         35.2        16.          1.96451063 19.21204048 30.        ]]\n",
      "58.24000000000001\n",
      "0\n",
      "[[12.5        35.8        18.         41.10034602 11.81089561 28.        ]]\n",
      "29.560000000000002\n",
      "[[ 6.6   27.9   19.    37.739 32.809 27.   ]]\n",
      "44.400000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.8        24.5        20.         27.26182283 57.83749943 23.        ]]\n",
      "66.16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.9        30.          7.         28.44144467 27.45863014 18.        ]]\n",
      "76.28000000000003\n",
      "[[ 8.    26.9   11.    27.182 48.062 17.   ]]\n",
      "31.680000000000007\n",
      "0\n",
      "[[ 6.6        29.5        12.         22.75285443 28.56852327 15.        ]]\n",
      "49.52000000000001\n",
      "[[ 4.3        19.3        12.         45.50746395 34.41893341 14.        ]]\n",
      "32.51999999999998\n",
      "[[ 9.6         7.8        13.         48.50489556 42.86597424 13.        ]]\n",
      "-40.31999999999999\n",
      "[[ 3.1        12.9        13.         43.79436414 29.34353211 12.        ]]\n",
      "20.200000000000003\n",
      "[[ 4.4        17.7        17.         55.32116354 41.92657889 11.        ]]\n",
      "26.72\n",
      "[[ 6.6   45.1   19.     4.905 40.592 10.   ]]\n",
      "99.44\n",
      "0\n",
      "[[ 2.4        24.4        19.         25.81896674 54.96921258  8.        ]]\n",
      "61.76000000000002\n",
      "[[10.8        21.5        20.         24.43507778 27.93600624  7.        ]]\n",
      "-4.639999999999986\n",
      "[[ 2.4    8.3   21.    17.256 20.673  6.   ]]\n",
      "10.239999999999995\n",
      "[[12.7        33.2         4.         14.5726011  56.98913427  5.        ]]\n",
      "19.879999999999995\n",
      "0\n",
      "[[ 1.2        18.6         7.         18.52689193 20.95487803  3.        ]]\n",
      "51.359999999999985\n",
      "0\n",
      "[[ 4.3        38.8         7.         34.0108423  55.36997707  1.        ]]\n",
      "1594.92\n",
      "0\n",
      "[[ 2.3        20.3        10.          7.41851336 34.48329566 -1.        ]]\n",
      "49.31999999999999\n",
      "[[ 1.3        24.6        10.         24.15157719 16.93477344 -1.        ]]\n",
      "69.88\n",
      "[[ 5.9   17.7   10.     8.64  24.322 -1.   ]]\n",
      "16.519999999999982\n",
      "[[ 2.8        26.8        10.         11.67697562 53.63381651 -1.        ]]\n",
      "66.72\n",
      "0\n",
      "0\n",
      "[[19.2        24.3        11.         20.36295063 29.03982283 -1.        ]]\n",
      "-52.80000000000001\n",
      "0\n",
      "[[ 5.         25.6        12.         26.35037103 17.57421116 -1.        ]]\n",
      "47.91999999999999\n",
      "0\n",
      "0\n",
      "[[ 9.2        25.2        13.         23.56095797 27.78049681 -1.        ]]\n",
      "18.080000000000013\n",
      "[[ 1.          7.9        13.         16.99966791 25.34050829 -1.        ]]\n",
      "18.479999999999997\n",
      "[[ 4.1        28.2        13.         39.51884405 14.6936176  -1.        ]]\n",
      "62.360000000000014\n",
      "[[11.         12.1        21.         50.24256649  0.05067108 -1.        ]]\n",
      "-36.08000000000001\n",
      "driver reward  2488.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.6   23.9   14.    32.929 22.852 30.   ]]\n",
      "52.0\n",
      "0\n",
      "0\n",
      "[[ 5.8        36.6        16.         33.08966402 55.68725339 27.        ]]\n",
      "77.68\n",
      "[[ 7.7       15.1       18.        43.6771246 44.2257315 26.       ]]\n",
      "-4.039999999999992\n",
      "[[ 4.4   12.    20.    34.953 33.498 25.   ]]\n",
      "8.480000000000018\n",
      "0\n",
      "[[ 3.5        34.6        23.         32.75063662 53.59677573 23.        ]]\n",
      "86.92000000000002\n",
      "[[10.3        24.3         5.         19.05614752 35.09382301 22.        ]]\n",
      "7.719999999999999\n",
      "[[ 6.         20.9         6.         44.18828028 38.78512119 21.        ]]\n",
      "26.080000000000013\n",
      "0\n",
      "0\n",
      "[[ 3.3        30.8        12.         29.44953519 22.33426453 18.        ]]\n",
      "76.12\n",
      "[[ 3.2         0.8        12.         32.22559992 21.99476289 17.        ]]\n",
      "-19.2\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.8        37.5        13.         13.58689772  6.36189549 13.        ]]\n",
      "46.56\n",
      "[[22.5        35.7        13.         39.07338644 35.77727938 12.        ]]\n",
      "-38.75999999999999\n",
      "[[15.5        24.5        16.         20.06592383 18.95368312 11.        ]]\n",
      "-27.0\n",
      "[[11.         29.7        16.         28.55420552  1.26747109 10.        ]]\n",
      "20.24000000000001\n",
      "[[ 8.2   19.    17.     3.481 11.269  9.   ]]\n",
      "5.0400000000000205\n",
      "0\n",
      "[[ 4.3        12.3        17.         12.28354684 23.18763771  7.        ]]\n",
      "10.11999999999999\n",
      "[[ 6.1         8.2        17.         21.35826796 26.77591382  6.        ]]\n",
      "-15.239999999999995\n",
      "0\n",
      "[[15.3   15.8   17.    14.048 19.36   4.   ]]\n",
      "-53.48000000000002\n",
      "[[ 5.     6.3   17.    14.115 13.713  3.   ]]\n",
      "-13.840000000000003\n",
      "[[ 9.6        39.5        18.         43.90362079  9.9595699   2.        ]]\n",
      "61.120000000000005\n",
      "[[ 7.6   35.3   16.     5.699 29.43   1.   ]]\n",
      "1561.28\n",
      "[[ 2.7        16.4        16.         14.94130825 16.93809349  0.        ]]\n",
      "34.120000000000005\n",
      "[[ 9.7   22.3   16.     7.902 42.474 -1.   ]]\n",
      "5.400000000000006\n",
      "[[13.         19.5        16.         16.30575701 40.78737804 -1.        ]]\n",
      "-26.0\n",
      "[[ 4.2        26.7        16.         20.11212025 10.32410558 -1.        ]]\n",
      "56.880000000000024\n",
      "0\n",
      "[[ 9.         51.1        17.         57.71264169 32.6297849  -1.        ]]\n",
      "102.32\n",
      "[[ 7.9         8.6        12.         58.1740884  31.76874099 -1.        ]]\n",
      "-26.200000000000003\n",
      "[[ 2.4        19.2        14.         44.30648274 19.13174693 -1.        ]]\n",
      "45.120000000000005\n",
      "[[ 5.5   12.5    8.    29.951 29.406 -1.   ]]\n",
      "2.6000000000000085\n",
      "[[ 6.2        29.6         8.         55.70058117 21.63136852 -1.        ]]\n",
      "52.559999999999974\n",
      "[[10.1        25.8        13.         54.30499313  4.14255037 -1.        ]]\n",
      "13.880000000000024\n",
      "driver reward  2128.48\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.9        21.4         8.         32.43840732 41.97503446 37.        ]]\n",
      "55.56000000000003\n",
      "0\n",
      "[[ 9.1        27.5        11.         39.41220761 42.02479329 35.        ]]\n",
      "26.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.7        18.5         8.         25.64540677 22.81914158 30.        ]]\n",
      "13.640000000000015\n",
      "0\n",
      "[[17.9        46.3        10.         38.57246067 49.77482746 28.        ]]\n",
      "26.44000000000011\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.4        36.7        12.         18.11469645 11.9593856  23.        ]]\n",
      "19.519999999999982\n",
      "0\n",
      "[[15.4        24.7        13.         28.18437441  5.83092413 21.        ]]\n",
      "-25.680000000000007\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.1        34.1        14.         35.61135802 38.49288803 13.        ]]\n",
      "67.63999999999999\n",
      "[[12.2   20.1   14.    22.809 10.353 12.   ]]\n",
      "-18.639999999999986\n",
      "[[ 2.5   23.7   16.     6.894 26.846 11.   ]]\n",
      "58.84\n",
      "0\n",
      "0\n",
      "[[ 8.3        12.5        16.         16.59669967 26.19368645  8.        ]]\n",
      "-16.439999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.7        15.1        17.         17.67170544 30.36512812  4.        ]]\n",
      "-17.639999999999986\n",
      "[[19.         24.7        17.         11.99154402 40.91981069  3.        ]]\n",
      "-50.160000000000025\n",
      "[[16.1        23.5        17.         38.21852938 13.99975523  2.        ]]\n",
      "-34.28000000000003\n",
      "0\n",
      "[[ 6.3   14.4   23.    15.779 25.313  0.   ]]\n",
      "3.240000000000009\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[16.         13.5         8.         15.12214029 23.36311776 -1.        ]]\n",
      "-65.6\n",
      "[[ 9.1        13.5         8.          0.41147692 10.92522755 -1.        ]]\n",
      "-18.680000000000007\n",
      "[[ 4.9   11.1    8.    10.009 23.591 -1.   ]]\n",
      "2.200000000000003\n",
      "[[ 9.3        40.3         8.         50.32181373 21.93733305 -1.        ]]\n",
      "65.72000000000003\n",
      "[[22.7        39.8        20.         49.04023403  1.19079294 -1.        ]]\n",
      "-27.0\n",
      "[[12.8        14.         12.         46.04192447 12.07555336 -1.        ]]\n",
      "-42.24000000000001\n",
      "0\n",
      "[[ 6.7        37.9        13.         52.28848358  2.16285395 -1.        ]]\n",
      "75.71999999999997\n",
      "driver reward  98.2800000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.5        22.8        12.         32.78871558 29.7574934  36.        ]]\n",
      "21.960000000000008\n",
      "0\n",
      "0\n",
      "[[ 5.9        17.8        19.         48.0905738  14.96623789 33.        ]]\n",
      "16.839999999999975\n",
      "[[ 7.5   38.2    8.     8.622 20.727 32.   ]]\n",
      "71.24000000000001\n",
      "0\n",
      "0\n",
      "[[ 5.2        36.8        10.         45.02106007 19.06681431 29.        ]]\n",
      "82.40000000000003\n",
      "[[ 3.9        18.7         8.         28.78385765 19.40886607 28.        ]]\n",
      "33.32000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[15.5        48.5        12.         50.89823424 56.85631824 20.        ]]\n",
      "49.80000000000001\n",
      "[[ 3.4         9.         17.         42.80042217 54.08453    19.        ]]\n",
      "5.680000000000007\n",
      "[[10.4        20.4        18.         42.58643475 40.84410061 18.        ]]\n",
      "-5.439999999999969\n",
      "[[ 7.3        23.5        19.         57.14235647 48.43799575 17.        ]]\n",
      "25.560000000000002\n",
      "0\n",
      "[[13.6        39.9        10.         17.72802415  8.63758892 15.        ]]\n",
      "35.19999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.1        26.9        15.         24.71882028 34.60109735  8.        ]]\n",
      "-3.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.7        21.5        17.         34.3967459  22.04908009  7.        ]]\n",
      "30.04000000000002\n",
      "[[ 6.6        22.6        19.         43.94979636  7.95079364  6.        ]]\n",
      "27.439999999999998\n",
      "driver reward  391.0400000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.5        29.7        17.         42.22705811 51.45213318 31.        ]]\n",
      "30.439999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.2        19.5        21.         34.66130283 40.17524641 26.        ]]\n",
      "20.24000000000001\n",
      "[[ 5.6        18.5        23.         41.32182016 51.63371454 25.        ]]\n",
      "21.120000000000005\n",
      "[[ 2.9        12.          0.         44.19246369 37.01732336 24.        ]]\n",
      "18.680000000000007\n",
      "[[ 7.9        17.9         8.         53.11921537 32.51309738 23.        ]]\n",
      "3.5600000000000307\n",
      "[[17.         26.2        12.         46.64092055 18.34635392 22.        ]]\n",
      "-31.75999999999999\n",
      "[[15.4        33.3        15.          6.28730047 25.44994481 21.        ]]\n",
      "1.8400000000000318\n",
      "0\n",
      "[[17.3        30.5        15.         28.66769387 30.21042278 19.        ]]\n",
      "-20.039999999999964\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.8        14.3        16.         38.92876707 51.69170612 14.        ]]\n",
      "-48.08000000000001\n",
      "0\n",
      "[[ 5.6   35.2   17.    11.071 20.298 12.   ]]\n",
      "74.56\n",
      "0\n",
      "0\n",
      "[[ 7.6        15.6        17.         18.12848856 24.640352    9.        ]]\n",
      "-1.759999999999991\n",
      "[[ 6.6        13.8        17.         38.31561294 25.21450498  8.        ]]\n",
      "-0.7199999999999989\n",
      "[[ 8.8        30.          1.          5.21230522 14.4900728   7.        ]]\n",
      "36.160000000000025\n",
      "[[ 3.3        35.7         6.         22.97654486 46.50808245  6.        ]]\n",
      "91.80000000000001\n",
      "0\n",
      "[[10.3        19.1         7.         13.71454456 36.75964686  4.        ]]\n",
      "-8.920000000000016\n",
      "0\n",
      "[[ 8.1        24.4         7.         20.85865415 23.64126542  2.        ]]\n",
      "23.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.7        15.4         8.         13.42679194 23.51740166 -1.        ]]\n",
      "17.319999999999993\n",
      "[[17.         32.7         9.         29.75074393 18.3950927  -1.        ]]\n",
      "-10.960000000000036\n",
      "[[ 7.1        18.4        12.          9.77038092 19.87400855 -1.        ]]\n",
      "10.599999999999994\n",
      "[[ 4.8   30.6   12.    21.435 43.337 -1.   ]]\n",
      "65.28000000000003\n",
      "[[ 1.3   24.3   12.     9.353 22.595 -1.   ]]\n",
      "68.91999999999999\n",
      "0\n",
      "[[14.4        27.7        12.         31.06155372 40.4474949  -1.        ]]\n",
      "-9.28000000000003\n",
      "[[ 4.6        16.8        12.         12.12241785 31.41217184 -1.        ]]\n",
      "22.480000000000018\n",
      "[[ 8.2        32.         12.         12.86478278  4.10751309 -1.        ]]\n",
      "46.639999999999986\n",
      "[[ 2.6   26.2   13.     2.49  30.192 -1.   ]]\n",
      "66.16\n",
      "[[ 2.5   16.4   13.    17.927 19.246 -1.   ]]\n",
      "35.48000000000002\n",
      "[[ 7.4         7.5        13.         11.02446974 13.79126011 -1.        ]]\n",
      "-26.319999999999993\n",
      "[[ 1.8        22.3        13.         32.34597556 14.63127624 -1.        ]]\n",
      "59.120000000000005\n",
      "[[ 7.1   23.7   13.     4.242 23.305 -1.   ]]\n",
      "27.56000000000003\n",
      "0\n",
      "0\n",
      "[[ 4.3   22.9   14.     4.026 34.487 -1.   ]]\n",
      "44.04000000000002\n",
      "0\n",
      "[[ 1.5        17.         14.         13.86503322 32.24833195 -1.        ]]\n",
      "44.2\n",
      "0\n",
      "[[ 6.8        20.4        14.          5.09180455 15.94492719 -1.        ]]\n",
      "19.04000000000002\n",
      "[[13.3        12.7        15.         12.07670731 21.86303033 -1.        ]]\n",
      "-49.79999999999998\n",
      "[[10.7        20.6        15.         20.31336202 16.31540861 -1.        ]]\n",
      "-6.840000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.         16.7        16.         18.91622098 39.55541461 -1.        ]]\n",
      "33.04000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.8        25.7        18.         22.78458113 18.0609713  -1.        ]]\n",
      "2.0\n",
      "[[25.2   25.8   18.     3.252 12.144 -1.   ]]\n",
      "-88.80000000000001\n",
      "[[10.         16.8        18.         18.78637269 31.61249544 -1.        ]]\n",
      "-14.240000000000009\n",
      "0\n",
      "[[ 1.         29.2        18.         31.0675483  36.61243995 -1.        ]]\n",
      "86.64000000000001\n",
      "[[ 2.8        13.6        19.         27.31170635 48.11666554 -1.        ]]\n",
      "24.480000000000018\n",
      "[[12.1   32.3   20.     2.05  17.369 -1.   ]]\n",
      "21.08000000000004\n",
      "[[ 4.7         8.8        20.         13.82295197 14.09940995 -1.        ]]\n",
      "-3.799999999999997\n",
      "0\n",
      "0\n",
      "[[ 8.1        15.5        20.         18.66472173 34.28130135 -1.        ]]\n",
      "-5.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.7        17.2        21.         11.77813337 29.29044078 -1.        ]]\n",
      "16.28\n",
      "0\n",
      "[[12.2        19.2        21.         13.30922524 22.80225638 -1.        ]]\n",
      "-21.519999999999982\n",
      "[[ 7.2        15.3        21.         20.617533   29.40025824 -1.        ]]\n",
      "0.0\n",
      "[[ 4.4        22.9        21.         38.8322791  32.82086793 -1.        ]]\n",
      "43.360000000000014\n",
      "[[13.7        19.6         7.         46.78347598  9.02260203 -1.        ]]\n",
      "-30.43999999999997\n",
      "driver reward  696.3600000000006\n",
      "0\n",
      "[[ 3.5        31.9        11.         41.04969913 28.71334823 39.        ]]\n",
      "78.28000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.2   28.4   14.     4.767 21.051 24.   ]]\n",
      "41.91999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.7        29.2        15.          1.08990215  6.8177949  20.        ]]\n",
      "75.08000000000001\n",
      "0\n",
      "[[ 3.7        52.6        16.         47.59673426 38.22596082 18.        ]]\n",
      "143.15999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.5        42.5        17.         34.82488473 17.55742633 13.        ]]\n",
      "98.60000000000002\n",
      "0\n",
      "0\n",
      "[[10.6        17.3        20.         22.59261347 45.12223047 10.        ]]\n",
      "-16.72\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[19.7        36.3        22.         29.64639781 12.41526639  5.        ]]\n",
      "-17.80000000000001\n",
      "[[18.         30.1         7.          6.60362413 38.08036581  4.        ]]\n",
      "-26.079999999999984\n",
      "0\n",
      "[[ 5.1        12.7         8.         16.13972437 27.75756603  2.        ]]\n",
      "5.960000000000022\n",
      "[[13.3         8.5         8.         15.79543028 20.97679915  1.        ]]\n",
      "1436.76\n",
      "[[12.4        31.7         8.         26.11417817 41.32106278  0.        ]]\n",
      "17.120000000000005\n",
      "0\n",
      "[[10.9       23.7       10.        31.9484737 59.5207776 -1.       ]]\n",
      "1.7199999999999989\n",
      "[[11.9   19.4   13.    11.324 36.464 -1.   ]]\n",
      "-18.839999999999975\n",
      "0\n",
      "[[ 9.         41.2        13.         47.26405187 25.28181461 -1.        ]]\n",
      "70.63999999999999\n",
      "[[16.    21.9    7.    12.944 40.763 -1.   ]]\n",
      "-38.71999999999997\n",
      "0\n",
      "[[ 6.1        23.8         7.         28.44749088 49.99830774 -1.        ]]\n",
      "34.68000000000001\n",
      "[[19.7        18.2        11.         25.82888575 19.22680003 -1.        ]]\n",
      "-75.71999999999997\n",
      "[[ 5.1        35.3        12.          0.24005649 49.92455648 -1.        ]]\n",
      "78.28000000000003\n",
      "0\n",
      "0\n",
      "[[ 2.7        46.6        12.         24.52842768 11.11579969 -1.        ]]\n",
      "130.76\n",
      "[[24.2        28.         13.         32.98894525 15.63275325 -1.        ]]\n",
      "-74.96000000000004\n",
      "[[ 3.5        30.9        13.         56.15552562 40.94678975 -1.        ]]\n",
      "75.08000000000001\n",
      "[[14.9        9.4       17.        55.7629546 46.4623954 -1.       ]]\n",
      "-71.24000000000001\n",
      "[[12.7        18.9         7.         24.96160788 39.49560803 -1.        ]]\n",
      "-25.879999999999967\n",
      "0\n",
      "[[16.4        39.2         8.         29.65377654 22.38024697 -1.        ]]\n",
      "13.920000000000016\n",
      "[[ 4.7         5.6         8.         21.71608131 27.89345561 -1.        ]]\n",
      "-14.040000000000006\n",
      "[[14.6        18.5         9.         15.67660798 36.05796242 -1.        ]]\n",
      "-40.08000000000001\n",
      "0\n",
      "[[ 5.2        30.3        10.         30.32372061 46.98428206 -1.        ]]\n",
      "61.599999999999994\n",
      "[[ 6.3        10.4        11.         35.46964845 53.61524691 -1.        ]]\n",
      "-9.559999999999988\n",
      "[[ 5.9   31.7   13.     8.424 50.189 -1.   ]]\n",
      "61.31999999999999\n",
      "[[ 8.3        19.7        13.          5.44699925 25.88554747 -1.        ]]\n",
      "6.599999999999994\n",
      "0\n",
      "0\n",
      "[[12.7        13.4        13.         19.72248041  8.49577403 -1.        ]]\n",
      "-43.48000000000002\n",
      "[[ 8.2        14.4        15.         31.65581816  4.90019113 -1.        ]]\n",
      "-9.680000000000007\n",
      "[[19.7        30.         17.         12.77111248 46.17596938 -1.        ]]\n",
      "-37.960000000000036\n",
      "[[12.6        23.         17.         33.22757726 21.78861761 -1.        ]]\n",
      "-12.080000000000013\n",
      "[[ 8.6   31.6   19.    11.076 49.394 -1.   ]]\n",
      "42.639999999999986\n",
      "[[ 9.8        24.5        20.          4.18406261 35.12434386 -1.        ]]\n",
      "11.76000000000002\n",
      "[[ 6.         27.7        20.         28.90206564 39.34022209 -1.        ]]\n",
      "47.839999999999975\n",
      "[[ 9.6   42.1   20.     2.078 17.923 -1.   ]]\n",
      "69.44\n",
      "0\n",
      "0\n",
      "0\n",
      "[[18.1        20.6        20.         37.33012703 35.67657516 -1.        ]]\n",
      "-57.160000000000025\n",
      "0\n",
      "0\n",
      "[[18.5        20.2         7.         24.44510889 47.65630313 -1.        ]]\n",
      "-61.160000000000025\n",
      "0\n",
      "0\n",
      "[[10.5        31.1         8.         15.4622332  31.91196914 -1.        ]]\n",
      "28.120000000000005\n",
      "0\n",
      "[[ 8.5        26.9         8.         32.22235509 37.75959497 -1.        ]]\n",
      "28.28000000000003\n",
      "0\n",
      "0\n",
      "[[11.         31.         12.         24.50192073 11.91818734 -1.        ]]\n",
      "24.400000000000034\n",
      "[[ 7.5        24.5        12.         22.13614926 41.65382677 -1.        ]]\n",
      "27.400000000000006\n",
      "0\n",
      "[[ 6.         21.9        13.          2.28579487 30.41555054 -1.        ]]\n",
      "29.28\n",
      "[[15.4        34.7        14.         36.54839691 45.31211218 -1.        ]]\n",
      "6.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 4.8        30.2        14.          9.41589529 15.89477896 -1.        ]]\n",
      "64.0\n",
      "[[ 6.8        20.3        15.         22.47584321 35.47329073 -1.        ]]\n",
      "18.72\n",
      "0\n",
      "[[ 3.4        35.8        16.         36.63403106  8.18450695 -1.        ]]\n",
      "91.44000000000005\n",
      "[[11.5   37.    12.     7.139 45.451 -1.   ]]\n",
      "40.19999999999999\n",
      "[[ 0.7        33.9        12.          3.77650266 12.16068963 -1.        ]]\n",
      "103.72\n",
      "[[16.2        21.7        13.         23.18059207 41.86723636 -1.        ]]\n",
      "-40.71999999999997\n",
      "[[15.    16.1   13.     4.355 19.563 -1.   ]]\n",
      "-50.48000000000002\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.8        40.3        14.         43.96910458  0.76922335 -1.        ]]\n",
      "103.12000000000006\n",
      "[[ 2.          7.3        16.         48.00585271  8.08324055 -1.        ]]\n",
      "9.759999999999998\n",
      "driver reward  2435.56\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.5        23.6        16.         31.53379354 24.97085848 34.        ]]\n",
      "51.72\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.1        32.         18.         18.13226745 20.46507549 29.        ]]\n",
      "81.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.6        29.7        19.         24.53027413 48.6341494  19.        ]]\n",
      "50.160000000000025\n",
      "0\n",
      "[[ 4.9        26.8        20.         37.16093274 31.72507316 17.        ]]\n",
      "52.44\n",
      "[[ 8.         15.6        20.         43.3356227  36.30777483 16.        ]]\n",
      "-4.480000000000018\n",
      "0\n",
      "0\n",
      "[[13.1        40.3         9.         49.0002062  36.81574196 13.        ]]\n",
      "39.879999999999995\n",
      "[[11.9         7.7        14.         49.33032206 52.25475517 12.        ]]\n",
      "-56.28\n",
      "[[ 2.8    7.2   15.    53.82  44.863 11.   ]]\n",
      "4.0\n",
      "[[13.1         7.7        18.         48.54297897 48.93388183 10.        ]]\n",
      "-64.44\n",
      "[[12.7   19.1   20.    41.264 48.508  9.   ]]\n",
      "-25.24000000000001\n",
      "[[ 5.4        23.         20.         26.64159987 31.58896827  8.        ]]\n",
      "36.880000000000024\n",
      "[[25.5        49.3        21.         53.80626136 37.90102049  7.        ]]\n",
      "-15.639999999999986\n",
      "[[ 5.1        15.         10.         41.69420366 44.89340967  6.        ]]\n",
      "13.319999999999993\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.9        27.9        12.         25.77414084 21.85013848  1.        ]]\n",
      "1549.16\n",
      "[[13.3        27.5        14.         28.12254474 45.37114009  0.        ]]\n",
      "-2.4399999999999977\n",
      "[[14.8        22.         14.         21.03050855 38.59961122 -1.        ]]\n",
      "-30.23999999999998\n",
      "[[ 6.5        19.1        14.         32.33791532 32.70263889 -1.        ]]\n",
      "16.919999999999987\n",
      "[[ 2.1        17.8        15.         24.91367671 14.26069839 -1.        ]]\n",
      "42.67999999999998\n",
      "[[16.3        15.1        17.         23.03713833 24.03514239 -1.        ]]\n",
      "-62.51999999999998\n",
      "0\n",
      "0\n",
      "[[12.7        27.6        19.         16.81172609 38.89438521 -1.        ]]\n",
      "1.9600000000000364\n",
      "0\n",
      "0\n",
      "[[ 6.9        24.         19.         25.44220057 46.05619108 -1.        ]]\n",
      "29.880000000000024\n",
      "0\n",
      "[[ 3.2        16.8        20.         23.1071594  31.97580743 -1.        ]]\n",
      "32.0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.1        27.8        21.         34.01634607 41.91825654 -1.        ]]\n",
      "40.68000000000001\n",
      "[[11.9   31.8    7.    13.541 55.022 -1.   ]]\n",
      "20.839999999999975\n",
      "[[ 5.8        27.8         8.         31.58489316 29.27696961 -1.        ]]\n",
      "49.52000000000001\n",
      "[[ 7.5         7.5         9.         20.41943827 20.76921653 -1.        ]]\n",
      "-27.0\n",
      "[[16.5        20.7         9.         11.53368643 12.14328869 -1.        ]]\n",
      "-45.96000000000001\n",
      "[[ 6.2         8.1        11.         13.27361354 15.11154545 -1.        ]]\n",
      "-16.24000000000001\n",
      "[[12.         15.         12.         19.88050263  3.37049294 -1.        ]]\n",
      "-33.599999999999994\n",
      "[[15.3        17.7        12.         19.09151254 14.26936996 -1.        ]]\n",
      "-47.400000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.7        21.6        12.         32.14577272 54.67018633 -1.        ]]\n",
      "-10.43999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.1        19.1        14.         14.50786159 32.05666705 -1.        ]]\n",
      "40.03999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.3        26.6        14.          5.48263246 27.10043495 -1.        ]]\n",
      "28.67999999999998\n",
      "[[ 8.6         9.         14.         16.56430614 30.80690941 -1.        ]]\n",
      "-29.680000000000007\n",
      "[[15.5        23.7        14.         14.49266483 19.404864   -1.        ]]\n",
      "-29.560000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.4        29.9        16.         37.39534611 40.35601603 -1.        ]]\n",
      "79.36000000000001\n",
      "[[ 4.3        11.6        17.         30.20755638 52.15762657 -1.        ]]\n",
      "7.88000000000001\n",
      "0\n",
      "0\n",
      "[[ 1.3   27.3   17.    22.358 39.537 -1.   ]]\n",
      "78.51999999999998\n",
      "[[ 2.9        12.         17.         28.31777518 51.84305008 -1.        ]]\n",
      "18.680000000000007\n",
      "[[ 9.7         6.5        17.         29.70574193 37.31120794 -1.        ]]\n",
      "-45.16\n",
      "[[13.4        23.5        17.          8.66468909 29.11224915 -1.        ]]\n",
      "-15.919999999999987\n",
      "[[11.5        37.1        18.         28.91142012 46.0593226  -1.        ]]\n",
      "40.51999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[20.    22.6   18.    33.839 24.239 -1.   ]]\n",
      "-63.68000000000001\n",
      "0\n",
      "[[ 3.2        26.4        19.         16.20689127  1.90057552 -1.        ]]\n",
      "62.72000000000003\n",
      "0\n",
      "0\n",
      "[[18.         37.         19.         32.12234817 23.40017575 -1.        ]]\n",
      "-4.0\n",
      "[[12.1         4.2        20.         19.80786497 21.73786506 -1.        ]]\n",
      "-68.84\n",
      "[[14.8        27.6        20.         31.97572386 36.84513826 -1.        ]]\n",
      "-12.32000000000005\n",
      "[[10.1   27.8   21.     4.06  11.716 -1.   ]]\n",
      "20.28000000000003\n",
      "[[ 8.1         9.          1.         14.30470431  8.55881821 -1.        ]]\n",
      "-26.28\n",
      "[[14.7        15.          6.         16.8807836  19.60771503 -1.        ]]\n",
      "-51.95999999999998\n",
      "[[ 6.          5.          8.         26.48814839 21.10815309 -1.        ]]\n",
      "-24.799999999999997\n",
      "0\n",
      "[[12.6   30.1    8.     8.816 45.058 -1.   ]]\n",
      "10.639999999999986\n",
      "[[ 3.5        36.9         9.         37.42627557 27.62981403 -1.        ]]\n",
      "94.28000000000003\n",
      "[[ 6.2        22.2        12.         14.22682772 42.72789007 -1.        ]]\n",
      "28.880000000000024\n",
      "[[ 4.9        15.7        12.         27.83613747 43.80126724 -1.        ]]\n",
      "16.919999999999987\n",
      "0\n",
      "0\n",
      "[[ 2.2        26.6        13.         24.34514507 24.16082098 -1.        ]]\n",
      "70.16\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.8        24.8        13.         25.02039313 34.95124012 -1.        ]]\n",
      "60.31999999999999\n",
      "0\n",
      "0\n",
      "[[ 6.8        19.8        14.         21.18910817 53.59531811 -1.        ]]\n",
      "17.120000000000005\n",
      "0\n",
      "[[17.6        29.8        14.          9.73274787 22.5299857  -1.        ]]\n",
      "-24.32000000000005\n",
      "0\n",
      "[[ 8.5        19.3        14.         13.60024984  8.32844047 -1.        ]]\n",
      "3.960000000000008\n",
      "[[16.4        29.         16.         33.80221882 17.43460047 -1.        ]]\n",
      "-18.71999999999997\n",
      "0\n",
      "[[ 7.6        13.5        17.         16.90844297 14.37535764 -1.        ]]\n",
      "-8.480000000000018\n",
      "[[ 7.2        24.1        18.          2.02418244 34.81352012 -1.        ]]\n",
      "28.159999999999997\n",
      "[[11.5        19.5        18.         23.44392417 33.39216383 -1.        ]]\n",
      "-15.799999999999983\n",
      "0\n",
      "[[ 6.6   10.1   19.    19.863 39.246 -1.   ]]\n",
      "-12.559999999999988\n",
      "[[16.3        23.5        19.         15.12469253  6.00859224 -1.        ]]\n",
      "-35.639999999999986\n",
      "[[13.2        19.6        21.          7.29466343 20.82690379 -1.        ]]\n",
      "-27.039999999999964\n",
      "0\n",
      "[[ 7.1   10.4    7.    11.996 19.728 -1.   ]]\n",
      "-15.0\n",
      "[[13.9        15.7         8.         15.11877125 19.09702568 -1.        ]]\n",
      "-44.28\n",
      "[[14.8        16.3        10.         19.8733498  21.60150715 -1.        ]]\n",
      "-48.48000000000002\n",
      "0\n",
      "[[ 5.1        18.6        10.         21.70863954 15.03390302 -1.        ]]\n",
      "24.839999999999975\n",
      "[[ 1.2   33.1   11.    23.72  49.303 -1.   ]]\n",
      "97.75999999999999\n",
      "0\n",
      "0\n",
      "[[ 5.2        30.3        12.         23.09188816 24.95608375 -1.        ]]\n",
      "61.599999999999994\n",
      "[[10.5        16.5        12.         32.58507116 31.08126552 -1.        ]]\n",
      "-18.599999999999994\n",
      "[[23.7        26.6        14.          5.20868642 25.31573546 -1.        ]]\n",
      "-76.03999999999996\n",
      "0\n",
      "0\n",
      "[[ 9.2        25.8        14.         28.37782991 52.17856004 -1.        ]]\n",
      "20.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.4        16.3        17.         19.65127851 42.28412001 -1.        ]]\n",
      "-11.76000000000002\n",
      "[[ 5.3        10.3        17.         29.65102962 30.85475859 -1.        ]]\n",
      "-3.0800000000000125\n",
      "[[ 9.3   22.3   17.     3.568 28.125 -1.   ]]\n",
      "8.120000000000005\n",
      "[[ 2.7        15.         17.         16.7511901  16.43884563 -1.        ]]\n",
      "29.640000000000015\n",
      "0\n",
      "[[ 9.6        27.         17.         19.68672957 21.27062894 -1.        ]]\n",
      "21.120000000000005\n",
      "[[12.8         9.9        17.         14.99130289 21.8077112  -1.        ]]\n",
      "-55.360000000000014\n",
      "[[15.         19.9        17.         19.41617499 19.6860849  -1.        ]]\n",
      "-38.31999999999999\n",
      "0\n",
      "0\n",
      "[[11.7        37.5        18.          0.39087975  2.79639439 -1.        ]]\n",
      "40.44\n",
      "[[ 2.8        12.9        18.         14.94578887  8.80112587 -1.        ]]\n",
      "22.24000000000001\n",
      "0\n",
      "0\n",
      "[[ 5.1   30.9   20.    35.094 45.882 -1.   ]]\n",
      "64.20000000000002\n",
      "[[11.2        15.5        23.         32.83149226 57.58084797 -1.        ]]\n",
      "-26.560000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.5        15.8         6.         20.46721739 33.41294517 -1.        ]]\n",
      "-14.039999999999992\n",
      "[[12.7        16.4         7.         36.39697955 30.81055028 -1.        ]]\n",
      "-33.87999999999997\n",
      "[[ 1.7    7.8   10.    28.435 35.887 -1.   ]]\n",
      "13.400000000000006\n",
      "[[ 7.1         2.6        10.         26.78915126 28.93574909 -1.        ]]\n",
      "-39.959999999999994\n",
      "[[11.2        31.3        10.         40.43845953 14.1070739  -1.        ]]\n",
      "24.0\n",
      "[[16.9   29.9   14.     8.874 48.648 -1.   ]]\n",
      "-19.239999999999952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.6        25.6        14.         25.31134782 26.53641193 -1.        ]]\n",
      "16.639999999999986\n",
      "0\n",
      "[[11.1        12.3        14.         20.05334715 35.51396988 -1.        ]]\n",
      "-36.119999999999976\n",
      "[[ 5.6        20.2        14.         32.96211573 28.12348358 -1.        ]]\n",
      "26.56000000000003\n",
      "[[ 9.1        14.2        14.         32.16402155 46.44409902 -1.        ]]\n",
      "-16.43999999999997\n",
      "0\n",
      "[[ 9.3        18.3        15.          3.08897228 28.1570356  -1.        ]]\n",
      "-4.680000000000007\n",
      "0\n",
      "[[ 5.5        41.5        15.         47.47188715 42.14761042 -1.        ]]\n",
      "95.40000000000003\n",
      "[[ 6.2   45.4   15.     7.878 43.289 -1.   ]]\n",
      "103.12\n",
      "0\n",
      "0\n",
      "[[11.8        24.8        16.          1.75211253 27.19858773 -1.        ]]\n",
      "-0.8799999999999955\n",
      "0\n",
      "[[12.3        16.2        16.         16.01638203 37.48175897 -1.        ]]\n",
      "-31.799999999999983\n",
      "[[10.1        18.6        16.         16.02745565 13.69229117 -1.        ]]\n",
      "-9.160000000000025\n",
      "[[ 9.2   13.8   16.    33.088  4.289 -1.   ]]\n",
      "-18.400000000000006\n",
      "driver reward  1970.800000000001\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.9        27.8        14.         40.60465366 38.14677643 36.        ]]\n",
      "42.03999999999999\n",
      "[[ 3.5        29.1        16.         52.86917875  9.31438936 35.        ]]\n",
      "69.32\n",
      "[[ 5.          6.1         8.         51.33928607 20.20695287 34.        ]]\n",
      "-14.47999999999999\n",
      "[[11.7        16.7        16.         59.0687891  27.64565422 33.        ]]\n",
      "-26.119999999999976\n",
      "[[17.9   34.    18.    29.334 29.854 32.   ]]\n",
      "-12.919999999999959\n",
      "0\n",
      "[[ 1.1        33.8        20.         51.55814438 58.70480076 30.        ]]\n",
      "100.68\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.6        35.7        21.         21.75407717 12.89529425 26.        ]]\n",
      "14.95999999999998\n",
      "0\n",
      "[[ 0.4        20.9        11.         15.82053798 18.70980968 24.        ]]\n",
      "64.16000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.2        42.3        13.         45.22898095 59.40318514 19.        ]]\n",
      "127.19999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.4        24.9        17.         13.00705062  5.16061093 14.        ]]\n",
      "63.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[18.5        28.5        18.         33.09874783 58.3978681   8.        ]]\n",
      "-34.599999999999966\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.         12.4         8.         17.73960217 31.38344843  2.        ]]\n",
      "19.28\n",
      "[[ 7.2         6.5         8.         29.51585067 24.98212187  1.        ]]\n",
      "1471.84\n",
      "0\n",
      "0\n",
      "[[ 3.         40.9        10.         52.60548529 34.00007775 -1.        ]]\n",
      "110.48000000000002\n",
      "[[ 3.3   14.2   12.    48.678 50.99  -1.   ]]\n",
      "23.0\n",
      "[[ 2.4        10.7        13.         37.16809858 45.28147914 -1.        ]]\n",
      "17.92\n",
      "[[15.4         7.         14.         45.10211837 51.13625319 -1.        ]]\n",
      "-82.32\n",
      "[[ 1.9         3.3        17.         48.98733419 53.46231383 -1.        ]]\n",
      "-2.3599999999999923\n",
      "driver reward  1951.4400000000003\n",
      "[[ 8.8        12.9         8.         56.39085793 41.71430914 40.        ]]\n",
      "-18.560000000000002\n",
      "[[ 8.4        26.9         8.         51.13144516 16.3512626  39.        ]]\n",
      "28.960000000000036\n",
      "[[ 7.5        15.7        19.         54.27220509 36.13309824 38.        ]]\n",
      "-0.7599999999999909\n",
      "0\n",
      "[[11.5        35.4         8.         51.56688815 17.51398574 36.        ]]\n",
      "35.08000000000004\n",
      "0\n",
      "0\n",
      "[[ 7.6   30.2   19.    25.517 39.135 33.   ]]\n",
      "44.960000000000036\n",
      "0\n",
      "0\n",
      "[[ 9.         33.2        19.         32.11300279 53.28771274 30.        ]]\n",
      "45.039999999999964\n",
      "0\n",
      "0\n",
      "[[ 5.5        21.1         6.         17.19658618 23.671412   27.        ]]\n",
      "30.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.1        18.2        12.         16.28312799 12.557696   14.        ]]\n",
      "-17.23999999999998\n",
      "0\n",
      "0\n",
      "[[ 2.1        22.         12.         12.93054877  0.70862754 11.        ]]\n",
      "56.120000000000005\n",
      "[[15.7        24.8        13.          0.49048845 12.16221002 10.        ]]\n",
      "-27.399999999999977\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.9        47.8        16.         54.94336319 27.56448711  6.        ]]\n",
      "112.84000000000003\n",
      "[[ 5.1        18.4        18.         50.26184292 12.67496164  5.        ]]\n",
      "24.200000000000017\n",
      "driver reward  313.36000000000024\n",
      "[[16.9        35.6         7.         20.52172244 27.94250573 40.        ]]\n",
      "-1.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.1        33.9        14.         39.1138089   9.93103526 34.        ]]\n",
      "53.400000000000034\n",
      "[[ 3.3   39.1   18.    14.707 43.923 33.   ]]\n",
      "102.68\n",
      "0\n",
      "0\n",
      "[[ 2.2        41.3        18.         16.64566201 14.10031426 30.        ]]\n",
      "117.19999999999999\n",
      "0\n",
      "[[ 3.         28.3        19.         22.69865639 14.29713519 28.        ]]\n",
      "70.16\n",
      "[[13.2        29.4        20.         32.72402056 51.14162069 27.        ]]\n",
      "4.32000000000005\n",
      "0\n",
      "[[ 6.         17.1        21.         26.04194755 33.06407541 25.        ]]\n",
      "13.919999999999987\n",
      "0\n",
      "0\n",
      "[[ 7.5        25.1         6.         27.93447906 48.58268788 22.        ]]\n",
      "29.319999999999993\n",
      "[[ 0.4        25.          7.         52.76705003 50.48484901 21.        ]]\n",
      "77.28\n",
      "[[ 6.1        10.7        10.         45.35947439 36.0004292  20.        ]]\n",
      "-7.239999999999981\n",
      "0\n",
      "[[14.8        35.3        15.         52.59858622 37.62689058 18.        ]]\n",
      "12.32000000000005\n",
      "[[ 6.1   12.4    8.    42.321 34.022 17.   ]]\n",
      "-1.7999999999999972\n",
      "[[ 4.3        22.7        10.         48.67462976 58.2449519  16.        ]]\n",
      "43.400000000000006\n",
      "0\n",
      "[[ 6.         17.9        17.         27.11476365 52.00955361 14.        ]]\n",
      "16.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.1        19.6        17.         21.87741676 20.53581932  8.        ]]\n",
      "0.839999999999975\n",
      "0\n",
      "[[ 1.9        30.3        17.         38.15793233 34.43988159  6.        ]]\n",
      "84.03999999999999\n",
      "0\n",
      "[[12.4        27.2        18.         30.48122238 27.54496486  4.        ]]\n",
      "2.7199999999999704\n",
      "0\n",
      "[[ 6.8        29.5        18.         18.76389402  3.29338145  2.        ]]\n",
      "48.160000000000025\n",
      "0\n",
      "[[ 8.4        18.5        19.         23.80321797 30.46073288  0.        ]]\n",
      "2.0800000000000125\n",
      "[[ 8.6   31.1   20.     8.384  1.514 -1.   ]]\n",
      "41.039999999999964\n",
      "[[ 3.2    8.6    8.     3.91  10.028 -1.   ]]\n",
      "5.759999999999991\n",
      "0\n",
      "[[ 6.2        33.4         8.         36.3073415  32.03714171 -1.        ]]\n",
      "64.71999999999997\n",
      "[[ 4.2        15.4        12.         27.2669706  39.07084109 -1.        ]]\n",
      "20.72\n",
      "0\n",
      "[[ 5.1        17.1        13.         15.24445066 38.45578504 -1.        ]]\n",
      "20.039999999999992\n",
      "0\n",
      "0\n",
      "[[12.3        23.8        14.         17.63605322 30.68229411 -1.        ]]\n",
      "-7.47999999999999\n",
      "[[10.7        30.6        14.         37.72451421 27.38030072 -1.        ]]\n",
      "25.160000000000025\n",
      "[[ 6.8    7.4   16.    38.482 31.507 -1.   ]]\n",
      "-22.559999999999988\n",
      "[[ 3.9        12.8        17.         36.76846176 21.77581317 -1.        ]]\n",
      "14.440000000000012\n",
      "[[ 3.5        25.7        17.         59.9725794  26.57440079 -1.        ]]\n",
      "58.44\n",
      "[[13.5        33.1        16.         48.02276774  5.43522576 -1.        ]]\n",
      "14.120000000000005\n",
      "[[ 3.5   51.7   13.    16.733 50.692 -1.   ]]\n",
      "141.64\n",
      "[[ 9.4        25.         14.         19.16905069 33.23232209 -1.        ]]\n",
      "16.080000000000013\n",
      "[[ 7.4        17.8        14.         32.63679176 28.24814835 -1.        ]]\n",
      "6.639999999999986\n",
      "[[17.9        33.6        15.         49.7640696  39.74339657 -1.        ]]\n",
      "-14.199999999999989\n",
      "[[ 5.5   39.    19.     5.377 40.934 -1.   ]]\n",
      "87.40000000000003\n",
      "[[ 9.         27.         20.         39.63985795 52.02310033 -1.        ]]\n",
      "25.200000000000017\n",
      "[[12.2   27.6    7.     6.57  46.122 -1.   ]]\n",
      "5.360000000000014\n",
      "[[12.2        24.5         7.         14.19848429 14.48850902 -1.        ]]\n",
      "-4.560000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.7        37.2        12.         40.16611935 26.39603567 -1.        ]]\n",
      "87.07999999999998\n",
      "[[10.7         5.8        16.         32.37117335 29.03521443 -1.        ]]\n",
      "-54.2\n",
      "[[ 7.5        27.2        16.         39.05304437 49.228251   -1.        ]]\n",
      "36.03999999999999\n",
      "[[ 9.1        29.8        17.         29.18930051 12.55281383 -1.        ]]\n",
      "33.48000000000002\n",
      "[[ 8.4        15.9        18.         35.03631993  7.87757369 -1.        ]]\n",
      "-6.240000000000009\n",
      "driver reward  1262.4000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.4        33.1        17.         36.58190868 44.15838822 28.        ]]\n",
      "96.4\n",
      "[[ 7.         10.         19.         41.30693032 39.78368886 27.        ]]\n",
      "-15.599999999999994\n",
      "[[ 2.5        39.3        23.          3.70479936 43.79486229 26.        ]]\n",
      "108.76000000000005\n",
      "[[ 2.3        35.9         1.         30.78469024 23.95578414 25.        ]]\n",
      "99.24000000000001\n",
      "0\n",
      "[[ 6.3        25.9         9.         32.79061559 31.02593635 23.        ]]\n",
      "40.04000000000005\n",
      "[[ 7.9        30.5        10.         54.93426504 26.47258337 22.        ]]\n",
      "43.879999999999995\n",
      "[[ 3.6        37.3        13.         59.90573954 59.87488933 21.        ]]\n",
      "94.88\n",
      "0\n",
      "0\n",
      "[[ 3.7        32.7         8.         33.29780463 33.24928622 18.        ]]\n",
      "79.47999999999996\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.1        21.3        16.         33.82955284 22.14310967 12.        ]]\n",
      "53.879999999999995\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 6.1        25.2        16.         23.92228914 50.95746099  8.        ]]\n",
      "39.160000000000025\n",
      "[[11.7        20.         16.         24.0732915  36.12081581  7.        ]]\n",
      "-15.560000000000002\n",
      "0\n",
      "0\n",
      "[[ 7.8        13.2        17.         21.67499399 31.88015445  4.        ]]\n",
      "-10.799999999999983\n",
      "[[17.    33.6   17.     6.412 43.181  3.   ]]\n",
      "-8.079999999999984\n",
      "0\n",
      "[[10.4        45.6        17.         32.64633529 19.2459085   1.        ]]\n",
      "1575.2\n",
      "[[16.4        10.4        17.         28.41608275 23.75577388  0.        ]]\n",
      "-78.23999999999998\n",
      "[[ 6.6   23.2   17.     4.228 38.111 -1.   ]]\n",
      "29.360000000000014\n",
      "0\n",
      "0\n",
      "[[19.9   23.5   18.     5.17  34.497 -1.   ]]\n",
      "-60.120000000000005\n",
      "[[24.1   25.1   18.    16.572 31.457 -1.   ]]\n",
      "-83.56\n",
      "[[14.6        27.4        18.         37.30622022 50.02339643 -1.        ]]\n",
      "-11.599999999999966\n",
      "[[ 7.7         7.2        19.         32.88302925 50.32385391 -1.        ]]\n",
      "-29.319999999999993\n",
      "[[29.6        27.9        19.         38.26119619 36.66139208 -1.        ]]\n",
      "-112.0\n",
      "0\n",
      "[[ 9.1        26.6        19.         14.12504633 27.02117363 -1.        ]]\n",
      "23.23999999999998\n",
      "[[11.5        29.2        20.         30.63962127 33.26042448 -1.        ]]\n",
      "15.240000000000009\n",
      "[[ 6.4        22.3        20.         38.1297563  13.05523143 -1.        ]]\n",
      "27.839999999999975\n",
      "[[ 8.1   18.6   21.    11.881  8.622 -1.   ]]\n",
      "4.439999999999998\n",
      "[[10.3        18.9        21.         11.62538045 19.44647317 -1.        ]]\n",
      "-9.560000000000002\n",
      "[[ 5.5        25.8        23.          8.9264214  50.09135341 -1.        ]]\n",
      "45.16\n",
      "[[ 5.7        23.9         0.         28.65136327 39.78638827 -1.        ]]\n",
      "37.72000000000003\n",
      "[[ 4.7        22.5         6.         36.01294304 56.03485974 -1.        ]]\n",
      "40.04000000000002\n",
      "[[ 4.9        43.          7.         43.40808225 18.30480619 -1.        ]]\n",
      "104.28000000000003\n",
      "0\n",
      "[[18.1        33.5        17.         29.49873817  9.74440675 -1.        ]]\n",
      "-15.879999999999995\n",
      "[[ 8.         11.5        17.         37.25624418 19.39846505 -1.        ]]\n",
      "-17.599999999999994\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.4        19.6         0.         12.26301812 19.88212366 -1.        ]]\n",
      "-8.0\n",
      "[[ 8.7        11.4         7.         15.00050841 21.09002945 -1.        ]]\n",
      "-22.680000000000007\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.2        17.4         9.         16.57358684 35.51981607 -1.        ]]\n",
      "-0.07999999999998408\n",
      "[[ 8.9        22.6         9.         13.36891419 12.86886767 -1.        ]]\n",
      "11.800000000000011\n",
      "[[ 6.     7.5   10.     4.584 11.011 -1.   ]]\n",
      "-16.799999999999997\n",
      "[[20.6        22.         10.         24.66487441 30.41927102 -1.        ]]\n",
      "-69.68\n",
      "[[ 6.5        13.7        11.         33.20208694 32.62482958 -1.        ]]\n",
      "-0.3599999999999852\n",
      "[[ 7.4        26.9        11.         17.14552158  3.00664066 -1.        ]]\n",
      "35.76000000000002\n",
      "[[19.4        25.8        16.         31.56389549 10.88204261 -1.        ]]\n",
      "-49.360000000000014\n",
      "[[ 8.6   36.1    5.     4.218 45.758 -1.   ]]\n",
      "57.039999999999964\n",
      "[[ 8.2        25.          7.         23.60923869 54.09835712 -1.        ]]\n",
      "24.23999999999998\n",
      "[[ 6.2        23.2         7.         40.53164326 59.45213155 -1.        ]]\n",
      "32.08000000000001\n",
      "[[10.3   39.     8.    11.735 59.022 -1.   ]]\n",
      "54.76000000000005\n",
      "0\n",
      "[[ 1.7       13.4       12.        19.0929579 45.7548146 -1.       ]]\n",
      "31.320000000000007\n",
      "0\n",
      "[[18.7        35.1        12.         15.50189127 23.10697509 -1.        ]]\n",
      "-14.839999999999975\n",
      "[[ 6.7   25.9   12.    22.421  5.105 -1.   ]]\n",
      "37.31999999999999\n",
      "0\n",
      "[[ 5.8   36.8   13.    55.083 33.751 -1.   ]]\n",
      "78.32000000000005\n",
      "[[13.         25.         15.         39.69621678 29.17195117 -1.        ]]\n",
      "-8.399999999999977\n",
      "[[ 8.2        21.3        15.         26.05270813 31.83716281 -1.        ]]\n",
      "12.400000000000006\n",
      "0\n",
      "0\n",
      "[[ 7.3        27.7        16.         29.93897025 55.03460396 -1.        ]]\n",
      "39.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.5        45.3        18.         40.56376701  7.57949248 -1.        ]]\n",
      "73.56\n",
      "driver reward  2387.7200000000016\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.7        28.2        12.         41.77816207 30.76814989 32.        ]]\n",
      "51.48000000000002\n",
      "0\n",
      "[[ 6.7   26.6   17.    19.881 33.956 30.   ]]\n",
      "39.559999999999974\n",
      "0\n",
      "[[ 8.8        25.5        17.         26.54699331 32.73424562 28.        ]]\n",
      "21.76000000000002\n",
      "[[ 5.9   25.2   17.    28.915 52.32  27.   ]]\n",
      "40.51999999999998\n",
      "0\n",
      "[[ 2.9        30.5        17.         24.78490518 29.03364048 25.        ]]\n",
      "77.88000000000002\n",
      "0\n",
      "[[12.3        30.8        18.         21.93360423  1.08038475 23.        ]]\n",
      "14.920000000000016\n",
      "[[22.6        27.         18.         33.60167746 11.56341812 22.        ]]\n",
      "-67.27999999999997\n",
      "[[ 4.2        28.8        20.         19.83387889 40.59525996 21.        ]]\n",
      "63.599999999999994\n",
      "0\n",
      "0\n",
      "[[16.6   30.2   20.    32.002 56.534 18.   ]]\n",
      "-16.239999999999952\n",
      "0\n",
      "0\n",
      "[[ 0.3        24.7        21.         21.93661345 17.64154212 15.        ]]\n",
      "77.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.1       41.5        7.        28.5110242 28.7607304 10.       ]]\n",
      "36.920000000000016\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.1        37.7         9.         52.983788   57.08927781  3.        ]]\n",
      "72.35999999999996\n",
      "[[ 6.9        15.9        10.         50.86108628 38.80825332  2.        ]]\n",
      "3.960000000000008\n",
      "[[11.         25.1        16.         15.97152803 42.00495082  1.        ]]\n",
      "1505.52\n",
      "[[14.8        23.6        16.         19.34958003 29.86157261  0.        ]]\n",
      "-25.120000000000005\n",
      "[[12.8        26.5        16.         33.20075385 19.62429135 -1.        ]]\n",
      "-2.2399999999999523\n",
      "[[ 4.8   20.    18.    25.882 39.092 -1.   ]]\n",
      "31.360000000000014\n",
      "[[22.3        26.4        18.         19.37829089 52.09426924 -1.        ]]\n",
      "-67.16000000000003\n",
      "0\n",
      "[[ 4.7        19.3        18.          6.6427696  27.18109694 -1.        ]]\n",
      "29.80000000000001\n",
      "[[ 4.9        18.7        18.         26.32544623 15.78692776 -1.        ]]\n",
      "26.519999999999982\n",
      "[[22.9        29.         18.         27.25166291 53.43472278 -1.        ]]\n",
      "-62.91999999999996\n",
      "0\n",
      "[[ 3.1        16.4        19.         25.14711676 51.43684372 -1.        ]]\n",
      "31.400000000000006\n",
      "[[19.3        28.         19.         31.01484051 42.50476087 -1.        ]]\n",
      "-41.639999999999986\n",
      "0\n",
      "[[ 7.         13.1        19.         20.37725313 46.38078842 -1.        ]]\n",
      "-5.680000000000007\n",
      "0\n",
      "[[ 2.4        43.3        19.          1.81830339  0.57453912 -1.        ]]\n",
      "122.24000000000001\n",
      "0\n",
      "[[11.8        16.          7.         12.80484718 30.10392084 -1.        ]]\n",
      "-29.039999999999992\n",
      "0\n",
      "0\n",
      "[[ 3.9        19.4         8.         28.32563721 51.55020352 -1.        ]]\n",
      "35.56000000000003\n",
      "0\n",
      "[[13.6        29.3        10.         22.83257293 29.92050981 -1.        ]]\n",
      "1.2800000000000296\n",
      "0\n",
      "0\n",
      "[[10.6        39.8        11.         45.79687321 31.85490542 -1.        ]]\n",
      "55.28000000000003\n",
      "[[ 0.8   13.    15.    58.05  31.484 -1.   ]]\n",
      "36.16\n",
      "[[ 1.8        24.2         7.         32.63744486 26.63894489 -1.        ]]\n",
      "65.20000000000002\n",
      "[[14.1        14.9         8.         12.70068474 29.6876332  -1.        ]]\n",
      "-48.19999999999999\n",
      "[[16.         11.8         8.         36.19126735 35.16735466 -1.        ]]\n",
      "-71.03999999999999\n",
      "[[ 9.7         6.1        12.         25.69789327 27.76269275 -1.        ]]\n",
      "-46.43999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[15.3        26.5        13.         41.88396528 40.36050518 -1.        ]]\n",
      "-19.239999999999952\n",
      "[[ 4.4        16.6        13.         25.445437   27.28549757 -1.        ]]\n",
      "23.200000000000017\n",
      "[[10.7        32.4        13.         49.60728448 36.21162077 -1.        ]]\n",
      "30.920000000000073\n",
      "[[ 5.6         7.2        15.         42.73294226 40.19096825 -1.        ]]\n",
      "-15.040000000000006\n",
      "[[ 2.1   18.2   16.    27.666 46.726 -1.   ]]\n",
      "43.96000000000001\n",
      "0\n",
      "0\n",
      "[[14.         16.         16.         19.05613118 41.32755711 -1.        ]]\n",
      "-44.0\n",
      "0\n",
      "[[ 2.8        14.4        16.          8.44094538 34.41610566 -1.        ]]\n",
      "27.040000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.9        23.9        17.         19.95544643 39.37752145 -1.        ]]\n",
      "15.960000000000036\n",
      "[[12.5        24.1        17.         26.86608451 24.64713787 -1.        ]]\n",
      "-7.8799999999999955\n",
      "0\n",
      "[[ 4.5        24.6        17.         25.46828725 24.75454354 -1.        ]]\n",
      "48.120000000000005\n",
      "[[21.3        18.6        17.         10.14655595 19.95982688 -1.        ]]\n",
      "-85.32000000000005\n",
      "[[10.8        23.1        17.         15.40074195  8.66184504 -1.        ]]\n",
      "0.47999999999996135\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.7        31.5        19.         31.94488849 14.6407932  -1.        ]]\n",
      "41.639999999999986\n",
      "[[ 1.    43.3    8.    15.27  53.541 -1.   ]]\n",
      "131.76000000000005\n",
      "0\n",
      "0\n",
      "[[ 8.4        16.6         8.         28.31924768 53.64598761 -1.        ]]\n",
      "-4.0\n",
      "[[ 6.3        12.7        10.         34.63472044 35.82088223 -1.        ]]\n",
      "-2.1999999999999886\n",
      "[[ 8.3        17.2        10.         31.22953139 56.79082864 -1.        ]]\n",
      "-1.4000000000000057\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.9        27.6        17.         32.02435175 44.99473566 -1.        ]]\n",
      "48.20000000000002\n",
      "[[ 9.         10.         18.         33.08191369 45.24619003 -1.        ]]\n",
      "-29.19999999999999\n",
      "[[ 9.6        15.9        20.         27.35078191 20.40608453 -1.        ]]\n",
      "-14.400000000000006\n",
      "[[ 4.9         2.          6.         23.60493113 20.33357285 -1.        ]]\n",
      "-26.92\n",
      "[[ 3.4        17.5         8.         17.01463456 38.61062342 -1.        ]]\n",
      "32.880000000000024\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.5        20.3        12.         29.74253594 31.75597004 -1.        ]]\n",
      "34.360000000000014\n",
      "0\n",
      "0\n",
      "[[12.7        18.1        13.          3.85844541 28.66217013 -1.        ]]\n",
      "-28.439999999999998\n",
      "[[ 7.8        29.3        13.         31.08229971 34.90720578 -1.        ]]\n",
      "40.72\n",
      "[[ 6.2   22.6   13.     4.995 27.966 -1.   ]]\n",
      "30.159999999999997\n",
      "0\n",
      "[[ 8.8        39.         13.         39.18127308 15.60959833 -1.        ]]\n",
      "64.96000000000004\n",
      "[[18.6   16.2   17.    35.384 46.697 -1.   ]]\n",
      "-74.63999999999999\n",
      "[[ 5.2        26.6        17.         40.01148852 24.00695085 -1.        ]]\n",
      "49.75999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.7   11.1   18.    50.226 34.902 -1.   ]]\n",
      "3.5600000000000023\n",
      "[[ 6.6         3.4        19.         50.47388283 30.33884907 -1.        ]]\n",
      "-34.0\n",
      "driver reward  2238.2799999999993\n",
      "[[12.2   37.4   12.    12.872 21.952 40.   ]]\n",
      "36.72000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[15.1        43.5        17.         58.12308442  6.90995102 36.        ]]\n",
      "36.51999999999998\n",
      "0\n",
      "[[ 2.1        32.7         4.         27.915741   27.34701261 34.        ]]\n",
      "90.35999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.8        30.1        14.         30.89624768  4.48163468 30.        ]]\n",
      "84.07999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.7        24.9        12.         27.41162801 39.52144174 26.        ]]\n",
      "61.32000000000002\n",
      "0\n",
      "[[24.4        57.         12.         22.57359205  2.8753626  24.        ]]\n",
      "16.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.3        13.5        18.         25.44312144 40.33698701 15.        ]]\n",
      "34.36\n",
      "0\n",
      "0\n",
      "[[ 5.3        19.8        19.         25.39491704 40.77653359 12.        ]]\n",
      "27.319999999999993\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2        44.1        19.         20.9463667  17.08061907  7.        ]]\n",
      "105.75999999999999\n",
      "0\n",
      "[[ 7.5        32.         19.          7.98130285 14.76847721  5.        ]]\n",
      "51.400000000000034\n",
      "[[ 4.3         7.7        19.         14.23607466 19.08886411  4.        ]]\n",
      "-4.599999999999994\n",
      "[[11.7        23.9        20.         26.37133609 17.81073736  3.        ]]\n",
      "-3.0799999999999557\n",
      "[[ 3.6         8.8        20.         38.0236155  16.15738641  2.        ]]\n",
      "3.680000000000007\n",
      "[[15.9        17.6        20.         47.37580849 40.22681805  1.        ]]\n",
      "1448.2\n",
      "driver reward  1988.5200000000002\n",
      "0\n",
      "[[ 4.3       21.8        8.        49.3488658 47.6915679 39.       ]]\n",
      "40.51999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.8   32.4   19.    31.731 21.727 31.   ]]\n",
      "30.24000000000001\n",
      "0\n",
      "0\n",
      "[[ 7.9        26.         20.         43.66519765 23.41300729 28.        ]]\n",
      "29.480000000000018\n",
      "[[ 5.1        24.1         7.         32.45342038 44.84151468 27.        ]]\n",
      "42.44\n",
      "[[ 4.1   35.5    7.     3.031 29.158 26.   ]]\n",
      "85.71999999999997\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.7        20.5        10.         23.84033555 46.49598252 19.        ]]\n",
      "54.04000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.7        15.8        12.         23.08297914 24.63339123 14.        ]]\n",
      "-49.400000000000006\n",
      "0\n",
      "[[ 7.2        18.1        12.         17.21578553 17.58485813 12.        ]]\n",
      "8.960000000000008\n",
      "[[ 7.7        21.3        12.          0.07501867 22.98442849 11.        ]]\n",
      "15.800000000000011\n",
      "[[12.9        23.2        12.          0.23811294 12.20436016 10.        ]]\n",
      "-13.47999999999999\n",
      "[[17.8        28.5        12.         42.82717743 19.36083028  9.        ]]\n",
      "-29.839999999999975\n",
      "[[ 5.8   12.8   17.    25.805 25.728  8.   ]]\n",
      "1.519999999999996\n",
      "0\n",
      "0\n",
      "[[ 3.8        26.         17.         27.99145343 55.28868908  5.        ]]\n",
      "57.360000000000014\n",
      "[[ 4.6        21.8        18.         45.49366765 53.98803477  4.        ]]\n",
      "38.48000000000002\n",
      "[[ 0.9        10.1        19.         37.68997187 59.06574694  3.        ]]\n",
      "26.200000000000003\n",
      "[[13.    29.3   22.     4.179 35.058  2.   ]]\n",
      "5.360000000000014\n",
      "0\n",
      "[[13.6        16.1        23.         18.12628336 21.58434738  0.        ]]\n",
      "-40.96000000000001\n",
      "0\n",
      "[[14.3        24.3         5.         17.02811711 31.50873237 -1.        ]]\n",
      "-19.480000000000018\n",
      "0\n",
      "[[ 8.4        29.2         6.         20.04514371 24.7051957  -1.        ]]\n",
      "36.31999999999999\n",
      "[[11.8        29.          7.         29.18736716 44.76472316 -1.        ]]\n",
      "12.560000000000002\n",
      "[[ 3.2        39.3         9.          0.29956022 13.66067265 -1.        ]]\n",
      "104.0\n",
      "[[ 9.2        11.2        11.         14.77622164 16.65394802 -1.        ]]\n",
      "-26.72\n",
      "[[14.4        21.2        11.         36.10756808 16.27800801 -1.        ]]\n",
      "-30.080000000000013\n",
      "[[ 9.8        13.1        12.         30.185832   34.49685281 -1.        ]]\n",
      "-24.72\n",
      "0\n",
      "0\n",
      "[[12.4        19.9        14.          6.73143803 32.47858502 -1.        ]]\n",
      "-20.639999999999986\n",
      "0\n",
      "[[ 7.9        39.3        14.         30.58002694 13.62673449 -1.        ]]\n",
      "72.04000000000002\n",
      "[[ 4.6        13.5        17.         19.18574743  6.66663958 -1.        ]]\n",
      "11.919999999999987\n",
      "[[16.8        26.6        17.         20.55668823 43.37088781 -1.        ]]\n",
      "-29.120000000000005\n",
      "[[19.7        28.4        17.         26.62581661 37.11393538 -1.        ]]\n",
      "-43.07999999999993\n",
      "0\n",
      "[[ 9.6        23.3        17.         25.96437648  2.59367509 -1.        ]]\n",
      "9.28000000000003\n",
      "[[ 9.8        26.8         8.         40.70122212 19.27771632 -1.        ]]\n",
      "19.120000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.8        33.7        22.         37.57581323 44.65114356 -1.        ]]\n",
      "88.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 3.9        18.1         4.          9.02720365 38.00763518 -1.        ]]\n",
      "31.400000000000006\n",
      "[[13.5        29.3         5.         18.02813758 24.6593808  -1.        ]]\n",
      "1.9600000000000364\n",
      "0\n",
      "[[11.5        19.4         7.         18.42713361 25.55841382 -1.        ]]\n",
      "-16.119999999999976\n",
      "[[16.3        17.8         8.         27.10701456 45.36320009 -1.        ]]\n",
      "-53.879999999999995\n",
      "[[13.6        23.4         8.         46.06375711 33.15384312 -1.        ]]\n",
      "-17.599999999999994\n",
      "[[13.7        22.5        14.         40.54835893 19.00874909 -1.        ]]\n",
      "-21.160000000000025\n",
      "[[ 2.5        10.3        16.         48.29698841 13.22717895 -1.        ]]\n",
      "15.959999999999994\n",
      "[[ 4.7        11.4        12.         58.08320227 25.06758805 -1.        ]]\n",
      "4.519999999999996\n",
      "[[ 6.1        16.2        14.         47.69315878 43.01420877 -1.        ]]\n",
      "10.360000000000014\n",
      "[[ 8.9   11.8   17.    31.406 38.967 -1.   ]]\n",
      "-22.76000000000002\n",
      "[[ 7.1   20.1   18.    22.662 17.536 -1.   ]]\n",
      "16.039999999999992\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.8        16.7        20.         17.78841693 40.79502988 -1.        ]]\n",
      "27.599999999999994\n",
      "0\n",
      "[[ 5.3        19.5        21.         17.09511178 37.96797399 -1.        ]]\n",
      "26.360000000000014\n",
      "0\n",
      "[[ 4.4        29.4        21.         28.3500455  25.11065474 -1.        ]]\n",
      "64.16000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.9        32.6         7.         33.95203791 11.75515551 -1.        ]]\n",
      "71.0\n",
      "[[13.8   12.5   12.    25.721 15.812 -1.   ]]\n",
      "-53.84\n",
      "[[10.2   16.3   12.    31.106 30.028 -1.   ]]\n",
      "-17.19999999999999\n",
      "0\n",
      "0\n",
      "[[ 0.8        24.4        13.         28.61929443 49.73670837 -1.        ]]\n",
      "72.64000000000001\n",
      "[[23.4        19.8        14.         34.58774635 33.22446201 -1.        ]]\n",
      "-95.75999999999999\n",
      "[[ 9.    11.    14.    19.764 36.093 -1.   ]]\n",
      "-26.0\n",
      "0\n",
      "[[ 3.4        29.4        14.         17.85669955 14.72772713 -1.        ]]\n",
      "70.96000000000004\n",
      "[[ 1.3   16.8   14.     2.977 21.194 -1.   ]]\n",
      "44.91999999999999\n",
      "0\n",
      "[[ 5.4        23.         15.         25.5419045  39.75179886 -1.        ]]\n",
      "36.880000000000024\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.2        42.5        16.         21.2320653   6.66424972 -1.        ]]\n",
      "87.03999999999996\n",
      "0\n",
      "[[12.8   12.2   17.     5.968 19.371 -1.   ]]\n",
      "-48.0\n",
      "[[ 1.1    6.5   17.     9.305 14.945 -1.   ]]\n",
      "13.32\n",
      "[[12.6        34.7        17.         40.37143031 16.95866561 -1.        ]]\n",
      "25.359999999999957\n",
      "0\n",
      "[[19.5        20.2        19.         17.42763117 42.32644105 -1.        ]]\n",
      "-67.96000000000004\n",
      "0\n",
      "[[ 4.8        41.8        20.         32.55895006 15.82330381 -1.        ]]\n",
      "101.12000000000006\n",
      "driver reward  743.96\n",
      "[[ 4.4   34.7    7.     7.903 22.969 40.   ]]\n",
      "81.12\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.6        43.9        13.         52.02366149 15.87429633 34.        ]]\n",
      "48.0\n",
      "[[ 2.8   32.9   15.    53.923 49.948 33.   ]]\n",
      "86.24000000000004\n",
      "[[ 5.8   18.9   16.    48.797 30.848 32.   ]]\n",
      "21.04000000000002\n",
      "[[ 2.8        20.8        13.         53.20908275 52.82245507 31.        ]]\n",
      "47.51999999999998\n",
      "[[ 1.6        36.         18.         17.08223458 46.98393918 30.        ]]\n",
      "104.32\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.2        39.3        19.         39.04111707 56.09788348 22.        ]]\n",
      "29.19999999999999\n",
      "[[ 9.9        20.1        20.         31.49978056 48.32834917 21.        ]]\n",
      "-3.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.         37.6        23.         41.78874591  7.46483555 13.        ]]\n",
      "52.31999999999999\n",
      "[[15.7        13.3         7.         37.63296063 28.49441937 12.        ]]\n",
      "-64.19999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 6.8        25.5        10.         21.14159627 54.88230113  8.        ]]\n",
      "35.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.7        25.6        12.         30.33249343 33.69380937  1.        ]]\n",
      "1529.56\n",
      "[[ 8.1        27.3        13.         39.7255096  16.95077457  0.        ]]\n",
      "32.28000000000003\n",
      "[[15.4        22.6        16.         39.18811538  6.71748297 -1.        ]]\n",
      "-32.39999999999998\n",
      "[[ 8.7        18.8        13.         48.25929909 25.14320409 -1.        ]]\n",
      "1.0\n",
      "[[ 2.5   38.1   16.     9.052 35.908 -1.   ]]\n",
      "104.92000000000002\n",
      "[[ 3.9        46.7        16.         46.78379837 56.99065018 -1.        ]]\n",
      "122.92000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2        37.7        21.         37.98225494 24.70202642 -1.        ]]\n",
      "85.27999999999997\n",
      "[[ 9.7        17.9        21.         31.78325459 13.56629287 -1.        ]]\n",
      "-8.679999999999978\n",
      "driver reward  2272.8000000000006\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.3        31.4         9.         34.57137675 29.92882483 37.        ]]\n",
      "50.84000000000003\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 2.4        23.9        16.         12.95408403  8.73235814 23.        ]]\n",
      "60.160000000000025\n",
      "[[ 7.6        24.1        16.         39.93998193  8.69786214 22.        ]]\n",
      "25.439999999999998\n",
      "[[ 3.9   39.6    0.    54.19  45.493 21.   ]]\n",
      "100.19999999999999\n",
      "[[ 6.2        21.4         9.         40.19687593 35.62546671 20.        ]]\n",
      "26.32000000000002\n",
      "[[ 4.2        7.3       12.        31.7390306 30.4559885 19.       ]]\n",
      "-5.200000000000003\n",
      "[[ 5.9        19.5        12.         45.50360585 31.86193071 18.        ]]\n",
      "22.28\n",
      "0\n",
      "[[ 6.3   27.9    7.     9.505 26.591 16.   ]]\n",
      "46.440000000000026\n",
      "[[ 2.5        18.9         7.         25.75538577 39.41094174 15.        ]]\n",
      "43.48000000000002\n",
      "[[ 6.4        23.1         9.         44.22629945 40.81804425 14.        ]]\n",
      "30.400000000000006\n",
      "[[12.7   32.6   12.     1.619 35.228 13.   ]]\n",
      "17.960000000000036\n",
      "0\n",
      "[[13.3        12.5        12.         26.41073518 23.11619052 11.        ]]\n",
      "-50.44\n",
      "0\n",
      "0\n",
      "[[ 9.2        36.4        14.          0.29173087 11.40092232  8.        ]]\n",
      "53.92000000000007\n",
      "[[12.9        22.2        14.         28.30202729 26.7454455   7.        ]]\n",
      "-16.680000000000007\n",
      "[[ 5.2        14.         14.         41.47310885 15.19528708  6.        ]]\n",
      "9.439999999999998\n",
      "[[ 4.    31.9   17.    17.325 36.297  5.   ]]\n",
      "74.88000000000002\n",
      "0\n",
      "[[ 4.6        18.4        17.         18.96412753 36.33975379  3.        ]]\n",
      "27.599999999999994\n",
      "0\n",
      "[[ 3.8        19.1        18.         23.53400689 36.94798693  1.        ]]\n",
      "1535.28\n",
      "[[15.    13.2   18.    20.838 37.516  0.   ]]\n",
      "-59.75999999999999\n",
      "[[11.2        22.4        18.         13.66939257 25.93046809 -1.        ]]\n",
      "-4.479999999999961\n",
      "[[11.1        10.6        18.         11.41641485 18.43472998 -1.        ]]\n",
      "-41.56\n",
      "0\n",
      "[[10.9         8.4        18.         13.89084946 16.0822528  -1.        ]]\n",
      "-47.24000000000001\n",
      "[[13.9        18.4        18.         24.99881928 34.27522875 -1.        ]]\n",
      "-35.639999999999986\n",
      "[[16.8        13.1        19.         32.02107853 36.63759636 -1.        ]]\n",
      "-72.32\n",
      "0\n",
      "0\n",
      "[[ 6.8        13.9        19.         22.0024899  45.82220318 -1.        ]]\n",
      "-1.759999999999991\n",
      "0\n",
      "[[11.5        33.9        20.          1.74772094  7.30133856 -1.        ]]\n",
      "30.28000000000003\n",
      "[[ 6.    38.9    9.     6.568 46.282 -1.   ]]\n",
      "83.68\n",
      "0\n",
      "[[ 6.2        22.2         9.         22.18553737  7.22903383 -1.        ]]\n",
      "28.880000000000024\n",
      "[[14.3   18.8   13.     5.221  4.437 -1.   ]]\n",
      "-37.08000000000001\n",
      "[[10.6        18.1        13.          5.4510427  33.06891721 -1.        ]]\n",
      "-14.160000000000025\n",
      "0\n",
      "[[ 3.1        17.3        13.         12.48046398 27.1162036  -1.        ]]\n",
      "34.28\n",
      "[[10.         30.1        13.         31.84603776 32.39197828 -1.        ]]\n",
      "28.319999999999993\n",
      "[[21.4        21.3        14.         31.13068393 32.68288245 -1.        ]]\n",
      "-77.36000000000001\n",
      "0\n",
      "[[ 6.5   25.1   14.    49.108 34.975 -1.   ]]\n",
      "36.120000000000005\n",
      "[[25.1        42.6        23.         37.82313181  5.81751645 -1.        ]]\n",
      "-34.360000000000014\n",
      "[[16.2   24.1    9.     7.557 32.2   -1.   ]]\n",
      "-33.039999999999964\n",
      "[[22.4        25.9        10.         28.27551662 21.12357597 -1.        ]]\n",
      "-69.44\n",
      "[[10.9   30.    10.    13.535 44.125 -1.   ]]\n",
      "21.879999999999995\n",
      "0\n",
      "[[ 1.5        28.5        12.         21.90952653 33.58798119 -1.        ]]\n",
      "81.0\n",
      "0\n",
      "[[17.7        16.9        12.          5.674663   17.62226028 -1.        ]]\n",
      "-66.27999999999994\n",
      "[[ 2.3        12.2        12.         18.75214203 19.37917417 -1.        ]]\n",
      "23.400000000000006\n",
      "[[ 5.2        32.2        12.         53.23620651 13.44342088 -1.        ]]\n",
      "67.67999999999998\n",
      "[[ 1.4         4.8        21.         55.41675413  9.86886077 -1.        ]]\n",
      "5.840000000000003\n",
      "[[17.5        10.1        15.         42.08615804 31.93178559 -1.        ]]\n",
      "-86.68\n",
      "[[ 6.3   13.6   16.    25.023 28.491 -1.   ]]\n",
      "0.6800000000000068\n",
      "0\n",
      "[[ 9.4        23.5        16.         19.16606727  0.21201435 -1.        ]]\n",
      "11.28000000000003\n",
      "[[18.1        36.2        18.         15.10696444 35.23202824 -1.        ]]\n",
      "-7.240000000000009\n",
      "0\n",
      "[[ 4.8        24.1        18.         31.41173536 25.77780178 -1.        ]]\n",
      "44.47999999999999\n",
      "0\n",
      "[[15.9        30.1        19.         30.42711786 26.80260327 -1.        ]]\n",
      "-11.800000000000011\n",
      "[[14.9        18.9        20.         53.32262441 13.03022754 -1.        ]]\n",
      "-40.839999999999975\n",
      "[[10.    44.9   20.     5.991 40.558 -1.   ]]\n",
      "75.68\n",
      "[[ 3.8        16.1        20.         15.96566972 48.35485733 -1.        ]]\n",
      "25.67999999999998\n",
      "0\n",
      "[[ 3.4   14.1   21.     6.13  20.036 -1.   ]]\n",
      "22.0\n",
      "[[10.4        27.4        21.         14.34359667  5.0277391  -1.        ]]\n",
      "16.960000000000036\n",
      "0\n",
      "[[ 5.4        52.         22.         56.19340581 16.55386331 -1.        ]]\n",
      "129.68\n",
      "driver reward  2079.080000000001\n",
      "0\n",
      "0\n",
      "[[ 3.3        25.6         9.         34.13694207 15.84267598 38.        ]]\n",
      "59.47999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.8        32.3        14.         42.78482521 40.36249975 28.        ]]\n",
      "91.12000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.3        18.5        20.         33.65762908 43.50784895 24.        ]]\n",
      "29.960000000000008\n",
      "[[ 6.1   23.9   21.    17.877 28.494 23.   ]]\n",
      "35.0\n",
      "[[ 0.4        17.1        21.         30.01063916 40.07979751 22.        ]]\n",
      "52.0\n",
      "0\n",
      "[[ 3.4        22.          6.         10.02403121 10.96589242 20.        ]]\n",
      "47.28\n",
      "[[ 2.7        25.9         8.         25.16035683 35.04703266 19.        ]]\n",
      "64.52000000000001\n",
      "[[ 4.8        15.8         8.         39.21284426 22.04629873 18.        ]]\n",
      "17.919999999999987\n",
      "0\n",
      "0\n",
      "[[ 8.2        31.         14.         34.87319059  7.37171173 15.        ]]\n",
      "43.44\n",
      "[[13.1        14.9        18.         31.07086741 31.92747138 14.        ]]\n",
      "-41.400000000000006\n",
      "[[ 6.3        26.5        19.         25.12042685 52.18916802 13.        ]]\n",
      "41.960000000000036\n",
      "0\n",
      "[[ 5.3        19.1        19.         10.72204265 27.4129425  11.        ]]\n",
      "25.079999999999984\n",
      "0\n",
      "[[ 7.4        28.5        19.         13.36736667 31.82807236  9.        ]]\n",
      "40.880000000000024\n",
      "0\n",
      "[[15.1        20.1        19.         22.91593397 47.73823863  7.        ]]\n",
      "-38.360000000000014\n",
      "0\n",
      "[[11.2        27.2        20.         37.83802555 22.4490275   5.        ]]\n",
      "10.879999999999995\n",
      "[[13.6        21.         22.         31.17584812 41.12137651  4.        ]]\n",
      "-25.28\n",
      "[[22.2        31.6         0.         38.36595625 33.7177585   3.        ]]\n",
      "-49.839999999999975\n",
      "0\n",
      "[[11.8        34.          1.         32.31944393 32.59861895  1.        ]]\n",
      "1528.56\n",
      "0\n",
      "0\n",
      "[[ 6.3   14.4    9.     4.449 19.857 -1.   ]]\n",
      "3.240000000000009\n",
      "[[18.6        30.         10.         23.90786237 12.02114202 -1.        ]]\n",
      "-30.480000000000018\n",
      "[[ 8.2         2.1        11.         19.08912464 17.99567512 -1.        ]]\n",
      "-49.03999999999999\n",
      "[[20.5        13.9        12.         18.63276957  9.39359372 -1.        ]]\n",
      "-94.91999999999999\n",
      "[[13.8        16.         12.         19.81085293  4.776814   -1.        ]]\n",
      "-42.639999999999986\n",
      "0\n",
      "[[ 1.5        12.3        12.          3.2659785  19.29902019 -1.        ]]\n",
      "29.159999999999997\n",
      "0\n",
      "[[ 2.3        23.8        12.         30.19291395 18.75872463 -1.        ]]\n",
      "60.51999999999998\n",
      "0\n",
      "[[ 6.3   26.6   16.     2.723 35.328 -1.   ]]\n",
      "42.28000000000003\n",
      "0\n",
      "0\n",
      "[[ 3.9        22.2        16.          4.79883731  8.10233737 -1.        ]]\n",
      "44.52000000000001\n",
      "0\n",
      "[[11.5        25.3        17.          3.14835327 40.50662012 -1.        ]]\n",
      "2.7600000000000193\n",
      "[[10.7        21.8        17.          7.82789962 30.07224477 -1.        ]]\n",
      "-3.0\n",
      "[[ 2.8        17.2        17.         22.05240087 36.75375611 -1.        ]]\n",
      "36.0\n",
      "[[ 6.3        36.2        17.         44.64857157 10.88327742 -1.        ]]\n",
      "73.0\n",
      "[[ 7.3   32.6    5.     6.471 21.359 -1.   ]]\n",
      "54.68000000000001\n",
      "[[ 3.2   25.9    6.    10.709 43.724 -1.   ]]\n",
      "61.12000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.1   24.5    8.     8.698 21.394 -1.   ]]\n",
      "9.719999999999999\n",
      "[[14.2        24.6         8.         27.05657832 23.26418792 -1.        ]]\n",
      "-17.839999999999975\n",
      "[[ 8.5   20.1    8.    10.893 32.628 -1.   ]]\n",
      "6.519999999999982\n",
      "[[ 3.         12.4         8.         18.90221261 27.49932626 -1.        ]]\n",
      "19.28\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.8        33.         10.         38.13614104 51.78755965 -1.        ]]\n",
      "66.16000000000003\n",
      "[[ 6.1   40.9   13.     6.491 26.606 -1.   ]]\n",
      "89.40000000000003\n",
      "[[ 6.2        20.1        13.         22.06822736 25.38354364 -1.        ]]\n",
      "22.159999999999997\n",
      "[[14.3   13.2   13.    10.589 16.601 -1.   ]]\n",
      "-55.0\n",
      "[[ 4.          6.         14.          7.90753294 14.76640497 -1.        ]]\n",
      "-8.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.6        18.2        15.         14.20051288 30.29136633 -1.        ]]\n",
      "33.75999999999999\n",
      "0\n",
      "0\n",
      "[[ 5.2        20.9        15.         22.22725731 52.92394288 -1.        ]]\n",
      "31.52000000000001\n",
      "[[23.3        22.3        16.         25.95444963 46.18130573 -1.        ]]\n",
      "-87.07999999999998\n",
      "0\n",
      "0\n",
      "[[18.9   26.4   16.     6.249  3.054 -1.   ]]\n",
      "-44.039999999999964\n",
      "0\n",
      "[[20.         29.2        16.         16.30363509 53.28576703 -1.        ]]\n",
      "-42.56\n",
      "0\n",
      "0\n",
      "[[12.4        16.1        17.         20.85380544 45.84395918 -1.        ]]\n",
      "-32.79999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.1        42.4        17.         45.75639069 23.58476204 -1.        ]]\n",
      "87.40000000000003\n",
      "[[ 1.9   11.4    7.    49.189 35.186 -1.   ]]\n",
      "23.560000000000002\n",
      "[[10.6        13.6        11.         46.55542915 32.23094766 -1.        ]]\n",
      "-28.560000000000002\n",
      "[[ 6.6        24.3        12.         24.71957292 34.54684421 -1.        ]]\n",
      "32.880000000000024\n",
      "[[ 1.6   29.4   12.    25.406  6.722 -1.   ]]\n",
      "83.20000000000002\n",
      "0\n",
      "[[ 7.7        24.5        13.         12.55336737  8.30956646 -1.        ]]\n",
      "26.039999999999992\n",
      "[[10.3        34.9        14.          8.63892197 36.74142737 -1.        ]]\n",
      "41.639999999999986\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.         26.3        14.         33.08863577 43.15455053 -1.        ]]\n",
      "-11.039999999999964\n",
      "[[20.6        19.7        15.         36.53516245 56.61514879 -1.        ]]\n",
      "-77.03999999999996\n",
      "[[ 9.6        19.9        15.         15.75687565 43.31997623 -1.        ]]\n",
      "-1.5999999999999943\n",
      "0\n",
      "[[ 6.2        20.2        16.          1.04085728  3.6988108  -1.        ]]\n",
      "22.480000000000018\n",
      "[[13.5        27.         17.         32.75890192 16.99747714 -1.        ]]\n",
      "-5.399999999999977\n",
      "0\n",
      "[[18.2        18.3        17.         23.63414331 42.39315238 -1.        ]]\n",
      "-65.19999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[14.9        29.5        18.         10.82711297 21.24371819 -1.        ]]\n",
      "-6.919999999999959\n",
      "[[ 8.4        17.8        18.         11.92068284 31.66413289 -1.        ]]\n",
      "-0.160000000000025\n",
      "0\n",
      "[[10.3        16.2        19.         13.30283957 26.46458734 -1.        ]]\n",
      "-18.19999999999999\n",
      "[[ 4.         11.8        19.         18.75679716 20.03796699 -1.        ]]\n",
      "10.560000000000002\n",
      "[[ 7.4        12.3        19.         31.18390158  6.10143465 -1.        ]]\n",
      "-10.960000000000008\n",
      "[[17.3        23.9        21.         41.83559759 30.13647855 -1.        ]]\n",
      "-41.160000000000025\n",
      "[[ 3.4        22.1        23.         30.76401353 48.06491883 -1.        ]]\n",
      "47.599999999999994\n",
      "0\n",
      "[[11.6        23.1         5.         12.54631805 13.28034491 -1.        ]]\n",
      "-4.960000000000008\n",
      "[[13.6        22.3         8.         21.38865884 33.60480374 -1.        ]]\n",
      "-21.119999999999976\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 8.2        38.9        13.         32.36776636 12.09121277 -1.        ]]\n",
      "68.72000000000003\n",
      "[[ 1.7    1.2   19.    34.094 13.353 -1.   ]]\n",
      "-7.719999999999999\n",
      "[[14.5         5.3        10.         42.4496414  17.70042221 -1.        ]]\n",
      "-81.64000000000001\n",
      "0\n",
      "[[19.1        22.5        14.         23.94405887 22.69796633 -1.        ]]\n",
      "-57.879999999999995\n",
      "[[ 3.         14.7        14.         33.03042675 31.93040387 -1.        ]]\n",
      "26.640000000000015\n",
      "[[12.6   17.1   15.     7.53  29.116 -1.   ]]\n",
      "-30.960000000000008\n",
      "0\n",
      "[[ 9.4   24.8   16.     5.948 10.549 -1.   ]]\n",
      "15.439999999999998\n",
      "[[ 8.6        10.7        16.         20.6121939  20.94760355 -1.        ]]\n",
      "-24.23999999999998\n",
      "[[ 0.5    8.7   17.    26.339 27.045 -1.   ]]\n",
      "24.440000000000005\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.4        12.9        18.         19.46622603 41.99743337 -1.        ]]\n",
      "-9.039999999999992\n",
      "0\n",
      "[[13.7         9.9        18.         10.31553608 20.63353283 -1.        ]]\n",
      "-61.48000000000002\n",
      "[[ 6.3        41.8        18.         42.90201423 36.44512813 -1.        ]]\n",
      "90.92000000000007\n",
      "[[11.2        21.         19.         33.1822789  21.50921011 -1.        ]]\n",
      "-8.960000000000008\n",
      "[[ 8.6        14.         19.         20.64202747 35.15724551 -1.        ]]\n",
      "-13.680000000000007\n",
      "[[19.1        21.8        20.         22.45938041 10.71318851 -1.        ]]\n",
      "-60.120000000000005\n",
      "0\n",
      "[[19.4        19.         20.          9.27071369  9.11484064 -1.        ]]\n",
      "-71.12\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.6        32.8        22.         26.50412375 30.5327018  -1.        ]]\n",
      "26.08000000000004\n",
      "[[ 4.2    1.4   22.    28.834 28.526 -1.   ]]\n",
      "-24.08\n",
      "driver reward  1995.9600000000005\n",
      "[[ 3.3        27.3         8.         49.29416463 20.13897862 40.        ]]\n",
      "64.91999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 3.8        17.6        10.         37.9173485  45.17934449 35.        ]]\n",
      "30.47999999999999\n",
      "[[ 2.8   33.8   13.    12.804 18.762 34.   ]]\n",
      "89.12000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[12.4        25.5        21.         27.6682066  38.90042488 27.        ]]\n",
      "-2.7199999999999704\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.7        23.          6.         25.18635488 42.13191424 22.        ]]\n",
      "0.839999999999975\n",
      "[[14.9        21.8         7.         37.82696109 46.44202901 21.        ]]\n",
      "-31.560000000000002\n",
      "[[15.         23.5         7.         32.61091102 23.06816001 20.        ]]\n",
      "-26.80000000000001\n",
      "[[13.1        29.5        10.         37.47718047 55.67849957 19.        ]]\n",
      "5.319999999999993\n",
      "0\n",
      "0\n",
      "[[ 8.5        40.6        12.          6.02105884 44.02448102 16.        ]]\n",
      "72.12\n",
      "[[ 9.7        22.4        13.         11.68632458 32.26554149 15.        ]]\n",
      "5.720000000000056\n",
      "[[13.6   28.5   13.    25.93  37.997 14.   ]]\n",
      "-1.2800000000000296\n",
      "0\n",
      "0\n",
      "[[ 9.2   31.    13.    11.568 28.331 11.   ]]\n",
      "36.639999999999986\n",
      "[[ 5.3        24.6        13.         32.06472022 33.4778651  10.        ]]\n",
      "42.67999999999998\n",
      "[[23.         31.3        14.         35.67006288 23.89388158  9.        ]]\n",
      "-56.23999999999995\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.2   33.5   16.     1.771 21.499  5.   ]]\n",
      "71.83999999999997\n",
      "[[12.8        28.1        16.         17.06692063 54.1165345   4.        ]]\n",
      "2.8799999999999955\n",
      "[[17.1        18.6        17.         20.72648953 28.92780207  3.        ]]\n",
      "-56.76000000000002\n",
      "0\n",
      "0\n",
      "[[ 7.7        11.6        17.         16.43128607 24.66773181  0.        ]]\n",
      "-15.240000000000009\n",
      "0\n",
      "[[ 6.5        18.3        17.         20.09720133 52.16358027 -1.        ]]\n",
      "14.360000000000014\n",
      "0\n",
      "0\n",
      "[[ 5.7        31.5        17.         32.36674692 43.47074935 -1.        ]]\n",
      "62.03999999999999\n",
      "[[14.          7.1        18.         27.59433858 36.23661823 -1.        ]]\n",
      "-72.48000000000002\n",
      "0\n",
      "[[14.8        23.8        18.         25.64586064 30.85001134 -1.        ]]\n",
      "-24.480000000000018\n",
      "[[15.8        32.7        18.         13.77557591  5.80779116 -1.        ]]\n",
      "-2.8000000000000114\n",
      "0\n",
      "[[19.2   10.5   19.    13.706 21.505 -1.   ]]\n",
      "-96.95999999999998\n",
      "[[11.         10.3        19.         11.67418222 20.28626513 -1.        ]]\n",
      "-41.84\n",
      "0\n",
      "0\n",
      "[[ 8.6        13.6        20.         17.86913726 41.85470079 -1.        ]]\n",
      "-14.95999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 9.1        23.         21.         23.77328985 40.5770957  -1.        ]]\n",
      "11.719999999999999\n",
      "[[18.         29.         21.         35.51516626 49.64975809 -1.        ]]\n",
      "-29.599999999999966\n",
      "[[10.9   39.4    0.     7.993 20.889 -1.   ]]\n",
      "51.960000000000036\n",
      "0\n",
      "[[ 8.5        31.          6.         21.95955861 47.55545132 -1.        ]]\n",
      "41.400000000000034\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.4        20.9         8.         18.65341493 39.39962935 -1.        ]]\n",
      "30.160000000000025\n",
      "0\n",
      "[[ 8.5        29.6         9.         30.21072342 55.67681441 -1.        ]]\n",
      "36.920000000000016\n",
      "[[ 8.1        37.         10.         20.29091279 19.47314888 -1.        ]]\n",
      "63.31999999999999\n",
      "0\n",
      "[[ 1.5        12.5        12.         15.65045001 30.34143278 -1.        ]]\n",
      "29.799999999999997\n",
      "0\n",
      "[[ 8.         13.6        12.          9.90176238 28.41538039 -1.        ]]\n",
      "-10.879999999999995\n",
      "0\n",
      "0\n",
      "[[ 5.6   14.5   12.     8.225 30.985 -1.   ]]\n",
      "8.319999999999993\n",
      "0\n",
      "[[ 7.5         9.7        13.         14.67973694 31.87415427 -1.        ]]\n",
      "-19.959999999999994\n",
      "0\n",
      "[[ 4.9        17.4        13.         22.39080734 44.38903092 -1.        ]]\n",
      "22.360000000000014\n",
      "0\n",
      "0\n",
      "[[ 3.6        22.5        13.         29.82095202 48.14056166 -1.        ]]\n",
      "47.51999999999998\n",
      "0\n",
      "[[19.1        25.8        14.         16.9096245  33.12116595 -1.        ]]\n",
      "-47.32000000000005\n",
      "[[16.1        36.9        14.         25.11501041 12.8273285  -1.        ]]\n",
      "8.600000000000023\n",
      "0\n",
      "[[12.7        19.6        15.         22.5018142  31.82524307 -1.        ]]\n",
      "-23.639999999999986\n",
      "[[12.9        10.9        15.         16.8219517  25.74214431 -1.        ]]\n",
      "-52.84\n",
      "[[17.         22.7        15.         19.6488137  20.61851919 -1.        ]]\n",
      "-42.960000000000036\n",
      "0\n",
      "[[ 5.2        25.6        16.         25.94059165 41.83966381 -1.        ]]\n",
      "46.56\n",
      "0\n",
      "[[10.3        36.1        16.         26.78378254 11.6817137  -1.        ]]\n",
      "45.47999999999996\n",
      "[[ 7.3         0.7        18.         20.66463081 14.85061174 -1.        ]]\n",
      "-47.4\n",
      "[[15.7        24.7        19.          7.441709   41.49493692 -1.        ]]\n",
      "-27.71999999999997\n",
      "0\n",
      "[[12.7        19.6        19.         26.15310997 28.82673729 -1.        ]]\n",
      "-23.639999999999986\n",
      "[[ 5.7   23.4   20.     6.583 38.517 -1.   ]]\n",
      "36.12000000000003\n",
      "0\n",
      "0\n",
      "0\n",
      "[[16.4        35.1        20.         32.31532669 24.59541471 -1.        ]]\n",
      "0.8000000000000114\n",
      "0\n",
      "[[15.3        32.9        22.          6.0213359  18.84048378 -1.        ]]\n",
      "1.240000000000009\n",
      "0\n",
      "[[ 7.4        15.4         1.         17.80583828 23.17768047 -1.        ]]\n",
      "-1.039999999999992\n",
      "0\n",
      "0\n",
      "[[ 4.2        24.6         6.         15.80628264  3.33228656 -1.        ]]\n",
      "50.16\n",
      "[[ 4.3        32.4         7.         44.43961881 12.80460438 -1.        ]]\n",
      "74.44000000000003\n",
      "[[12.5   26.8   19.    10.586 32.415 -1.   ]]\n",
      "0.7600000000000477\n",
      "[[ 9.8        32.4        19.         16.57883306 54.52316048 -1.        ]]\n",
      "37.039999999999964\n",
      "[[17.8        20.2        20.         24.7748361  33.63657978 -1.        ]]\n",
      "-56.39999999999998\n",
      "[[24.3        25.2        20.         16.55341253  0.05414439 -1.        ]]\n",
      "-84.59999999999997\n",
      "[[17.3        16.8        20.         16.25357412  0.36714327 -1.        ]]\n",
      "-63.879999999999995\n",
      "0\n",
      "[[ 4.7        15.9         8.         11.6384092  21.05556304 -1.        ]]\n",
      "18.919999999999987\n",
      "[[ 8.         11.7         8.          7.3194171  10.28815741 -1.        ]]\n",
      "-16.95999999999998\n",
      "[[ 8.6        23.          8.         25.44499818 29.21284414 -1.        ]]\n",
      "15.120000000000005\n",
      "[[ 5.9        12.4         8.         18.48975406 25.00121889 -1.        ]]\n",
      "-0.4399999999999977\n",
      "[[15.         28.2         8.         24.56136513 43.6276584  -1.        ]]\n",
      "-11.759999999999991\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.1        32.3        11.         27.30452822 54.82943588 -1.        ]]\n",
      "34.68000000000001\n",
      "[[ 8.8        13.9        11.         47.19718675 55.23615747 -1.        ]]\n",
      "-15.360000000000014\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.7        14.3        14.         18.94997072 42.00230164 -1.        ]]\n",
      "34.2\n",
      "[[ 6.4        19.8        14.         39.73797075 45.51311463 -1.        ]]\n",
      "19.839999999999975\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 1.2        14.5        18.          5.86061946 29.3599566  -1.        ]]\n",
      "38.24000000000001\n",
      "[[ 7.         14.3        18.         17.03017676 29.5489507  -1.        ]]\n",
      "-1.8400000000000034\n",
      "[[ 8.3         9.7        19.         19.33828706 32.75095424 -1.        ]]\n",
      "-25.39999999999999\n",
      "[[15.1        25.         19.         22.98625383 45.15158486 -1.        ]]\n",
      "-22.680000000000007\n",
      "[[ 4.9        26.7        19.         15.07657235 24.34999563 -1.        ]]\n",
      "52.120000000000005\n",
      "[[13.5        12.2        19.         14.5189876  16.41261018 -1.        ]]\n",
      "-52.75999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.         17.         20.         22.33067945 52.61659853 -1.        ]]\n",
      "-13.599999999999994\n",
      "0\n",
      "[[ 4.         15.         20.         21.42429527 46.37969085 -1.        ]]\n",
      "20.80000000000001\n",
      "0\n",
      "0\n",
      "[[ 5.7        10.6        23.         12.15675868 31.73889458 -1.        ]]\n",
      "-4.840000000000003\n",
      "[[ 7.1        30.         23.         37.79503218 22.05075648 -1.        ]]\n",
      "47.72\n",
      "[[11.         13.8         8.         31.48913576 39.38486425 -1.        ]]\n",
      "-30.639999999999986\n",
      "0\n",
      "[[12.1        35.1        10.         46.6141507  47.52233042 -1.        ]]\n",
      "30.039999999999964\n",
      "[[10.7        30.         12.         28.06228826 53.11727487 -1.        ]]\n",
      "23.24000000000001\n",
      "[[11.1   27.4   12.     9.875 47.871 -1.   ]]\n",
      "12.199999999999989\n",
      "[[ 6.9        10.4        13.         21.47244828 36.50838642 -1.        ]]\n",
      "-13.64\n",
      "[[ 3.4   17.1   13.     3.409 29.619 -1.   ]]\n",
      "31.599999999999994\n",
      "[[ 8.7        15.2        13.         22.04361577 35.17336825 -1.        ]]\n",
      "-10.519999999999982\n",
      "0\n",
      "[[ 1.2   20.    13.    21.136 18.303 -1.   ]]\n",
      "55.84\n",
      "[[ 3.1        11.2        14.         19.54930804  6.8245985  -1.        ]]\n",
      "14.760000000000005\n",
      "[[16.5        14.2        17.         20.50570078 11.09888813 -1.        ]]\n",
      "-66.75999999999999\n",
      "[[22.8        14.         18.         15.91061878 20.66743726 -1.        ]]\n",
      "-110.23999999999998\n",
      "[[13.2        18.9        18.         21.52165918 19.67359808 -1.        ]]\n",
      "-29.279999999999944\n",
      "[[ 1.6   21.8   19.     7.247 37.631 -1.   ]]\n",
      "58.879999999999995\n",
      "0\n",
      "[[ 6.2        21.7        19.         12.90957352 41.25053339 -1.        ]]\n",
      "27.28\n",
      "[[ 3.8   23.6   19.    22.021 15.483 -1.   ]]\n",
      "49.67999999999998\n",
      "[[13.5        20.6        20.         15.17489127  3.34734583 -1.        ]]\n",
      "-25.879999999999995\n",
      "[[ 1.5        14.6        20.         15.68791471 19.42184307 -1.        ]]\n",
      "36.519999999999996\n",
      "[[ 9.3         7.4        21.         24.4403436  12.49124042 -1.        ]]\n",
      "-39.56000000000002\n",
      "[[18.9        20.         22.         25.42489364  9.21678777 -1.        ]]\n",
      "-64.51999999999998\n",
      "0\n",
      "[[ 7.6         8.9         8.         13.55293115 20.94113965 -1.        ]]\n",
      "-23.200000000000003\n",
      "[[ 1.7        35.1         8.         25.89130889 53.34815804 -1.        ]]\n",
      "100.75999999999999\n",
      "[[ 9.6   33.9   10.    11.138 29.123 -1.   ]]\n",
      "43.19999999999999\n",
      "0\n",
      "[[ 6.4        35.2        11.         36.56556828 34.49383463 -1.        ]]\n",
      "69.12\n",
      "[[18.1        23.         13.         47.39243885 19.77458608 -1.        ]]\n",
      "-49.48000000000002\n",
      "[[ 4.7        18.8        19.         48.79546565 43.16015237 -1.        ]]\n",
      "28.200000000000017\n",
      "[[ 2.4        15.5        20.         36.69117903 50.23191154 -1.        ]]\n",
      "33.280000000000015\n",
      "[[ 5.8        31.3        21.         16.70592643 19.06634151 -1.        ]]\n",
      "60.72\n",
      "0\n",
      "[[ 1.9        29.         23.         26.74600323 47.29760166 -1.        ]]\n",
      "79.88000000000002\n",
      "0\n",
      "driver reward  575.1200000000003\n",
      "[[ 0.7        19.4         6.         40.07942422 39.9049647  40.        ]]\n",
      "57.32000000000002\n",
      "0\n",
      "0\n",
      "[[ 8.9   30.6   14.    12.665 29.373 37.   ]]\n",
      "37.400000000000034\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[14.3        41.         17.         52.35088344 25.79805995 21.        ]]\n",
      "33.960000000000036\n",
      "[[ 1.7   37.7    8.    16.337 26.3   20.   ]]\n",
      "109.07999999999998\n",
      "[[14.2        22.6         9.         22.33319255 20.35669093 19.        ]]\n",
      "-24.23999999999998\n",
      "[[15.4        32.5        10.         43.74599569 19.29745242 18.        ]]\n",
      "-0.7199999999999704\n",
      "[[ 2.4        13.6        14.         35.79335005 30.77721696 17.        ]]\n",
      "27.200000000000003\n",
      "0\n",
      "[[ 7.2        43.2        15.         34.93745908 25.21398436 15.        ]]\n",
      "89.27999999999997\n",
      "0\n",
      "0\n",
      "[[13.2        31.1        17.         49.80643414 35.92104639 12.        ]]\n",
      "9.760000000000048\n",
      "[[ 6.7         5.1        18.         52.61693368 35.36465237 11.        ]]\n",
      "-29.24000000000001\n",
      "[[ 6.3        24.6        20.         34.18538058 58.69404352 10.        ]]\n",
      "35.879999999999995\n",
      "[[12.2   24.9   11.    25.318 36.124  9.   ]]\n",
      "-3.2799999999999443\n",
      "0\n",
      "[[ 5.6        27.2        12.         31.96060039 40.14135205  7.        ]]\n",
      "48.960000000000036\n",
      "[[13.5        33.1        13.         51.42197406 55.31411911  6.        ]]\n",
      "14.120000000000005\n",
      "[[ 8.    39.8   17.     7.743 48.398  5.   ]]\n",
      "72.96000000000004\n",
      "0\n",
      "[[11.4        19.1        17.         21.76899028 30.04727276  3.        ]]\n",
      "-16.400000000000006\n",
      "[[ 7.5   18.3   17.     2.98  12.476  2.   ]]\n",
      "7.560000000000002\n",
      "0\n",
      "[[13.1        52.4        18.         44.23050367 51.40801783  0.        ]]\n",
      "78.60000000000002\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 5.7        40.4        20.         23.60464294 20.46961163 -1.        ]]\n",
      "90.51999999999998\n",
      "0\n",
      "0\n",
      "0\n",
      "[[13.6        13.6        21.         13.77724675 24.9613506  -1.        ]]\n",
      "-48.95999999999998\n",
      "0\n",
      "[[17.3        18.8        21.         19.09461767 22.96562644 -1.        ]]\n",
      "-57.47999999999999\n",
      "[[22.6        23.1        21.         38.53974517 24.4276763  -1.        ]]\n",
      "-79.75999999999999\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[10.9        17.1         6.         18.98437513 32.69768764 -1.        ]]\n",
      "-19.400000000000006\n",
      "0\n",
      "[[ 5.6        11.7         7.          9.27941927 22.91788981 -1.        ]]\n",
      "-0.6399999999999721\n",
      "[[ 4.7         8.6         7.         12.72392305 26.76686151 -1.        ]]\n",
      "-4.439999999999998\n",
      "[[ 5.3         4.6         7.         14.52734761 23.09379966 -1.        ]]\n",
      "-21.319999999999993\n",
      "0\n",
      "[[ 9.2        24.7         8.         17.51425041 33.64357243 -1.        ]]\n",
      "16.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 4.7        14.9        10.         12.752795   31.87752213 -1.        ]]\n",
      "15.719999999999999\n",
      "[[ 6.2    5.9   10.    24.876 31.741 -1.   ]]\n",
      "-23.28\n",
      "[[ 7.9        12.         11.          8.87193416 20.55182385 -1.        ]]\n",
      "-15.319999999999993\n",
      "[[12.1         6.9        11.         27.21393074 25.24453618 -1.        ]]\n",
      "-60.19999999999999\n",
      "[[23.3   22.    12.     5.457 26.143 -1.   ]]\n",
      "-88.03999999999996\n",
      "[[ 5.8       18.        12.        18.4910461 39.4294946 -1.       ]]\n",
      "18.159999999999997\n",
      "0\n",
      "[[ 2.2        34.9        12.         53.06157582 23.16702662 -1.        ]]\n",
      "96.72\n",
      "[[19.1   10.5   17.    33.317 45.151 -1.   ]]\n",
      "-96.28\n",
      "[[ 5.1   27.7   18.     5.319 46.091 -1.   ]]\n",
      "53.960000000000036\n",
      "[[ 6.5        22.4        18.         23.09779998 57.57627172 -1.        ]]\n",
      "27.480000000000018\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[11.3        23.3        19.          5.68243094 20.32190864 -1.        ]]\n",
      "-2.280000000000001\n",
      "[[ 4.6        17.7        19.         18.93268464 13.75330699 -1.        ]]\n",
      "25.360000000000014\n",
      "0\n",
      "[[ 1.1        28.2        20.         37.34738535 16.06574773 -1.        ]]\n",
      "82.75999999999999\n",
      "[[12.3        10.9         8.         33.14321567 14.06216914 -1.        ]]\n",
      "-48.76000000000002\n",
      "[[22.4        16.4         5.         39.3697179  15.89954247 -1.        ]]\n",
      "-99.83999999999997\n",
      "[[17.2   18.2    8.    52.661 46.819 -1.   ]]\n",
      "-58.71999999999997\n",
      "[[ 7.8         8.6         8.         51.44942401 50.59782961 -1.        ]]\n",
      "-25.519999999999982\n",
      "[[ 4.8   35.8    9.    19.64  28.465 -1.   ]]\n",
      "81.92000000000007\n",
      "0\n",
      "[[ 2.6        13.4        10.         13.19542428 35.90572319 -1.        ]]\n",
      "25.200000000000003\n",
      "[[ 4.1   22.3   10.     5.641 16.198 -1.   ]]\n",
      "43.48000000000002\n",
      "0\n",
      "[[15.8        23.         11.         11.32272927 34.66956857 -1.        ]]\n",
      "-33.839999999999975\n",
      "[[ 7.2        32.7        11.         36.20827641 43.09061478 -1.        ]]\n",
      "55.67999999999995\n",
      "[[ 2.7        26.         13.         12.91142853 27.00105578 -1.        ]]\n",
      "64.84\n",
      "0\n",
      "0\n",
      "[[ 5.7        34.         14.         42.16613934 54.16568895 -1.        ]]\n",
      "70.03999999999996\n",
      "[[ 8.4        33.4        18.         21.11652161 18.0234669  -1.        ]]\n",
      "49.76000000000005\n",
      "[[ 4.1        20.7        18.          6.47364861 26.30440655 -1.        ]]\n",
      "38.360000000000014\n",
      "[[ 1.6        22.8        18.         23.414601    9.28055281 -1.        ]]\n",
      "62.079999999999984\n",
      "[[21.6        27.2        19.         17.63833268 43.65981258 -1.        ]]\n",
      "-59.839999999999975\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[[ 7.4        17.7        23.         12.09116339 20.72791053 -1.        ]]\n",
      "6.319999999999993\n",
      "[[ 7.7        51.1        23.         57.8608746  47.16713693 -1.        ]]\n",
      "111.15999999999997\n",
      "driver reward  740.2800000000008\n",
      "total reward  28870.600000000006\n",
      "trips [ 11.  13.   0.   0.  10.  16.  35.  81. 141.  47.  83.  46. 161. 135.\n",
      " 138.  71. 132. 196. 112. 139.  98.  81.  15.  26.]\n",
      "[0.5454545454545454, 0.38461538461538464, nan, nan, 0.3, 0.4375, 0.4857142857142857, 0.5061728395061729, 0.46808510638297873, 0.46808510638297873, 0.5060240963855421, 0.5217391304347826, 0.4906832298136646, 0.5333333333333333, 0.4782608695652174, 0.43661971830985913, 0.4772727272727273, 0.4846938775510204, 0.625, 0.49640287769784175, 0.5918367346938775, 0.41975308641975306, 0.6, 0.5769230769230769]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-ff47d4624acb>:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hrly_acceptance_rates.append(hrly_accepted_trips[j]/hrly_trip_counts[j])\n"
     ]
    }
   ],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluateSavedPolicy(policy, policy_state, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf, policy_state)\n",
    "\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: pol/policy_10/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-724e56976280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load saved policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaved_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pol/policy_10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"time_step = ...\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdon\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0ma\u001b[0m \u001b[0mMetaGraph\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mSavedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m   \"\"\"\n\u001b[0;32m--> 578\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, loader_cls)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m   saved_model_proto, debug_info = (\n\u001b[0;32m--> 588\u001b[0;31m       loader_impl.parse_saved_model_with_debug_info(export_dir))\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m   if (len(saved_model_proto.meta_graphs) == 1 and\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model_with_debug_info\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mMissing\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0minfo\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   debug_info_path = os.path.join(\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot parse file %s: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_to_pbtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     raise IOError(\"SavedModel file does not exist at: %s/{%s|%s}\" %\n\u001b[0m\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: pol/policy_10/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "#load saved policy\n",
    "saved_policy = tf.compat.v2.saved_model.load('pol/policy_10')\n",
    "policy_state = saved_policy.get_initial_state(batch_size=3)\n",
    "\"\"\"time_step = ...\n",
    "while True:\n",
    "  policy_step = saved_policy.action(time_step, policy_state)\n",
    "  policy_state = policy_step.state\n",
    "  time_step = f(policy_step.action)\n",
    "\"\"\"\n",
    "observations = [8, 10, 0, 35]\n",
    "#observation_ts = ts.transition(np.array(observations, dtype=np.float32), reward=0.0, discount=1.0)\n",
    "observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]),\n",
    "                                tf.convert_to_tensor(np.array([observations], dtype=np.float32), dtype=tf.float32))\n",
    "action = saved_policy.action(observation_ts, policy_state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(saved_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluateSavedPolicy(saved_policy, policy_state, eval_env)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntime_step = tf_env.reset()\\nrewards = []\\nsteps = []\\nnum_episodes = 5\\n\\nfor _ in range(num_episodes):\\n  episode_reward = 0\\n  episode_steps = 0\\n  while not time_step.is_last():\\n    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\\n    time_step = tf_env.step(action)\\n    episode_steps += 1\\n    episode_reward += time_step.reward.numpy()\\n  rewards.append(episode_reward)\\n  steps.append(episode_steps)\\n  time_step = tf_env.reset()\\n\\nnum_steps = np.sum(steps)\\navg_length = np.mean(steps)\\navg_reward = np.mean(rewards)\\n\\nprint('num_episodes:', num_episodes, 'num_steps:', num_steps)\\nprint('avg_length', avg_length, 'avg_reward:', avg_reward)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]]\n",
      "(500, 267)\n",
      "5.3919373\n",
      "5.5419006\n",
      "nan\n",
      "nan\n",
      "conv 1.6891718407829268 1.6434600975557612\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAEUCAYAAADz3nD2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOyde3xkZXn4v8+eycxkMpMdkrC7LCxmhRWloigrasWfqEWUWvXn3YqVasFrq7XWa39ea+ul1dpqFVoVKuANFfCCQFFssZXCVm6y4iKsZAlkd5NNdpKZzGTOPr8/3vfMnNxPkpnM7f1+PuczM+fMOfOeZJ55nvd5n4uoKg6Ho33Y0OgBOByO2uKE2uFoM5xQOxxthhNqh6PNcELtcLQZTqgdjjYj1ugBRGFgYEAHBwcbPYxI7Np1iNNOO6rRw2hZdu3adVBVj67V9U4U0XzE9z4E16rqc5d6j4jsBXKAD5RVdeec42cCVwH3213fUdWPhI57wK3Ag6r6/IhDWxEtIdSDg4PceuutjR7GgshRoIcaPYr2QUR+W8vr5YE3RHzvh2Ag4lufqaoHlzj+n0sI7NuA3UBvxM9aMS0h1M2A/HYG9nbBvcDDmMd7gWRjx+VYGqF5vuQichzw+8DHgHfU63Oa5X6bhj1s425O5k5O4V5OYIhtDLGNrvR2ZtJdkMYIcrq6SRfoTGPH7VgYAbqiv31ARMIm4UWqetGc9yhwnYgocOECxwGeKiK3A8PAO1X1l3b/PwDvAjLRh7RyOk+oHyNmNrQJ2AFsB46323bIbN9IlnGyjJOiQIoCGXKk0nkm0r1Voc7aLRBuR1OyAeiO/vaDc+fIC3CGqj4oIpuA60XkV6r6H6Hj/ws8QlUnReQc4Epgh4g8H9ivqrvsvLtutLX3+2IRrhbhZhHuF0H7xQhkAigDE8CYfT4NFCHl58mQCwl2nhg+3Yk8xBT9fdC3YAQ5EOokiIDIaIPu1LEYgfkdZYuCqj5oH/cD3wVOn3P8sKpO2uc/BLpEZAB4GvAC62j7OvAsEbl0TTe3CG0t1AVgxj4WgLJvD2y0j0WMME9REezuyRlSGMHOkKObPN0USFCCZLF68RizzHBVUO1H5I51uTdHNALzO8q27LVEekQkEzwHngPcNec9W0RE7PPTMTI2qqrvVdXjVHUQeCXwY1U9d+13OJ+WNL9vFuFY4LhlMszepMqlIsxgZLYwDV1TGKEO7nwKo623AkXoKkI3gbY+ZExv8sQp0pUsUfGMle35SWY5y1QfV6O7dNSCGjvKNgPftTIbAy5X1R+JyBsBVPWLwEuBN4lIGaNLXqnrnArZkkI9AgwCu0Q4bZm/17mqXGMFe6ZMVRg9+3wKY4ZP2OdT2Ll0ngQlUhSIUyRBiURIU+u7Qf7cvmjJv2JnsEJH2ZKo6n3A4xfY/8XQ888Bn1vmOjcCN9ZoWPNoWfN7ZAXvfZ4qp6lS8DEm95Q9EAh52AQvQ8IvEqdEt9XQCUrEKRJPlpB7FviAmFmvFlFEVjIyR72p9Zy6FWjJexnDzJVXunp/GMhPQKoX4yyLYTzhPlUB98Er+yS8YkhTl0hQwvN8iCnmq0JV64M1wQXVzWu4M0etqaWmbhVaUqjPsyb3NSKR3v8NETZjljZyU5CaYv4yVOBE88CPeQBWS5stTgnPSrFciwk8gfb9uW8TOlGoW9b8BmNWXxxBsAPvdwyj4WcOA7co3KTwAzVa27NvjkHRS1DGI4aPh281tdHcxKx6DrR0maon3Al1U+LM7xYjSiBXsLRVXuwNgUDazYiy+dN4+BVNHafIhmSJI+Hf/vBFF/0AR6NYYfBJW9DSmhrg/AirBW9SpRD1ggkj1ABlPDzKePh4lI3mjvnzf9qn7VbGrVM3GbVcp24VWl5Tr5RgaavrUWLCRLcCfVQEdSYBeVKUiFe0dRgvVmYm2B3DaOeDwD6AuzFRgm6tullopoSO9aLlNXVUgnk1QFfwX562jzHMvBqI+VAiPkuwA83tBd608Pw5hsnaYgRj6J2OyM11vBPHSnCauo0J5t6z/nnBUlbwV/Cg7FU1dbBC7VVE2wp12DEWw2Th7uvGJN+009ej9elETd1R9ztDyPyeG3RiiflGIweCbebVfsVhlkoXjIIPC3ZleawA2RQ8GuQxwMOugEKj6cQlrbYW6o/Z8NAuzLQZQvJbxGS79mMEuwj4IGXwKJMjU/GC+1awAXLjGbjJXiicV00vkDeZW0EsuEvJbDhOU7cZ7w95xj8hwmHmLIH1UI39nqKiuROUKvPqPN0UiTNOlmG2MrO3FyYxQh2kXg4AW4CHU8ZhljbHdWgdbtKxJE5TtzlB2szYlHGWdSUwghwkdBTNZmK+SxwiS8zq6qI1xckqPFaMxzuYT48Dx1EV9rsW+nRHI3BC3eYUMGUgA1O8kqUV3spwlD/OoLeXcbL4xIhT4ijG2cownAD705s5srfHeL2DCigDGMEuYzR1DGQH6J71vUfHbATojvotb5PgoY5Z0nq3DUAJtpkyRjMHj6EUzN7RGQY4iIePj2fzqUuAScvMDozDlpnZ5YyyGKHeQrUm5TSOBiMCsVi0rV1oo1tZngKmZGwBUzChOwESrn4yAewH+mHTpv1sZZicrRHn4VOyi9me59OVLjCT7Zot1IH5PY0xycfNl8p1C24cItDlLf++dqKjhPodNgEkBxz2obtoM7YCgR4DRs2WKubpT4xWMrMSFNnGED4eQ2xjnKy5aLCkFWyBds5STed0NIxAU3cSHWN+B4xRNcEL06Dh6icjGE09Cj37j7CVYQYwxQSDJa0gGXNmOm6cZQeZra2DJa3w5mgYgnWKRtjahY4T6neoMooR7jEfxiZAJzAaen9oG4atxWEG2VupURbkagFsHBgnuXMMTsVo62xoC+fzOaFuLIJJq42ytQlt9PsUnXNV+UaQh+0DE9CfxEj6MKYm+Aj0DB9h6/ZhhtjGdvaSYZIC3Qyzlb2JQQ4lsvxmMMGRfT3VdetpjCBPUvnrunl1A+nA6JMOu90qr7AFCbswoaHdE5AKglGCCLMpyDLOIHs5gXtJUSBHhl/zKO7gFH7NSYxvPooDAz3GDA+b3GFNHQORGVQ7bcW0CXBC3VmMYv4A3UDvNKQCL/goleWt7NQEm3tGyDLOlvsnmOmD3MY093AScUqkyM82secKdbCVO/pP3Vg67E9f1zm1iOwVkTtF5LagR5GI9InI9SKyxz42rO/ruaocxhQkPOzbMkfB3NoKdXK/iTBL+XnYDV37zbkH6SdHxgj1gJq16cXq49iihI4GsAGTVhtli8BC3+k5x88UkQl7/DYR+YDdv01EfiIid4vIL0XkbbW4vYVYD0fZM1X11FCPovcAN6jqDuAG+7phvCQUlHJ4kmrI6BjGBJ8wEWbx6Rmzbz8MMMoAZrmrmzzpLQdN0EmwRg1uOatZqE+N4Lnf6bn8pz1+aqg3dRn4C1U9GXgK8BYROXmltxOFRhgmLwTOtM8vwRQ1f3cDxlEhiAkv+KBTIIeBj1U9W71gkj9sk5Sn2K1CD/A0kCB7KxDmydmfIzKKan/tb8CxNE3g2VbVh4CH7POciOwGjsWUy6kp9dbUQdvPXSJygd232d4gmOjpBQtli8gFInKriNx64MCBug5yH3CaKsepIqNqKo2uhUWLEXYhcv/aru1YGSvT1APBd85uFyxwxYW+03N5qojcLiLXiMjvzBuSyCDwBKAuJXLqranntf0MH1RVtX1+52H7/l4EsHPnzrouCC1YvPDZAjdU9+uYMNK3kZSfp9crVfbLg9PoscZTpu8G+X/McZDNvbDzgK8rK/N+162VbWU4Imng28DbVfVw5JGtgLpq6kXafo6IyDEA9nF/PccQlX8R4RoR7hYhnxYzfz6/6tySUdjC+CyBBhOEMouw+Z1kjlDHcELdAGoYfLKGVraISBdGoC9T1e+s9bYWo25CvUTbz6uB19q3vRa4ql5jWAlBUcIckJpU+IXCv4Q0+AI/PWdzFZsSI3gjU8gPQD5LVajDxf4rGIEWqfk0yrEYNXSUraWVrd33JWC3qn66Fre2GPU0vxdr+3kL8E0ReT3wW+DldRxDZP5sqZCv7wpshwNkuJnT2ct2hthGhhOIhYsRTs/ZgpJHB4MLBd+eLkTucG1v14PaBp+supWtiJwBvAa4U0Rus9d7n9XmNaVuQr1E289R4Nn1+tyacobATsxWhOzEJNmN4yQwLW3LeBRJcGQ6PluYJ0PPZ2mBsNqO0lvEsWZqKNRraWWrqjexTsEKHRZrszzfFmEQGPSg/zFUo8umTThpwhZLKFV6YcZhumt2W9xAsIPX7q/cOITIgSXtgvu6zSFHdc26Eogygak0Oo2JIMOkYOZJkfdTVVO7zGwtPUlVuGcxA9EbATnWQgfGfndc6uVynBeKMJs5TCWKjFEqRQm7KeDjkSNNfrJ7vuk9jlmBf3gGJmdgOqg4PkY4o1vk6nW/v47DpV46wIhdHyZstN/mVjMMTJgaZSny+HiUSDA9njGOsIfttg/YixXkgL1UF60P47T0OuI0tQNMWmYOGzYaJHgMAcOQKeaIU8TDNyWNDnYZYT5INSx0EBjsgoGgS9ODdhsLfYpbr143ah/73dS00a3UllGMtu4rQmqUSqmj1NQRUokCcYpmPj2OydB6NNU64IHG3hdcCcxqSIFqAMphnGCvA4H53UE4Tb0IL1E1M+ApqvPqB0xkWYk4OTLEvSIMKhuf+zD83owpbZTGCPW9QDmoXXos5ifiREyfn26MQHe7eXW9qU+WVlPTRrdSe0YxlY3yE5CawCjXKXPsKMZ5ArdxzwkF8nTjD3hMZo82mrvi8S7Yk7A7upj/LXLauq64ObUjTOAFL5epLm3th35GiVNiEyMM2HCxctmbVb6ouowVBBYFWrsbs3AWCPiM09b1xmlqR0DQpudwEXqD2mX74QTu5WTuxrcRZaOjA2T7x5nO9lWa482+SuDt7rVbjKqA91KddztqTlD5pINwmnoJzrfljgqY4glBbfC+oWn6OUg3ebYxxIn991IqxquF/WNg1qVH7Is+jKMskPYdGE1dxgh2ymnretGBc2on1MsQxJ/kpjBr1XZpK8s4KQp8mndw982n0Z8YNcEnx2H8YXRh9Hw3VaEO6kF0ARnMNyljj7u5dd3osOATJ9TLUAkbDffcGoGtDLOZEXb8bB/83HjE2Yup/f1YrGCfiBHk4yDWNUcbHIcxvbsx7rjudbqjDqMDNXUb3Up9OB14Yh9ID9WiB/vhkcMP88itl8PTLudFT/sa++7ZYTzfSYy8PhbY129+CAaohpEGpLtgMli77saEje5C9bR1vLsOoAO93x12uyvntEcDj6HaEXMY+JPZudd3c7IR6LTdkcRUJjyICUCJ2edhB1oSmOy1L2KYX4KRutxDR9OBQu3M7+XYCjwSY0UXgWurAj1GNx/m3RzFOF0nHoanKJwxU22WdwbWDKcq8CiQr3b0oBdI2ccFazA61kqHzak77DdsFdyPEey5KZQfFPr+L5x66m3czyDZ/nGG+7dSJM6e8uPgJjEx4AeppmZOAuOCcYqNwPTm6pp2DCj3ur5btaYDNXWH3e4quE+ZyQpdPSayrBAT+rditPAeOOXUO/gNJ7CVh4hTYj+b6BscZuxXx5rzJzEm+EGqPaunu6BsBXoL1WZ6Qeqmo3Z0oFA78zsCXeMKDyqpSWW3D6PDwAPAHnjk/Q9zOjezlWH6OUiaHFu9YTNFvtdeIFi7Djen30I1Ai1L1WRPgylt56gJQeWTGrXdaQWcUK+QoK81wxjTfDdsZy/9HLTtbnNkyJE+8YAR0sD8DshiBH6QauvbYP8ARtiz63MvHYFb0nJE4TCmUb0MY2LBp8YY6BllP+MMMIpPjG09Q+wePNqsXZepVhQ9ESPU41T/+uEvVdDb2lEbOjD10gn1CnmBKjeKsH8MNtslruQY9PeM0s8om9hPnhSb2M/uwRk4rssIcVDmKIkxtScx+wOBDjR20ArXURvcnNoRhTNVzYryCGZu/QBkOUSGHAMcJEWeDDm2PGLIFE/YgtHQWzAaO40JTjnVPj4aI+CBCT4Acta631b7UkPze7WtbO2x54rIPSJyr4jUrdtrh/2G1Y4gyUPGgFETC76ZkUrmVhmP4xmisDPFxPQWuBWjgffax0GqUWbjVAXelRSuLfXR1M9U1YNLHP9PVX3+rGGIeMDngbMw6yG3iMjVqlrzdi3u67NKDgNjE9D/APAr6HvMNMfsGAYgTwofD58Y5YTH3TvjTNNntHAaI7yDVD3eQSnhoBySNb/lVaBfW9/7ajuaZ059OnCvbQiAiHwd09bZCXWzMKs2uF3e2rz1ALEenzKeFWrPJHpshL1nbjdr10nMUtc4RrCDpa5gHh0utLCULnBEY2WaemCOSX2R7b4aJmhlq8CFCxwH28oWs0byTlX9Jaam1VDoPfuAJ0ce2QpwQr1KZjXNsQUUkmNw7NQY3ibTXytu2/PEKZHyCtzzOz4Hpo83WvkgcBPVSLXHYkzwwJk2q3qKY01E/5bXvZXteuAcZaskKP4LGKGesM8nYMvQBNvZy1YeYhtDbGOIrQyb0keBJg6WrvZh5tH7qDbUCzzkA7bftWP1BJVPahR8soZWtg8C20JvPc7uqzlOU6+SbiDmYYSvBwhM8RgwAscmxshtGiFHmnGyjJNlE/sZOvEAkw8fbQR4IHTBIOAk0NLBo/sPrY0aOsps+9oNqpoLtbL9yJz3bAFGbKfLSitbjA22Q0S2Y4T5lcAf1mZks3FfmVVSBg770D+KmTn9AvNrv8MeHIat/cPkvAyjDHCQAbIcYlvPEPfuTDAz0GuEOgglPRGjuReIJpPPgr5tHW6qXamdo2zVrWyBsoi8FbjWjujLdq5dc5xQr5LnqXKNCLn74ZQJkE0YTT2Myeqagt7hGbZuG8aEpfSzlYcokIJ+2D19MjycrDrIgpjwcJSZfdTXruuttRdN0srWHvshUPN+1HNxQr0GnqdKPi3IOcDbYGYHdO2nOr/G9LD28Om2ASkZcsQp0ZUsMbMlWQ0h3Uc1wizQ2OHYcMfq6MCIsg673dqTegzwMrhk58u5ldN49cbLecr9t0MCHt66kf1spkicGP6s8zLZHGMD3TDeVQ0htXnVenYDbqRdaVGhtt71p2HsvgJwF3Crqh5Z7ty6e79FxBORX4jI9+3r7SJysw2V+4aIxOs9hnrxbRFjbk+ZQoRZxolTQjfCga3GQebbCZ2HT5yiWd4ib1r2lGPVAglJI8xal5XLDqeFKp+IyDNF5FrgB8DzgGOAk4G/Au4UkQ+LSO9S11iP37C3Absx9XoAPgF8RlW/LiJfBF4PfGEdxlFzXqLKzSI8+To4a+tNPH3nTeR7kgz39THENo7BRJgVSVAgRYpCpWOmTww9QeCEBt9Eu9N6mvoc4HxVfWDuARGJAc/HhJp+e7EL1PV2ReQ44PeBjwHvEOM2fBZVV/4lwIdoUaEGEyYEwF2Q9CH52Gnu25TFw7e9rAummZ7V0BfzpkYOt/NoMaFW1b9c4lgZE8yyJPU2v/8BeBcQzAP6gXE7ODDuoWMXOlFELhCRW0Xk1gMHDtR5mKunC0yxhP2YxYob4JF7HmYbQ/h4ZKYmeRJ38kKupd+111l/WrTyiYi8TUR6xfAlEflfEXlOlHMjC7WI9NhMk6jvfz6wX1V3RT0njKpepKo7VXXn0UcfvZpLrAubVeFGha8AP7PbT2HL/RPEKZLsqVYRPJcrGjXMzqV1K5+8TlUPYwJcjgJeA3w8yomL3oqIbMBEvbwaeBKmQG5CRA5iJvEXquq9i52P8dy9wMa/JjFz6s8CWRGJWW1dt1C59eaaITh9GPpvskL8LaFve2Hpkxz1p8XM7xBiH88Bvqqqv7TT12VZSlP/BOPGeS+wRVW3qeomTB3NnwOfEJFzFztZVd+rqsep6iDmx+HHqvpqe92X2re9FrgqykCbneep0l8O1fZ9mavz2yyoF21rMnaJyHUYob5WRDJUp7FLstRv2O+p6szcnao6hvG8fVtEVtPV7d3A10XkrzHBlV9axTUcjkiogN+amvr1mNo496lqXkT6gT+OcuKimjoQaBH56txjwb6FhH6Ra90YVIJQ1ftU9XRVPVFVX6aqxSjXcDhWhRXqKFuToZj16T+zr4NubssS5VZ+J/zCOstcFzdHS6ACZS+qPziSdbte/DNmQM/CZILlMBbyk5Y7cSlH2XuB9wHdInKY6sS9BCxU7cHhaDpUBD8WVQ2X6jqWFfJkVX2iiPwCQFUPRY2+XMr8/ltVzQCfUtVeVc3YrV9V31ujgTtWyCbmBRo5lsH3vEhbkzFjrWIFEJGjqYGjLOD91su9XVU/KiLbgGNU9X9WPVzHqtnP8Y0eQktxhA0UiZpe0FRLkP+IqayySUQ+hlkx+qsoJ0YR6s9Tte0/iskp+jwRbHuHoxnwW3ChWlUvE5FdwLMxU98XqeruKOdGudtV2/YOR6NRpJIp14LswVSjjgGIyPELJXrMJYpQr9q2dzgaTasKtYj8KfBBTB8YH6OtFXjccudGEerAtt+8Utve4WgGWlGoMSnLJ6nqirOAlhXqtdj2DkejUYRyawr1ELMKY0UnqgdhAMir6ldE5GgR2a6q96/mAx2O9cSY363jKBORd9in9wE3isgPgErUpap+erlrLHu3IvJBYCdwEibBsAu4FJOF5XA0PbU0v0VkLya6ywfKi3X0EJEnAf+NKRF8hd33SUzRkA3A9cDbbPngMBn7aPupErdbZKL8hP1f4AmYdiKo6rDNGHE4mp46OcqW7HppHcufAK4L7ftdjCIMHF03Ac8Abpw1XtUPr3VwUYJiS/bXJPB+96z1Qx2O9UIRiiQibTXkTzFx2vtnDcUkZMQxdVa6MJ7tBRGR60UkG3p9lC1IuCxRhPqbInIhprjB+cC/A/8S5eIOR6MJNHWUDdv1MrRdsOAlTdfLXQsdF5FjMdbtrLp7qvrfmFoCD9nt2mUczker6njo/EPApij3HMX7/XcichZmEfwk4AOqen2UizsczcAKzO9adL38B+DdqnokXKhERE4EHoOp9oM99+mq+p+LDTscbCIij8Bay8uxVJbWxap6nn25dakqhw5Hs1LrOXW466WIBF0vw0K9E1MEBMyq0Tm2r9YO4OdBR0wRuQZ4KrCYUL8fuElEfopZSn46sJDlMI+lzO9wzyDXns3RkgTr1FG25bDFNzPBc0xRwLtmfZ7qdlUdtGW8rgDerKpXYjzZzxCRmK0Y9AxMPfyFx636I+CJwDeArwOnqWqkOfVS5rcrstXiyIPT6LGRimW0NTVcp47S9XIxrsAkRd2Jka0fqer3lvm8BDBmP+tkEWGOqb8gS93tcSLyjxjVHzyvoKp/tvBpjmbBCXRtze8oXS/n7D8v9NwH3hD1s0TkE8ArgF9SzbVQZpv6C7KUUIfn0LdGHYyjhuwX2FQfg0neC/q3dbl0U9GqCR3AizCx3yuu4beoUKvqJWsakmPt1EugL8O4cDqEFo39vg+zll07oRaRfwE+q6p3LXCsB2MaFFX1spV+qGMZvibwKiPQB8hwD4/iDKI3OnkUt/Pr+VYiAPID0FfXZJQtwRE2UGq2njrRyAO3icgNzI79Xnbau5T5/XngAyJyCsbDdwATEbMD023jy4AT6Hrgh596K/5S/prHM8iv2MujAeieGGN6PAPjXejv13KgrUGLmt9X223FLGV+3wa8XETSmLW3YzBFnHar6j2r+TBHRM6tmt1bGGfLKi4RCDRAIlmi8IgueEQNxtZitGrq5Vqmv1EiyiaZE3TuaB22sYfxxI5Z++RnwLjZ2t0Ub8HUy2DJa0FUtSaVTxz15kwxnTMDbhZ48tqcZL/LjzlIP0Nz5tbyG4WkQBoog1wIGnmhpTVpMfP7+Wu9gBPqZuDGOQK8iEAf9uP0essXnH8Fl9BNP7/mWfOObUjnOUIKYgJlIDv//HaiBZe0Hlggx3oWIiJLvafeTecdNSQQ6K/xIt7HBxZ93zd47azXJ/BLtrGHfv9B4skixMrmQAxIgny2XiNuPCvM0moGfiIifyoiswq8i0hcRJ4lIpfAnH/wHKJUPnkUJhDlEeH3q+p8NeBYF17FlbxqmfckQi1kfDzztfV8YjEfYj7QVW22nq7jYJuAFnOUPRd4HfA1EdmO8X4kAQ9TdOEfVPUXS10givn9LeCLmBxqf5n3OmrNBwU+vPL5dYp85fleHs0gvzJpCzGfDTG/Y2o8t5qjTFWnMc3x/tkmfgwAhXBu9XJEuduyqn5h+bc56sL06k6LU+JMfsSNPBcAD58UBfxEjFQ6z2Q6BWXrMGvjebUilFZW4qtpsK2iH1rpeVHm1N8TkTeLyDEi0hdsKx+iY1V8QuHz1WT7A2T4Mb8b6dQUeU7hFraxp7IvQZF4sgTJojHqkkDaho62IbVMvWwVomjqYFIeTvBQ4JG1H05nMxoT+ssLmNpvsfvuErzHJnkW/7XstS7ndQA8mZ9yp2179ihuJw6UPY90NsckUOljXl7z8JuSVjO/a0GU4JPtq7mwiCQxaWIJ+zlXqOoH7eT/60A/sAt4jao2VWPgRvGgb/4oi7KKEOY0udDpJYIezH5PjHLZY7rsAV0wDfIl0Nev/DOanSbybK8Ly5rfItIlIn8mIlfY7a12Ar8cReBZqvp44FTguSLyFEzp1M+o6onAIaANv0ar43Gq8BiZvfM1Ah8R+JbADqWPArdwSqTrvZRLSVDid/kxp/Jz4hTx8Kv1M5Ml6wmnbb3gLbikBYCIvFhE9ojIhIgcFpGciByOcm6UOfUXgNOwHjn7fFnHmRom7csuuymm+sMVdv8lmLxRR8DuOeb3VxU+oLPM49Gl9fksguUsH4+YFehu8nSTJ54okUznIaltvWbdikINfBJ4gapuVNVeVc2oam+UE6NMNp5ktW3Aj0Xk9igXt0XNdwEnYrK+fgOMq2rwFd0HHLvIuRdgC60df7xrtM5mmJ4Sbu15IveHNPUp3AJQmTeHSZMjR4Y4RbKMV764N/MMwASl+GnPZHAF69ZtViylVRM6gJHV9qyLItS+iJygqr8BEJFHEnG92pZwOdUWJR2uWyIAACAASURBVP8uhFKHlj/3IuAigJ07d3Z2vbSPCLwYRnqOZpIMBVKVQwsJc8DFvGnW6zP50azlnQRFPC8UjGI94e1ECzvKbhWRbwBXMjuf+jvLnRjlbv8SE7p2H6Ze2SOAP17J6FR1XER+gimJmhWRmNXWxwEPruRaHckU8FjlEZg//nMjnPIdzuHF/HDWvmDN+myuokiCOFkSFKvBKNNADOTvQf+ipnfQUJrQtI5CL6ZQwnNC+xRYu1Cr6g0isgNTyB/gnih1k2xz+hkr0N3AWRgn2U8wPa6/jlkuu2q5a3U8wys/JUOOqzibvQzyNi6cdwyspsYnniwyTU/V9G4jE/wIGyi2YPCJqq5IcYZZqpzRs1T1xyLy4jmHTrSlSpf7xTgGuMTOqzcA31TV74vI3Zhi538N/AL40moH3zF8NTT7uE3g1MVnIz/nVMp4eCRIUGTTnHZNf8iXbRvFUsVFlEiWmN4yA5NdJtJ4S3tp61Y0v0XkOOCfqHaX/U9Ml8x9y5271N0+A/gx8AcLHFvWDFDVOzDdMufuvw/T1cCxBDNZoWt8AeFdQKD3sI0dDAFGWE0PNqOF5zZ+S5GnaAU+TokMOfyEB1tg4mAWSMIkbRM6WuvUyzW2sj0e+FdgG0aGzlHVvYt81FeAy4GX2dfn2n1nLTfGpcoZfdA+/cjcBvM2gMRRRxYU6EXIUo31T1ifSrCQlSHHJbyC1/INAP6Vt3IeXyBOiRR5SsTJ0008UaIrWWImljTOsjYxwZulla3l34CPqer1tkzYUnk1R6vqV0KvLxaRt0cZXJR16m8vsO+KBfY51pMLBX4qMCRkpiY5QIaHyZImR4YcKfIVwe1n9vcvSMv08IlTIkHJzK9jZUjOtJ0XvAHr1PNa2YrIyUAsaC6pqpOqml/kfIBRETlXRDy7nQuMRvnwpebUjwZ+B9g4Z17dS9v8jjc3e0TY8WjmB6QATGAasmyEpAcwSSnZZZaoqHp8PXwyTPIt/oARNtt162OIEydBnBLxaqRZskQpWeJIssskebRBqaMVrlMPiEi4ccVFdml19iVNK1sFLpx7PNTK9pkwa73xUcC4iHwH2I5pCf0eu+y7EK/DzKk/Yz/zv4i46rTUnPokTL2kLLPn1Tng/CgXd6yNHark00LqSQK3hAT7rQI9wEYqkfVJH7zeGaNhPSPUgdc3RZ4s4+Tt+naWbgqk6KZAkQQpCuQoEk+UjCc8marWMWtxVrhOXbdWthhZezrGz/QApvHdeSziKFbV3wIviDrwuR+0IKp6FXCViDzVNsx2NIDU5AJa+nN23yetcMeAsonDLSV98IyJ7VOgRJwyHhlylaiyoxhnkgwePiXi5MhUTPBSMs50rAyxrraxx5qkle0+4DbrKEZErgSewhyhFpF3qeonReSfWKCq6JqK+QcXB/5QROZVz3EN8pqAd6kRbA+jsZMQnz5CMVGeszIbt0US8uTppkicNDmKJOgmT5ZDlIhTJE4xkWBDssSRdHuY4LV0lNnONBtUNRdqZfuRWZ8XymoUkYuB76vqldZ5lhWRo1X1ACYHYqEedUFo6Kr71y1ll6z54o7aMSvXOlxSeBRjhm8EJqFrCmJpo609+2UuW194N3lSpCiQIkEJjzIxWxElQ468PRZPFplOx8EKdiujyLxlvTWw6la2quqLyDuBG8RcYBemRNjc9wXtbfOq+q3wMRF52dz3L8RS5vf37GOlU4CIbADSqhopBcxRO3b78OSs0NUDbA0dKGKcZr0YU3wK4tPGg+17Pp5N7wpivjPkKBEny3gl2ytuM7dS5I0pniwxHfMh2x5C3QytbO3r64Fli/Fb3oupD7jcvnlEqSZ6OfBGzGL7LUCviHxWVT8VcXCOGnCGKiMibE5gYrT/0M6ng9dT1S05BX7Mx/PKeKHcmxg+hAS4mwJ5UpX6ZcH+UiKOn/WYnE5AVpDLWreTR6vV/RaR5wHnAMfO6QnfS8T6NFHWqU+2mvlFwDUYd/xrVjhWRw3YBxyewGjnSah8V327b8o+FsErz45rCAt3ghLd5BngYGU5K06RhF3XjlOq1jFL0/LausVqlA1jprzTGBM92K4Gzo5ygSi+/i5b6eRFwOdUdcau0TnWmdNsUwbtF2Q/Zh7dj/n9nsYI9LTZvNBvegy/UgU8FsR72xDRBCXG7VJXijzjNnMr7hVNhFnaRJi1aqmjVku9VNXbgdtF5HJbTXTFRNHUFwJ7Mcbef4jIIwA3p24ghUArT9kdVjtXBLtoUqR9K8IBZbyK0yiIJuu29cEDYY9b8Y/hk0gWTYRZC2vrVi1nBAza8mF3i8h9wRblxGWFWlX/UVWPVdVzbImi32KiZRwN4BoRclOYaLIhZgu2z7xZl0/MCnMcnxgl4rO+yAlKFfMzbteqA8H2gm4eLV4RpUWF+iuYsmFljLz9G3BplBOjFB7cKCKfFpFb7fb3GK3taADPUzVz61HMvHrCHvDsZlvpFBPG410KRXcX6J4VaWZO822gijFRg3jx8By8Ur9s1R2TG0cLa+puVb0BEFX9rap+CPj9KCdGmWx8GbgLeLl9/RrMr8jcPGvHOjGGMcF7D2PWqbdS7YuVAHog35MkT8qmd2QYJ8tB+imQIss4cUpWe3uVogk5MniU7U9B0WhqMGZ9mZasDa60XC+tgKJdQt4jIm/FVAiKNAmKItQnqOpLQq8/LCK3rWKQjhpxlir3izAzBV2HMdo6EOwkaI8R0FH6uZ9BhtjGfpvM4eHTz0GOZ8gGnuQrgSkePnm6gWD5Cyh7VUfcJMh7Qf+2Ibe9KpQNlGoXfLKevA1IAX8GfBQTgbZkt8uAKEJdEJEzVPUmABF5GlBY5UAdNSIHFKahawIzr05QWeI61JdkL9u5k1MYYlsl/jtYrgLsa7M2HTwvkSNFloJduwZMyGi2q+JVbyFHcoUmNK2XRVVvsU8nWWFNwCj/ojdhyhJtxBQeHCPiL4ajfowAfUXoDcziMsbT0Qd72c5vOIE83WxjCI/yLG0V1ACP24IKJRKMk6p8+buDrC4vRSkdpxjzmYn1GmfZw+t8o2uk1UoEi8g/qOrbReR7LJzQsWzmVpTCg7cBjxeRXvvaLWc1AWepcneQ2pfEeL/TMHX8BkrEeR2XA3ATpxGzZY0KdHPIamKoRpjFKVLGq0SXBaa3R5n+nlHyPSnGgZlyL0zOG0pT02rr1MBX7ePfrfYCUcJE+4EPAmcAKiI3YUocRarC4KgfBYD77I/5GwV2QGrqCJsS1WKDWxnGw6dAinGylPEokKJInG67pJVhkhQFHiLOENsYpZ+h4ja8mM9mz1yrlI4zkcxAWuYPpMlpJfNbVXfZp/3AD6JU7p1LlOCTrwMHgJdgSvseAFvwytFQcuEXY8AUyARsLlaq6LBt4mE2Tx0gQ454aOkqWOYKemtlbCrmMMeY6iiJEp4XBKSYAgob0nkTXfaDdb3NNdHCS1p/APxaRL4qIs8XkcjmRpQ3HqOqHw29/msRecWKh+ioOWcGSR7bgB2YilgPQE/5CPQIHDaOtK4e2Lp1jHJfEHBSZITNAJVmeUUSbGWY7eylbI3wHGmbopky69zpbiaTPS0ViKII/pGmE9hlUdU/tuHZzwNeBXxeRK5X1T9Z7twoQn2diLwS+KZ9/VLg2lWP1lFTRoBNUyDDwG0YD/hjgHPULHN9S6AfJAFbvQMkNlaDS0bYXAkNzXKIbQzZ3K0UZTwmyXDI1gouESffk2IyPQ3pJPIN0Fb4aVcol1tPqAFsnsU1GIdZNyb/oiZCfT7wdqoTeA+YEpE3mM+N1onPUR/2AJvHYHMMuNPuPIxJ3oPqctcUdI3BluIE8U1Fq59NRZQsh0hQpGATMt/FPwHwZj4NGIeabx1pyXSe6XSyZeqCqwp+uaUcZUAlBfMVwJnAjZh64S9f4pQKUbzfmTWMzVFnXqLKLhE2j1iH2ROsI+vdNt+6h2p7tSTQB33laQpbRyohoykKJChRJFEpTghwCnfwG05kiG22QGGeVLrAdHoGklFalDcePSKUpluv7Q7wRxjf1RtW6ixrvZ8wxzyClEwAfQCkB7P0FKNadXQC6KOSALKpZwxvo8+wLaMS5FeXiHMpL2WUfg6ylSLxSiePHGniXrFSw0yuAn3hut7qilEVyjOtZ36r6qtsRuTTgX+3/ehiqppb5lQn1O1CPi3snjIVRU/ZbQU7iRHqTZjglClMJlcPdO2HfiaMwENlfTrQ1vvZVLl24EwzFcwKpNJ5JtM9LZKOKRzxW+9rLiLnY/qz9wEnYDrEfhF49nLnLlVN9IfAm5fo9eNoInJTVY19twjHWY2c6THLXJXKKAnM12TC/AD0M0Eh3YVX9knEihS9OONkK4UIY5W8azPjzpAj15MxDrNksvm1tWLi11uPt2DKD98MoKp7bK3xZVnqJ+wrGM/3JcAnV1uFwbE+bA6Z4A8CFI3Qjhahfwp6gwyuzRhTfD/gQ1cSchs9U6wQSHilSv0ywK7glsmQo0B3pTJKpe9Wsy9vqbSqUBdVtRQ0BLDr1JEqDi1VTfRb1p3+/zBd7b9KqKGXqn56TUN21Jw7RNgLzGAEOoapVtcbLnc0aR9DaZQlEtADqalp4olSpRZ41rq4c2TI002aHGlydJMnkSwyk7bXbWYUKLdeFBzwUxF5H9AtImcBbwa+t8w5wPIRZSWqRltmzuZoMh6nSh8mfPQwc1LpfMx/MU2lVU9QWMHHY5QBvDIk/KLNwJ4ky3jF5A7qgydsLFp3TwGy0ybCrNmjFsoRtwiIyF4RuVNEbpvTd2vu+54kImUReemc/b0isk9EPrfMR70HE715J/AG4IfAX0UZ41Jz6ucCn8ZUMXziMh36HE3CGaqcYZ9fbU237iRGZScwX96gptkUcBj6N47h9Zgkju7JGVIbC2Q5ZPVymqINJw2a6aVsKeHKmnUzJ3mYKgm1ZrWtbMHkRv/HAvtnYXtxXQlcaTt6RGYpTf1+4GWq+p7VCLSIbBORn9jCab8UkbfZ/X0icr2I7LGPR6302o5oHMb8ancnQjtG7RZ80cumTriHjx8zJclSfr6irYNOHkClMFI3BSPc6UKlMGHTxoMHQl0jTR2Rea1sAUTkNIxXYyFhD94jIvIhETkI3APcIyIHROQDUT98UaFW1aer6i+jXmgBysBfqOrJmEZgb7E9et8D3KCqO4Ab7GtHHTjXOs8K4WqjgW0WhDPEzPNUMU++J4mUrba2rXiMCV6wlVHKleKECUrEvSLJbM44y5rVYXaEaoGH5Tbbyja0XbDAFYNWtrsWOh5qZfuFOfs3AH8PvHOZEf858DTgSarap6p9wJOBp4nIn0e55bot4KnqQ8BD9nlORHYDxwIvxIS+AVyCCYF7d73G0YncIcLjAoHGVEhJQbWjBxjTe4KKMPaMHqFo2/l0TUEqnSfl5W1tcLNGPWlrmAXN9lKkKCRTJsIs3cQRZtG1cD1b2b4Z+KGq7puzfy6vAc4Km/eqep+YpvPXYfpVL8m6rMqLyCCmL+/NwGYr8GDqaGxe5JwLMIvvHH/88fUfZBvxuNDy1hjQ55tlrVlz6XDxfyvoRyWnK3PuzMQM3X15uu3yViDMQcWUbgokKNKdyJNPp5hJdiE3gC4bGrHO1HhOvYZWtk8Fni4ib8a4K+MiMqmqcy3VroXm66p6wGZtLUvdhVpE0pj5xdtV9XD4V0pVdbFuH6p6EXARwM6dO11HkFVylip7RNAyyNy5Y5lqrfBpkFEqTfYkBpm+nI0iy4dOMWu+vu2kCdjlLYXpJlw6qqFQr6WVLXBlaP95wM4FBBqoNFNZiKWOVairUNtflm8Dl6nqd+zuERE5RlUfEpFjmONMcNSeeVUiA20dKu1ND2i/idPosgUXjhqbZrzPrFdnyDHOUaZzR1DsP+gBEvNN361kE06sa6upV93KdgU8XkQWKhkmRPRc1E2obQ/eLwG75wSqXI0pXPhx+3hVvcbQ6VwvwlmqPC4opjCBsccDNmLiwm2MuEybmHCmgJjJwc70TVYcZjkys1ri5sgQp4QX883y1mR7C/VaW9mG9l8MXLzIsTWHv9VTUz8NM+m/M1Qn/H0YYf6miLwe+C0Rc0QdK+es0Nx6syozWaErkDtrZrMfs8QVtO8JkkDslp2YJLMxx1GMM06Wkk34SFlvuEeZuFekFItDcga5vQud97VvMC3YhGAt1NP7fRPGZFiIZnOndAR7J2DzbugNBPl+jLbuoZqmucluYJI+NkJmowlDyTBZ6eLRbTO2cqHgwg3JEkeaLc9aMXGzHUTr5aQ5Vs1uYG8R+obhuGHY1AfST1WQy1Rb9ySpdNbM+DkynmngkyJbEeawAw0gniwynfZoqkVrZbbvoANwQt1BvMCa4xeLMAYMjsGxRUh5GC2dxIR8Bm1ybVBGZmKGTJ+ZV4+TJUW+kpJpOnskmASmxzNwsAu5DTRSK7d1IAg+6SCcUHcg51nhvl4EpuDE/SBpqkkehzFmufWSy5RxmGU5RIb+inDnSZEhxyj9prhfOWZWYLc06s4WoD6x302NE+oOZgyT55Gbgt4JqhFnoeb1gWBnpibJ9OToZ5QcGbKMc5ABsoyTZRw2Qi5Zwi97FKcTyD296EkNu7XZOKF2dAqHMT6kQhF6A3M7CFAJnk+aLTkFiZ6STfQwpvgAB/EoVzplxhMl8gnzfCYZZHU3GKepHZ1GkHu9aRpkjnZmimo10iKVZI4s4xSJk7f1wY9ivNIps0TcBKPEfJxQN4YobXccHUDZZ7amDhI+gphxa4Ib51i+orETlOxyV64SHx5PlEim88g9jbufCsGSVpStTXCauoPpxsjvDLbXdUiAiWGEeiOzTPBMT84uZxUq/blMzrVZNwq6f5SScaabwQTvwCUtp6k7mHNVGcWE9Y0UIT+GiTALqo9O2O0wFc2d8vO2eX2xUgUlSMXMMFlJ1YwnSnSl50WdN4b1L5LQUJxQdzjnqrIP2As8OAUz+zFu8WC9eoyqGT5lCijEKVYSO/o5WHGcZTlkYsFtJ00vVm68Cd6YyicNxZnfDs5X5VIRuoHuCThuFGN2ByZ4aG7dVYQUBeKUiFOsFCg0rxI2yaNozPJkqfEm+BEWSFNrb5ymdgBGY49humgeDmLDg/l1SFMzZUzwoBqKKSV8aFax/5itFR5PlNiQLCG3N+y2qnPqKFub4ITaUeEFqjwI7CuCDmOEOewJtyZ5fHqm4hiL4Vfm0iY2vGCropSsCe433h7sMPPbCbVjFi9Q5bfAg4HTLBDokLZOFKkIdVAJpbKcRRGPcqUySiJZbKxQd+Cc2gm1Yx4jdpsJzPCguIKNOpNpKh5wnxgFG1Hm20b1owxwiKwR8mTJlBFuFB24Tu2E2jGP86wZPhIIc3hubZ93kydBqeIg84lVoswS1jvuUSbl5c28+uYG3YybUzschhdYxxlTVGPBg/hwKwCmeZ55MUo/D7GVezmBgwxU9htt3UAT3JnfDkeVESAfaGefSpseJiEzMV0JPMmR4SD9DLO10kwvEGofj1jMN108bmjQjXSYUDfaL+loYgqYtMzUBFVNbeuZdRWNwAbVT/ayHQ+foxivCHqJOGU84wFPT0O6ARVRXDkjh6NKxX80txGALVJYxrNza9PDp5+DlIhziCwjE5soTZsuAam0babXiBLCR6i2GOoQnPntWJQyNhgrEObATLWvBxgFTLng3+W/2Mx+tjFEDJ/pyRRHHu7hyL4e8pMp/HIMkg0wwWs8p15tK1sROVVE/ts2i7xDRF6xlttaCqepHUtSANQuY1WYBiZg08QYxY3xSifMazmbvM2r7tsyylh5EzzcxZF9PRyJUU3tXE/qY36vppVtHvgjVd0jIluBXSJyraqO13pwTlM7FqXXPpbDyz2Bw2zMdPLIMo6PR4k4WQ6xjSE2MYLn2UIJAwpZbaz6WP8lrXmtbFX116q6xz4ftseOrumnWpymdizIv4gwiPmCxIKeEeECCpOYVMypaeI9JbrJs40hhthGghKbGSFzrCmcUCDF+FSWyYNZoAv5NuhL1ulGVlb5ZGCOSX2R7ek294rX2R5wF849Hmpl+0zgSQt9iIicDsSB30Qe2QpwQu1YlC67SfAtCZrpFTFLW0VITsDmnhFyZIzwchT9jFbSM/OkGGET5R6P/GTKFPtfT3/ZyoS6nq1sAbD9474KvFZVj0Qe2QpwQu2YxydE2Ez1y6FlkKDcUeBJDjR20USXmWyt8UpiR4Ycvo0LT1CqNNI7EgOyIN8ArZurKESN59SrbWWrqleKSC/wA+D9qvrz2o1qNm5O7ZhHd2jr80CSmHrgc1WANccTRVPSyASMFis9tqAadRanRCabg6Sa2uDpdbqZGoaJikiPiGSC55hWtnfN+jjV7ao6qKqDwBXAm61Ax4HvAv+mqlfU5N4WwWlqxzx67dYN9PcDfZiiCQmqBf9DJrlXPkI8UbRpl6Z7h4kH9yqVUIJGehvSeY5M96yvCd4crWxfDvwfoN/2pwY4T1VvW/yU1eGE2jGP81S5RoQckJ+C1CagH1MuGGY1qg/XBA/qlKXJVa5VtlFnCUoUbBz4ND2mVe5loK+u883UsPLJWlrZquqlwKW1GcnSOKF2LMjzbGue+0XY7mNUd6Clg/l0D8ZhNgaZ/hz9nuneMUo/JRKVawVpmia/usR0DKOp18MEd9VEHY7ZbFeFPcAwVZM5nIo5CYyaJnpxW+h/EyNkGbemd3lWGWEv5pv86iSQXYcbcFlaDsd89g0DQ1STOoIUzCCDqwwySqWq6Gb2k+UQHj6+NQa7bZP6uFc0pYOtUMu163ADHSbUzvx2LMthMPFPw5i5dbCsFWN2E72JaQobg+4dk6TIM27VcSLU9jaTzTE2HYdYArLz13JrSgdmadVNU4vIl0Vkv4jcFdrXJyLXi8ge+3hUvT7fUTsqBX6nqM6pQ4kdwRy7a4pK6eAU+VlN6YNKo3GKpLw8XckSxMr1N8Fd5ZOacjHw3Dn73gPcoKo7gBvsa0eTs0PVCPMU87/8YdN1ChK+WdoyzrFSpZCCh18JTukmTyabY0OyxIbsFPLLOg7ezalrhw2dG5uz+4XAJfb5JcCL6vX5jhoTmNoTVCdtgdYuVh9N+eCyLRNcrORbB2vVCes0S3l5Uum8KXWUrqN97IS67mxW1Yfs84cxi/kLIiIXiMitInLrgQMH1md0jsW5T41zLMg7KjM7Ftw+96xwBF7voJuHZ9v0mO6YJsIsniwBsME+1gVXTXT9UFXF/MkXO36Rqu5U1Z1HH12XDDXHSiljNHU4Lzrsao2ZbEsPn5itAZ4iz2ZGyHKoEhNeCSH1fPxyjCPT8fqa4E5T15URm6USZKvsX+b9jmZit1aXsmIYQQjiwmGWgMetVo5bEzzQ2KY9T6Ei2ACUY20lVI1mvYX6auC19vlrgavW+fMda2UMeAgjzInQfiuUUqYitLFQa54g2SNhtXdwzIuVjRfcLa7WjHouaX0N+G/gJBHZJyKvBz4OnCUie4Dfs68dLYQGGnVu1lYQC142HvAYPmU8SrYTph96s1n2mjOPLoP8rL5j7xTq9vuoqq9a5NCz6/WZjvojowpni/nm9FA1vYM1a7vkFQi0j2eF2iRkxm3jvJT1iudIG/N7GrPVnM6LPnFGj2PlxOy2MfQ8SMsEyp5XNa+tpztHOhQyalI0u8kD/VAW9Mnm3NrHl62s9Ek74ITasXJs3W8SGG2dtI8bqWpuqtlZAD4xSsTt0laxEjYai/noSfUcrNPUDsfy3KjwfqtTkxhh7jGbJiHm+3ieX1mbBshbv3fcNtX7IS825/cs9AG1xGlqhyMaRaqmd+A0S0DResTDzfP8ysq12fuNygLIeuA0tcMRjb9TuFRmlzgCSsku8l6qUnTQD9njV3Du+o8TpWalT1oEJ9SO1TMCbMeY4EnQGBS9BHm6KRLHx6to7K9yfoMG6TS1wxGdIeAJ9nkC8j0bKBGv1AD38HkdlzdyhLg5tcOxEvZgyhn1wnQP5BMpq6FjPIWaF8lcJZ2nqV05I8fq+YFC0Xi88z1JSsR5JA/xRO5u9MhC1Db3crVdL+2+19oCIXtEpG7eQqepHWtjwsR79zWtM6oumnrFXS9FpA/4IKaDh2K6Xl6tqodqPTinqR1r408UNi2aQdsENKRKwryul8DZwPWqOmYF+XrmVwaqCU6oHW3OiqokDASFOex2wSIXvE5Edi10PNT18gtzDh2LcS0G7LP7ao4zvx1tzoq833XverkeOKF2tDm1DT5ZbddL4EHgzND7jgNurNnAQjihdrQ5tXOU2U6XG1Q1F+p6+ZFZn6a6PfT+i4Hv266XfcDfhMpiPwd4b00GNgcn1I42p6be71V3vVTVMRH5KHCL3fURVZ1bbbcmOKF2tDm1iyhbS9dL+/rLwJdrMpglcELtaHM6L6LMCbWjzXGx3w5Hm+E0tcPRZjhN7XC0GU5TOxxtxhFc5ROHo63oPPNbTJ+65kZEDgC/beAQBoBFU+2aiHYY5yNUtWYdEUXkR/bzonBQVeuSObWetIRQNxoRuTVCoH/DceN0gEu9dDjaDifUDkeb4YQ6Ghc1egARceN0uDm1w9FuOE3tcLQZTqgdjjbDCXUEROQvRERFZMC+FhH5RxG5V0TuEJEnNnh8H7XjuE1ErhORrc02ThH5lIj8yo7juyKSDR17rx3jPSJydqPG2C44oV4GEdmGKT3zQGj384AddruA+ZUj15tPqerjVPVU4PvAB+z+Zhrn9cBjVfVxwK+xpXxE5GTglcDvYErm/rOtm+1YJU6ol+czwLsw8YYBLwT+TQ0/B7IickxDRgeo6uHQyx6qY22acarqdaoaxGv+HFN4Lxjj11W1qKr3A/diivk5VokT6iUQkRcCD6rq7XMOrVsN56iIyMdEZAh4NVVN3XTjtLwOuMY+b9Yxtiwdn9AhIv8ObFng0PuB92FM74azqfAEEAAABMNJREFU1DhV9SpVfT/wfhF5L/BWTIuXdWW5Mdr3vB+TYXHZeo6tk+h4oVbV31tov4icgum+fLutHnkc8L8icjqmhvO20NuPs/vWfZwLcBnwQ4xQr+s4lxujiJwHPB94tlYDJNb9b9nuOPN7EVT1TlXdpKqDqjqIMQufqKoPA1cDf2S9y08BJlT1oUaNVUR2hF6+EPiVfd404xSR52J8Ey9Q1Xzo0NXAK0UkISLbMU69/2nEGNuFjtfUq+SHwDkYp04e+OPGDoePi8hJmIoAvwXeaPc30zg/ByQwrWoAfq6qb1TVX4rIN4G7MWb5W1TVb+A4Wx4XJupwtBnO/HY42gwn1A5Hm+GE2uFoM5xQOxxthhNqh6PN6FihFpFtInK/7RuMiBxlXw/W6fPeKCJ/ZJ+fF2RS2df/ahMbavE5LxKRD9jnHxKRB2321m0i8vFVXC8rIm8OvT7aVuh0NCkdvaQlIu8CTlTVC0TkQmCvqv7tOnzujcA7VfXWOlz7vzABHgdF5EPApKr+3RquN4hpnP7Y0L6vAP+qqj9b43AddaBjNbXlM8BTROTtwBnAvC+/iAzaPODLRGS3iFwhIil77Nki8gsRuVNEviwiCbv/4yJyt80d/ju770Mi8k4ReSmwE7jMas9uEblRRHba973KXu8uEflEaByTNmnjdhH5uYhsXmCsjwKKqrpo7W8R8Wxu8y12fG8IHfvL0P4P290fB06wY/2U3XclJnHE0YyoakdvwNmYVMWzFjk+aI8/zb7+MvBOIInJLnqU3f9vwNuBfuAeqlZQ1j5+CKOdAW4EdoY+40aMoG/F5G0fjYn2+zHwIvseBf7APv8k8FcLjPWPgb8Pvf4QJo76Nrudjcmr/it7PAHciolxfw6mIKBgfuy/D/wfe/93zfmcY4E7G/2/c9vCW6drajCFBB4CHrvEe4a0ampeitHqJwH3q+qv7f5LMEIwAUwDXxKRF2PCM6PyJOBGVT2gJvf4MntNgBJG0AB2YYRtLscAB+bs+4yqnmq3azHC+0cichtwM+ZHaIfd/xzgF8D/Ao+2+xdiP+YHyNGEdHTst4icCpwFPAW4SUS+rgsnPMx1PCzqiFDVss3kejbwUkwa5LNqMNwZtWoS8Fn4f1cANi5zHQH+1Ap4dacpI/S3qnrhnP2DC1wjSad1nWshOlZTi8kq+ALwdlV9APgUC8ypLceLyFPt8z8EbsKY2IMicqLd/xrgpyKSBjaq6g+BPwcev8D1ckBmgf3/AzxDRAZsSZ9XAT9dwW3tBk5c5j3XAm8SkS4w83AR6bH7X2fHj4gcKyKbFhnro4C7VjAuxzrSsUINnA88oKrX29f/DDxGRJ6xwHvvAd4iIruBo4AvqOo0Zg77LRG5E5Mh9UWMAHxfRO7ACP87FrjexcAXA0dZsNNaCe8BfgLcDuxSW1wgIv8BPMH+YC3Gv2Iyov5XRO4CLgRiqnodcDnw3/Z+rgAyqjoK/Mw67gJH2TOBH6xgXI51pKOXtKKw0JJOMyMinwW+p6r/XsfP+A/ghap6qF6f4Vg9nayp25W/AVL1uriIHA182gl08+I0tcPRZjhN7XC0GU6oHY42wwn1/2+fDmQAAAAABvlb3+Mrh2BGapiRGmYCzFudCvz87bsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.figure import Figure\n",
    "\n",
    "#img = np.load(\"1600332703880_npimg.npy\", allow_pickle=True)\n",
    "img = np.load(\"1600371385971_npimg.npy\", encoding = 'bytes')\n",
    "img = img[:, 1:]\n",
    "fig = plt.figure()\n",
    "print(img)\n",
    "print(img.shape)\n",
    "\n",
    "print(np.nanmin(img[img != -np.inf]))\n",
    "print(np.nanmax(img[img != np.inf]))\n",
    "\n",
    "zmax = np.nanmin(img[img != -np.inf])\n",
    "zmin = np.nanmax(img[img != np.inf])\n",
    "print(img.max())\n",
    "print(img.min())\n",
    "\"\"\"\n",
    "maxz= 298.6142120361328/2\n",
    "minz= 298.4643096923828/2\n",
    "maxx= 2.2676000595092773\n",
    "minx= -2.5011000633239746\n",
    "maxy= 3.3237826631069183\n",
    "miny= 0.47209998965263367\n",
    "\"\"\"\n",
    "maxx= -18.123884002685546\n",
    "maxy= 58.166160583496094\n",
    "maxz= 5.541902542114258\n",
    "minx= -47.775150299072266\n",
    "miny= 2.8466339111328125\n",
    "minz= 5.391929626464844\n",
    "\n",
    "print(\"conv\", maxz/3.28084, minz/3.28084)\n",
    "ax = fig.add_axes([.57,.05,.9,.8])\n",
    "ax.set_xlabel(\"X position (Feet)\")\n",
    "ax.set_ylabel(\"Y position (Feet)\")\n",
    "\n",
    "img_plotted = ax.imshow(\n",
    "        img,\n",
    "        vmin=minz,\n",
    "        vmax=maxz,\n",
    "        extent=(minx, maxx, miny, maxy),\n",
    "        cmap = 'jet'\n",
    "    )\n",
    "\n",
    "fig.colorbar(img_plotted).set_label(\"Deviation (Inches)\")\n",
    "\n",
    "plt.show()\n",
    "nanRem = img[~np.isnan(img)]\n",
    "np.histogram(nanRem[nanRem != np.inf])\n",
    "\n",
    "print(np.where(img==147.25175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EventCollection\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# create random data\n",
    "xdata = np.random.random([2, 10])\n",
    "\n",
    "# split the data into two parts\n",
    "xdata1 = xdata[0, :]\n",
    "xdata2 = xdata[1, :]\n",
    "\n",
    "# sort the data so it makes clean curves\n",
    "xdata1.sort()\n",
    "xdata2.sort()\n",
    "\n",
    "# create some y data points\n",
    "ydata1 = xdata1 ** 2\n",
    "ydata2 = 1 - xdata2 ** 3\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(xdata1, ydata1, color='tab:blue')\n",
    "ax.plot(xdata2, ydata2, color='tab:orange')\n",
    "\n",
    "# create the events marking the x data points\n",
    "xevents1 = EventCollection(xdata1, color='tab:blue', linelength=0.05)\n",
    "xevents2 = EventCollection(xdata2, color='tab:orange', linelength=0.05)\n",
    "\n",
    "# create the events marking the y data points\n",
    "yevents1 = EventCollection(ydata1, color='tab:blue', linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "yevents2 = EventCollection(ydata2, color='tab:orange', linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "\n",
    "# add the events to the axis\n",
    "ax.add_collection(xevents1)\n",
    "ax.add_collection(xevents2)\n",
    "ax.add_collection(yevents1)\n",
    "ax.add_collection(yevents2)\n",
    "\n",
    "# set the limits\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_title('line plot with data points')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
