{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "#soft actor critic agent\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import normal_projection_network\n",
    "from tf_agents.policies import greedy_policy\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 30 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#catagorical dqn agent\n",
    "gamma = 0.99\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1))\n"
     ]
    }
   ],
   "source": [
    "print(tf_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "preprocessing_combiner layer is required when more than 1 input_tensor_spec is provided.\n  In call to configurable 'EncodingNetwork' (<function EncodingNetwork.__init__ at 0x7fa02ddfd1f0>)\n  In call to configurable 'ActorDistributionNetwork' (<function ActorDistributionNetwork.__init__ at 0x7fa02de15670>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75157c3b9233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m actor_net = actor_distribution_network.ActorDistributionNetwork(\n\u001b[0m\u001b[1;32m     34\u001b[0m      \u001b[0mtf_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m      \u001b[0mtf_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m_capture_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0;31m# Avoid auto tracking which prevents keras from tracking layers that are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;31m# passed as kwargs to the Network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/networks/actor_distribution_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_tensor_spec, output_tensor_spec, preprocessing_layers, preprocessing_combiner, conv_layer_params, fc_layer_params, dropout_layer_params, activation_fn, kernel_initializer, batch_squash, dtype, discrete_projection_net, continuous_projection_net, name)\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglorot_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     encoder = encoding_network.EncodingNetwork(\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0minput_tensor_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mpreprocessing_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/gin/config.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m_capture_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m       \u001b[0;31m# Avoid auto tracking which prevents keras from tracking layers that are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;31m# passed as kwargs to the Network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/networks/encoding_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_tensor_spec, preprocessing_layers, preprocessing_combiner, conv_layer_params, fc_layer_params, dropout_layer_params, activation_fn, weight_decay_params, kernel_initializer, batch_squash, dtype, name, conv_type)\u001b[0m\n\u001b[1;32m    206\u001b[0m     if (len(tf.nest.flatten(input_tensor_spec)) > 1 and\n\u001b[1;32m    207\u001b[0m         preprocessing_combiner is None):\n\u001b[0;32m--> 208\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    209\u001b[0m           \u001b[0;34m'preprocessing_combiner layer is required when more than 1 '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m           'input_tensor_spec is provided.')\n",
      "\u001b[0;31mValueError\u001b[0m: preprocessing_combiner layer is required when more than 1 input_tensor_spec is provided.\n  In call to configurable 'EncodingNetwork' (<function EncodingNetwork.__init__ at 0x7fa02ddfd1f0>)\n  In call to configurable 'ActorDistributionNetwork' (<function ActorDistributionNetwork.__init__ at 0x7fa02de15670>)"
     ]
    }
   ],
   "source": [
    "#soft actor critic\n",
    "\n",
    "#hyper parameters\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "gradient_clipping = None # @param\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "#agent networks\n",
    "critic_net = critic_network.CriticNetwork(\n",
    "    (tf_env.observation_spec(), tf_env.action_spec()),\n",
    "    observation_fc_layer_params=None,\n",
    "    action_fc_layer_params=None,\n",
    "    joint_fc_layer_params=critic_joint_fc_layer_params)\n",
    "\n",
    "def normal_projection_net(action_spec, init_means_output_factor=0.1):\n",
    "  return normal_projection_network.NormalProjectionNetwork(\n",
    "      action_spec,\n",
    "      mean_transform=None,\n",
    "      state_dependent_std=True,\n",
    "      init_means_output_factor=init_means_output_factor,\n",
    "      std_transform=sac_agent.std_clip_transform,\n",
    "      scale_distribution=True)\n",
    "\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "     tf_env.time_step_spec(),\n",
    "     tf_env.action_spec(),\n",
    "    fc_layer_params=actor_fc_layer_params,\n",
    "    continuous_projection_net=normal_projection_net)\n",
    "\n",
    "#agent\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "tf_agent = sac_agent.SacAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    critic_network=critic_net,\n",
    "    actor_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=actor_learning_rate),\n",
    "    critic_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=critic_learning_rate),\n",
    "    alpha_optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=alpha_learning_rate),\n",
    "    target_update_tau=target_update_tau,\n",
    "    target_update_period=target_update_period,\n",
    "    td_errors_loss_fn=tf.compat.v1.losses.mean_squared_error,\n",
    "    gamma=gamma,\n",
    "    reward_scale_factor=reward_scale_factor,\n",
    "    gradient_clipping=gradient_clipping,\n",
    "    train_step_counter=global_step)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fc402c55400>\n"
     ]
    }
   ],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=n_step_update+1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npolicy.action(reset)\\n#tf_env.time_step_spec()\\nprint(reset)\\n#print(env.reset())\\n#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\\nprint(\" \")\\nprint(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\\n\\n#print(tensor_spec.to_array_spec(reset))\\n#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\\n#encoder_func(env.reset())\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation(acceptPol)\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-140\n",
      "105\n",
      "200\n",
      "50\n",
      "-5\n",
      "70\n",
      "-55\n",
      "100\n",
      "125\n",
      "-5\n",
      "90\n",
      "80\n",
      "150\n",
      "25\n",
      "75\n",
      "-50\n",
      "0\n",
      "135\n",
      "0\n",
      "10\n",
      "45\n",
      "70\n",
      "35\n",
      "10\n",
      "110\n",
      "50\n",
      "10\n",
      "145\n",
      "-10\n",
      "95\n",
      "105\n",
      "125\n",
      "175\n",
      "140\n",
      "-100\n",
      "160\n",
      "driver reward  2125\n",
      "70\n",
      "25\n",
      "50\n",
      "15\n",
      "140\n",
      "175\n",
      "5\n",
      "130\n",
      "45\n",
      "60\n",
      "120\n",
      "-25\n",
      "-5\n",
      "-20\n",
      "210\n",
      "40\n",
      "75\n",
      "200\n",
      "135\n",
      "40\n",
      "40\n",
      "105\n",
      "110\n",
      "40\n",
      "-40\n",
      "100\n",
      "85\n",
      "55\n",
      "190\n",
      "70\n",
      "140\n",
      "20\n",
      "40\n",
      "55\n",
      "70\n",
      "30\n",
      "75\n",
      "10\n",
      "110\n",
      "530\n",
      "570\n",
      "driver reward  3890\n",
      "60\n",
      "155\n",
      "70\n",
      "25\n",
      "50\n",
      "-20\n",
      "50\n",
      "170\n",
      "0\n",
      "105\n",
      "-10\n",
      "-60\n",
      "185\n",
      "30\n",
      "100\n",
      "155\n",
      "125\n",
      "85\n",
      "235\n",
      "55\n",
      "70\n",
      "40\n",
      "10\n",
      "70\n",
      "110\n",
      "95\n",
      "185\n",
      "70\n",
      "155\n",
      "-15\n",
      "75\n",
      "5\n",
      "55\n",
      "155\n",
      "45\n",
      "55\n",
      "75\n",
      "95\n",
      "40\n",
      "driver reward  2955\n",
      "-15\n",
      "175\n",
      "15\n",
      "110\n",
      "-25\n",
      "165\n",
      "70\n",
      "35\n",
      "165\n",
      "5\n",
      "115\n",
      "235\n",
      "165\n",
      "180\n",
      "105\n",
      "85\n",
      "5\n",
      "100\n",
      "-50\n",
      "-50\n",
      "65\n",
      "100\n",
      "25\n",
      "20\n",
      "25\n",
      "240\n",
      "-60\n",
      "-40\n",
      "-10\n",
      "125\n",
      "250\n",
      "25\n",
      "85\n",
      "-5\n",
      "60\n",
      "125\n",
      "105\n",
      "35\n",
      "65\n",
      "575\n",
      "610\n",
      "595\n",
      "575\n",
      "560\n",
      "600\n",
      "driver reward  6340\n",
      "55\n",
      "165\n",
      "-50\n",
      "240\n",
      "135\n",
      "60\n",
      "235\n",
      "250\n",
      "230\n",
      "95\n",
      "150\n",
      "115\n",
      "115\n",
      "35\n",
      "65\n",
      "100\n",
      "140\n",
      "170\n",
      "25\n",
      "110\n",
      "100\n",
      "100\n",
      "-40\n",
      "10\n",
      "-35\n",
      "125\n",
      "105\n",
      "265\n",
      "100\n",
      "40\n",
      "40\n",
      "50\n",
      "75\n",
      "125\n",
      "195\n",
      "15\n",
      "245\n",
      "165\n",
      "70\n",
      "driver reward  4190\n",
      "0\n",
      "15\n",
      "115\n",
      "185\n",
      "5\n",
      "125\n",
      "90\n",
      "15\n",
      "60\n",
      "20\n",
      "95\n",
      "80\n",
      "305\n",
      "55\n",
      "110\n",
      "60\n",
      "-35\n",
      "45\n",
      "45\n",
      "105\n",
      "155\n",
      "140\n",
      "60\n",
      "-20\n",
      "60\n",
      "-75\n",
      "160\n",
      "60\n",
      "85\n",
      "5\n",
      "220\n",
      "170\n",
      "70\n",
      "60\n",
      "140\n",
      "50\n",
      "65\n",
      "175\n",
      "65\n",
      "595\n",
      "565\n",
      "505\n",
      "635\n",
      "525\n",
      "605\n",
      "driver reward  6570\n",
      "65\n",
      "60\n",
      "-80\n",
      "110\n",
      "120\n",
      "285\n",
      "140\n",
      "-70\n",
      "155\n",
      "145\n",
      "-15\n",
      "25\n",
      "10\n",
      "85\n",
      "55\n",
      "190\n",
      "25\n",
      "115\n",
      "115\n",
      "-20\n",
      "15\n",
      "185\n",
      "105\n",
      "80\n",
      "55\n",
      "225\n",
      "210\n",
      "-10\n",
      "275\n",
      "50\n",
      "160\n",
      "45\n",
      "-55\n",
      "115\n",
      "135\n",
      "105\n",
      "-5\n",
      "105\n",
      "85\n",
      "driver reward  3395\n",
      "45\n",
      "115\n",
      "10\n",
      "110\n",
      "110\n",
      "35\n",
      "150\n",
      "215\n",
      "240\n",
      "90\n",
      "-50\n",
      "-35\n",
      "200\n",
      "80\n",
      "30\n",
      "55\n",
      "230\n",
      "180\n",
      "20\n",
      "100\n",
      "60\n",
      "85\n",
      "-15\n",
      "105\n",
      "205\n",
      "185\n",
      "-35\n",
      "95\n",
      "20\n",
      "5\n",
      "140\n",
      "110\n",
      "120\n",
      "70\n",
      "-30\n",
      "driver reward  3050\n",
      "175\n",
      "100\n",
      "150\n",
      "70\n",
      "130\n",
      "15\n",
      "45\n",
      "125\n",
      "50\n",
      "-5\n",
      "200\n",
      "85\n",
      "115\n",
      "-10\n",
      "50\n",
      "0\n",
      "55\n",
      "25\n",
      "100\n",
      "-20\n",
      "-35\n",
      "80\n",
      "155\n",
      "-25\n",
      "65\n",
      "115\n",
      "95\n",
      "200\n",
      "20\n",
      "190\n",
      "-30\n",
      "175\n",
      "55\n",
      "-10\n",
      "180\n",
      "85\n",
      "35\n",
      "10\n",
      "145\n",
      "520\n",
      "525\n",
      "650\n",
      "555\n",
      "500\n",
      "710\n",
      "driver reward  6420\n",
      "-135\n",
      "70\n",
      "130\n",
      "140\n",
      "215\n",
      "155\n",
      "80\n",
      "-85\n",
      "70\n",
      "145\n",
      "90\n",
      "40\n",
      "55\n",
      "100\n",
      "160\n",
      "40\n",
      "100\n",
      "-35\n",
      "15\n",
      "70\n",
      "-25\n",
      "20\n",
      "80\n",
      "20\n",
      "5\n",
      "30\n",
      "115\n",
      "100\n",
      "90\n",
      "45\n",
      "110\n",
      "75\n",
      "65\n",
      "15\n",
      "35\n",
      "130\n",
      "55\n",
      "50\n",
      "130\n",
      "505\n",
      "590\n",
      "640\n",
      "driver reward  4300\n",
      "55\n",
      "120\n",
      "75\n",
      "130\n",
      "155\n",
      "-45\n",
      "15\n",
      "55\n",
      "100\n",
      "245\n",
      "145\n",
      "120\n",
      "30\n",
      "155\n",
      "285\n",
      "175\n",
      "70\n",
      "60\n",
      "150\n",
      "-10\n",
      "115\n",
      "40\n",
      "130\n",
      "25\n",
      "-20\n",
      "180\n",
      "80\n",
      "65\n",
      "220\n",
      "55\n",
      "110\n",
      "30\n",
      "170\n",
      "10\n",
      "140\n",
      "15\n",
      "95\n",
      "driver reward  3545\n",
      "15\n",
      "155\n",
      "55\n",
      "70\n",
      "230\n",
      "250\n",
      "225\n",
      "105\n",
      "50\n",
      "20\n",
      "75\n",
      "95\n",
      "45\n",
      "120\n",
      "220\n",
      "95\n",
      "5\n",
      "140\n",
      "75\n",
      "165\n",
      "105\n",
      "-15\n",
      "120\n",
      "35\n",
      "65\n",
      "-75\n",
      "155\n",
      "145\n",
      "130\n",
      "115\n",
      "-25\n",
      "65\n",
      "55\n",
      "5\n",
      "65\n",
      "115\n",
      "120\n",
      "110\n",
      "-5\n",
      "driver reward  3495\n",
      "125\n",
      "75\n",
      "165\n",
      "-10\n",
      "75\n",
      "30\n",
      "-20\n",
      "170\n",
      "-15\n",
      "20\n",
      "-130\n",
      "55\n",
      "-15\n",
      "70\n",
      "-90\n",
      "85\n",
      "15\n",
      "135\n",
      "55\n",
      "80\n",
      "-40\n",
      "35\n",
      "75\n",
      "120\n",
      "215\n",
      "200\n",
      "-10\n",
      "50\n",
      "205\n",
      "205\n",
      "105\n",
      "-55\n",
      "-20\n",
      "120\n",
      "60\n",
      "40\n",
      "190\n",
      "-55\n",
      "90\n",
      "660\n",
      "735\n",
      "535\n",
      "565\n",
      "driver reward  4900\n",
      "40\n",
      "150\n",
      "90\n",
      "145\n",
      "180\n",
      "70\n",
      "140\n",
      "-40\n",
      "110\n",
      "145\n",
      "180\n",
      "130\n",
      "35\n",
      "-45\n",
      "-5\n",
      "-35\n",
      "0\n",
      "10\n",
      "85\n",
      "-45\n",
      "100\n",
      "55\n",
      "120\n",
      "-5\n",
      "95\n",
      "120\n",
      "185\n",
      "50\n",
      "80\n",
      "30\n",
      "-55\n",
      "5\n",
      "40\n",
      "70\n",
      "10\n",
      "10\n",
      "55\n",
      "85\n",
      "-5\n",
      "driver reward  2385\n",
      "195\n",
      "155\n",
      "35\n",
      "85\n",
      "45\n",
      "70\n",
      "-15\n",
      "10\n",
      "50\n",
      "115\n",
      "115\n",
      "5\n",
      "140\n",
      "80\n",
      "-20\n",
      "125\n",
      "50\n",
      "265\n",
      "-25\n",
      "115\n",
      "60\n",
      "270\n",
      "105\n",
      "90\n",
      "50\n",
      "0\n",
      "-40\n",
      "30\n",
      "75\n",
      "-25\n",
      "165\n",
      "185\n",
      "-35\n",
      "35\n",
      "-40\n",
      "0\n",
      "60\n",
      "driver reward  2580\n",
      "110\n",
      "80\n",
      "35\n",
      "105\n",
      "110\n",
      "140\n",
      "75\n",
      "35\n",
      "70\n",
      "-5\n",
      "95\n",
      "110\n",
      "190\n",
      "135\n",
      "145\n",
      "-45\n",
      "70\n",
      "140\n",
      "75\n",
      "175\n",
      "140\n",
      "120\n",
      "-10\n",
      "-5\n",
      "225\n",
      "-15\n",
      "115\n",
      "-10\n",
      "165\n",
      "165\n",
      "65\n",
      "210\n",
      "-60\n",
      "60\n",
      "50\n",
      "165\n",
      "35\n",
      "95\n",
      "75\n",
      "driver reward  3430\n",
      "65\n",
      "-20\n",
      "90\n",
      "65\n",
      "195\n",
      "105\n",
      "-95\n",
      "90\n",
      "45\n",
      "15\n",
      "-10\n",
      "70\n",
      "115\n",
      "65\n",
      "145\n",
      "180\n",
      "25\n",
      "70\n",
      "30\n",
      "135\n",
      "55\n",
      "130\n",
      "65\n",
      "5\n",
      "10\n",
      "260\n",
      "30\n",
      "45\n",
      "100\n",
      "-35\n",
      "135\n",
      "110\n",
      "35\n",
      "90\n",
      "0\n",
      "125\n",
      "160\n",
      "140\n",
      "driver reward  2840\n",
      "195\n",
      "90\n",
      "110\n",
      "-5\n",
      "30\n",
      "70\n",
      "-10\n",
      "-10\n",
      "0\n",
      "85\n",
      "85\n",
      "120\n",
      "90\n",
      "80\n",
      "-25\n",
      "60\n",
      "55\n",
      "-60\n",
      "50\n",
      "105\n",
      "155\n",
      "245\n",
      "105\n",
      "115\n",
      "10\n",
      "50\n",
      "-55\n",
      "120\n",
      "5\n",
      "110\n",
      "35\n",
      "50\n",
      "-10\n",
      "10\n",
      "-20\n",
      "205\n",
      "90\n",
      "160\n",
      "45\n",
      "685\n",
      "655\n",
      "480\n",
      "540\n",
      "470\n",
      "driver reward  5370\n",
      "155\n",
      "25\n",
      "95\n",
      "190\n",
      "160\n",
      "10\n",
      "70\n",
      "5\n",
      "0\n",
      "145\n",
      "90\n",
      "0\n",
      "105\n",
      "135\n",
      "40\n",
      "90\n",
      "-70\n",
      "45\n",
      "20\n",
      "-15\n",
      "60\n",
      "105\n",
      "120\n",
      "55\n",
      "110\n",
      "45\n",
      "-10\n",
      "10\n",
      "45\n",
      "15\n",
      "65\n",
      "220\n",
      "105\n",
      "65\n",
      "75\n",
      "185\n",
      "30\n",
      "115\n",
      "driver reward  2710\n",
      "-5\n",
      "10\n",
      "-60\n",
      "-20\n",
      "-55\n",
      "95\n",
      "40\n",
      "90\n",
      "40\n",
      "150\n",
      "55\n",
      "-5\n",
      "150\n",
      "65\n",
      "170\n",
      "160\n",
      "130\n",
      "190\n",
      "155\n",
      "85\n",
      "155\n",
      "85\n",
      "15\n",
      "-45\n",
      "100\n",
      "95\n",
      "10\n",
      "145\n",
      "65\n",
      "-15\n",
      "85\n",
      "130\n",
      "140\n",
      "65\n",
      "40\n",
      "40\n",
      "90\n",
      "100\n",
      "185\n",
      "545\n",
      "driver reward  3470\n",
      "total reward  77960\n"
     ]
    }
   ],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "\n",
    "evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average returnstep\n",
    "def compute_avg_return(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        episode_reward = 0\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            driver_reward = 0\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                driver_reward += state_tf.reward\n",
    "            episode_reward += driver_reward\n",
    "        \n",
    "        #take average reward for all drivers in the episode\n",
    "        episode_reward = episode_reward / len(state_lists)\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_return = total_reward / num_episodes\n",
    "    print(avg_return)\n",
    "    return avg_return.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories\n",
    "\n",
    "def collect_data(num_iterations, policy, replay_buffer):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        print(\"driver count : \", len(state_lists))\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            actions = []\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                #create time step\n",
    "                if i == 0:\n",
    "                    #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                    state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                    #print (state_tf)\n",
    "                elif i < (len(state_list) - 1):\n",
    "                    #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                else:\n",
    "                    state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                #create action\n",
    "                \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                    action = tf.constant([1], dtype=tf.int32)\n",
    "                else:\n",
    "                    action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                action = state_list[i][\"action\"]\n",
    "\n",
    "                #print (action)\n",
    "                states.append(state_tf)\n",
    "                actions.append(action)\n",
    "\n",
    "            for j in range(len(states)-1):\n",
    "                present_state = states[j]\n",
    "                next_state = states[j+1]\n",
    "                action = actions[j]\n",
    "                traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                #print(action)\n",
    "                # Add trajectory to the replay buffer\n",
    "                replay_buffer.add_batch(traj)\n",
    "                #print(traj)\n",
    "\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4606.25, shape=(), dtype=float32)\n",
      " Average Return = 4606.25\n",
      "driver count :  20\n",
      "WARNING:tensorflow:From /home/haritha/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/utils/value_ops.py:85: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/haritha/anaconda3/envs/tf_agent/lib/python3.8/site-packages/tf_agents/utils/value_ops.py:85: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(4419.875, shape=(), dtype=float32)\n",
      "step = 5: Average Return = 4419.875\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 10: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=1.6583428>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(4103.375, shape=(), dtype=float32)\n",
      "step = 10: Average Return = 4103.375\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(-598.75, shape=(), dtype=float32)\n",
      "step = 15: Average Return = -598.75\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 20: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=1.0880823>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(11484.125, shape=(), dtype=float32)\n",
      "step = 20: Average Return = 11484.125\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(4762.875, shape=(), dtype=float32)\n",
      "step = 25: Average Return = 4762.875\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 30: loss = LossInfo(loss=<tf.Tensor: shape=(), dtype=float32, numpy=1.8615413>, extra=DqnLossInfo(td_loss=(), td_error=()))\n",
      "tf.Tensor(4793.625, shape=(), dtype=float32)\n",
      "step = 30: Average Return = 4793.625\n",
      "evaluation\n"
     ]
    }
   ],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print(' Average Return = {0}'.format( avg_return))\n",
    "returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize progress\n",
    "iterations = range(0, num_iterations +1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5\n",
      "41.0\n",
      "13.5\n",
      "-5.5\n",
      "52.5\n",
      "21.5\n",
      "62.5\n",
      "17.0\n",
      "0\n",
      "59.0\n",
      "33.5\n",
      "-3.0\n",
      "131.0\n",
      "52.5\n",
      "0\n",
      "156.5\n",
      "67.0\n",
      "61.0\n",
      "0\n",
      "27.0\n",
      "-1.5\n",
      "13.0\n",
      "44.0\n",
      "48.5\n",
      "0\n",
      "31.5\n",
      "93.5\n",
      "0\n",
      "137.5\n",
      "139.0\n",
      "151.5\n",
      "3.5\n",
      "0\n",
      "22.5\n",
      "62.5\n",
      "0\n",
      "0\n",
      "151.5\n",
      "123.0\n",
      "511.5\n",
      "driver reward  2366.0\n",
      "14.0\n",
      "0\n",
      "0\n",
      "93.0\n",
      "66.5\n",
      "67.0\n",
      "95.5\n",
      "100.0\n",
      "60.0\n",
      "3.0\n",
      "19.0\n",
      "0\n",
      "53.0\n",
      "20.0\n",
      "117.5\n",
      "105.5\n",
      "0\n",
      "61.0\n",
      "78.5\n",
      "0\n",
      "11.0\n",
      "70.5\n",
      "12.0\n",
      "0\n",
      "-7.0\n",
      "75.5\n",
      "0\n",
      "0\n",
      "4.5\n",
      "60.5\n",
      "104.0\n",
      "31.0\n",
      "37.0\n",
      "0\n",
      "0\n",
      "125.0\n",
      "6.5\n",
      "44.0\n",
      "driver reward  1528.0\n",
      "-19.0\n",
      "0.0\n",
      "100.0\n",
      "0\n",
      "94.0\n",
      "0\n",
      "60.0\n",
      "38.0\n",
      "8.0\n",
      "29.5\n",
      "0\n",
      "0\n",
      "6.0\n",
      "45.5\n",
      "72.5\n",
      "12.5\n",
      "177.5\n",
      "0\n",
      "52.0\n",
      "0\n",
      "98.5\n",
      "64.5\n",
      "47.5\n",
      "0\n",
      "3.0\n",
      "83.5\n",
      "70.5\n",
      "2.0\n",
      "25.5\n",
      "-3.5\n",
      "0\n",
      "10.0\n",
      "0\n",
      "-1.0\n",
      "38.0\n",
      "5.0\n",
      "0\n",
      "driver reward  1120.0\n",
      "46.5\n",
      "77.5\n",
      "60.5\n",
      "84.0\n",
      "94.5\n",
      "75.0\n",
      "1.5\n",
      "0\n",
      "74.0\n",
      "141.0\n",
      "54.0\n",
      "59.5\n",
      "0\n",
      "64.0\n",
      "76.0\n",
      "20.5\n",
      "-28.0\n",
      "49.0\n",
      "103.5\n",
      "0\n",
      "0\n",
      "37.0\n",
      "25.5\n",
      "0\n",
      "-9.5\n",
      "0\n",
      "14.0\n",
      "0\n",
      "-21.5\n",
      "21.0\n",
      "83.0\n",
      "50.0\n",
      "78.0\n",
      "54.0\n",
      "0\n",
      "27.0\n",
      "0\n",
      "35.0\n",
      "40.5\n",
      "580.5\n",
      "511.5\n",
      "577.5\n",
      "571.5\n",
      "driver reward  3728.0\n",
      "55.5\n",
      "10.0\n",
      "0\n",
      "8.0\n",
      "8.0\n",
      "60.5\n",
      "18.0\n",
      "125.5\n",
      "42.0\n",
      "134.5\n",
      "21.5\n",
      "-20.0\n",
      "0\n",
      "117.0\n",
      "0\n",
      "29.5\n",
      "9.5\n",
      "2.0\n",
      "0\n",
      "0\n",
      "-23.0\n",
      "72.0\n",
      "7.5\n",
      "0\n",
      "85.5\n",
      "72.0\n",
      "104.0\n",
      "0\n",
      "110.0\n",
      "-5.0\n",
      "0\n",
      "driver reward  1044.5\n",
      "58.5\n",
      "149.0\n",
      "46.0\n",
      "60.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "120.5\n",
      "49.0\n",
      "0\n",
      "0\n",
      "49.0\n",
      "132.5\n",
      "157.0\n",
      "-15.5\n",
      "160.5\n",
      "-5.0\n",
      "94.0\n",
      "46.0\n",
      "53.0\n",
      "58.0\n",
      "54.0\n",
      "157.5\n",
      "120.5\n",
      "0\n",
      "45.5\n",
      "31.0\n",
      "108.5\n",
      "driver reward  1729.5\n",
      "170.0\n",
      "55.5\n",
      "0\n",
      "38.5\n",
      "75.5\n",
      "109.5\n",
      "0\n",
      "112.0\n",
      "42.0\n",
      "42.5\n",
      "0\n",
      "87.0\n",
      "0\n",
      "-3.0\n",
      "18.0\n",
      "0\n",
      "13.5\n",
      "54.0\n",
      "-16.0\n",
      "-1.5\n",
      "46.5\n",
      "180.5\n",
      "149.0\n",
      "84.5\n",
      "44.5\n",
      "23.0\n",
      "0\n",
      "0\n",
      "87.0\n",
      "0\n",
      "0\n",
      "76.5\n",
      "0\n",
      "52.5\n",
      "100.5\n",
      "66.5\n",
      "0\n",
      "driver reward  1708.5\n",
      "33.0\n",
      "99.5\n",
      "27.0\n",
      "0\n",
      "0\n",
      "130.0\n",
      "7.5\n",
      "0\n",
      "0\n",
      "86.0\n",
      "101.5\n",
      "150.0\n",
      "36.0\n",
      "42.5\n",
      "8.0\n",
      "-1.0\n",
      "33.5\n",
      "46.5\n",
      "3.5\n",
      "48.5\n",
      "0\n",
      "-2.0\n",
      "0\n",
      "0\n",
      "0\n",
      "41.5\n",
      "60.5\n",
      "-2.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57.5\n",
      "-11.0\n",
      "0\n",
      "0\n",
      "-27.0\n",
      "16.5\n",
      "0\n",
      "driver reward  986.0\n",
      "124.0\n",
      "-4.0\n",
      "33.0\n",
      "81.0\n",
      "131.5\n",
      "83.5\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "11.0\n",
      "94.5\n",
      "130.0\n",
      "7.5\n",
      "60.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-4.5\n",
      "67.5\n",
      "-5.5\n",
      "0\n",
      "0\n",
      "82.0\n",
      "22.5\n",
      "0\n",
      "47.5\n",
      "58.0\n",
      "15.0\n",
      "140.5\n",
      "0\n",
      "57.0\n",
      "18.5\n",
      "93.0\n",
      "188.0\n",
      "109.5\n",
      "34.0\n",
      "575.5\n",
      "560.5\n",
      "500.5\n",
      "557.0\n",
      "0\n",
      "539.5\n",
      "536.5\n",
      "600.5\n",
      "0\n",
      "520.5\n",
      "driver reward  6065.5\n",
      "0\n",
      "-12.0\n",
      "172.0\n",
      "-9.0\n",
      "-3.0\n",
      "19.5\n",
      "0\n",
      "88.5\n",
      "0\n",
      "0\n",
      "39.0\n",
      "35.0\n",
      "0\n",
      "62.5\n",
      "77.5\n",
      "0\n",
      "25.0\n",
      "40.0\n",
      "34.0\n",
      "39.0\n",
      "6.5\n",
      "0\n",
      "72.5\n",
      "71.5\n",
      "175.5\n",
      "7.5\n",
      "31.0\n",
      "26.5\n",
      "9.0\n",
      "0\n",
      "3.0\n",
      "0\n",
      "83.0\n",
      "0\n",
      "8.5\n",
      "0\n",
      "37.0\n",
      "0\n",
      "71.0\n",
      "536.0\n",
      "0\n",
      "611.0\n",
      "555.5\n",
      "552.0\n",
      "driver reward  3465.0\n",
      "93.0\n",
      "63.5\n",
      "7.5\n",
      "-9.0\n",
      "0\n",
      "0\n",
      "0\n",
      "48.5\n",
      "0\n",
      "41.0\n",
      "0\n",
      "19.5\n",
      "77.5\n",
      "0\n",
      "40.5\n",
      "88.5\n",
      "54.0\n",
      "0\n",
      "36.0\n",
      "18.0\n",
      "64.0\n",
      "54.0\n",
      "0\n",
      "0\n",
      "43.0\n",
      "-15.0\n",
      "14.5\n",
      "89.5\n",
      "0\n",
      "11.0\n",
      "94.0\n",
      "83.0\n",
      "0\n",
      "24.5\n",
      "41.0\n",
      "0\n",
      "40.5\n",
      "11.0\n",
      "0\n",
      "driver reward  1133.5\n",
      "127.5\n",
      "138.5\n",
      "138.5\n",
      "0\n",
      "6.0\n",
      "15.0\n",
      "14.0\n",
      "0\n",
      "123.0\n",
      "112.5\n",
      "16.0\n",
      "55.5\n",
      "90.5\n",
      "0\n",
      "18.5\n",
      "0\n",
      "23.0\n",
      "9.5\n",
      "4.5\n",
      "72.5\n",
      "0\n",
      "-5.5\n",
      "118.0\n",
      "62.5\n",
      "0\n",
      "0\n",
      "-12.0\n",
      "0\n",
      "18.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "42.5\n",
      "4.5\n",
      "91.0\n",
      "0\n",
      "driver reward  1284.0\n",
      "32.5\n",
      "-28.0\n",
      "0\n",
      "0\n",
      "45.5\n",
      "0\n",
      "30.5\n",
      "41.5\n",
      "0\n",
      "85.0\n",
      "-5.5\n",
      "0\n",
      "0\n",
      "21.5\n",
      "78.0\n",
      "98.5\n",
      "102.0\n",
      "3.0\n",
      "0\n",
      "86.0\n",
      "0\n",
      "99.5\n",
      "55.0\n",
      "1.0\n",
      "0\n",
      "73.5\n",
      "0\n",
      "-29.5\n",
      "0\n",
      "12.5\n",
      "94.0\n",
      "149.0\n",
      "23.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "57.5\n",
      "driver reward  1126.0\n",
      "119.0\n",
      "59.0\n",
      "104.0\n",
      "0.0\n",
      "0\n",
      "5.5\n",
      "43.5\n",
      "135.0\n",
      "68.5\n",
      "-7.0\n",
      "0\n",
      "13.5\n",
      "50.0\n",
      "120.0\n",
      "119.0\n",
      "0\n",
      "0\n",
      "34.5\n",
      "0\n",
      "139.5\n",
      "48.0\n",
      "86.0\n",
      "141.5\n",
      "96.5\n",
      "179.0\n",
      "34.5\n",
      "53.5\n",
      "-8.0\n",
      "113.0\n",
      "116.5\n",
      "81.0\n",
      "0\n",
      "63.5\n",
      "0\n",
      "6.0\n",
      "62.5\n",
      "70.5\n",
      "0\n",
      "126.5\n",
      "480.5\n",
      "572.5\n",
      "596.5\n",
      "driver reward  3924.0\n",
      "0\n",
      "0\n",
      "85.5\n",
      "60.5\n",
      "0\n",
      "48.5\n",
      "122.5\n",
      "-14.0\n",
      "67.0\n",
      "123.5\n",
      "123.5\n",
      "73.5\n",
      "0\n",
      "44.0\n",
      "65.0\n",
      "0\n",
      "148.0\n",
      "45.0\n",
      "0\n",
      "20.5\n",
      "-5.5\n",
      "0\n",
      "36.0\n",
      "46.0\n",
      "24.0\n",
      "0\n",
      "0\n",
      "0\n",
      "49.0\n",
      "9.0\n",
      "7.0\n",
      "18.5\n",
      "0\n",
      "0\n",
      "0\n",
      "5.0\n",
      "33.5\n",
      "26.0\n",
      "95.0\n",
      "497.0\n",
      "545.5\n",
      "541.5\n",
      "482.5\n",
      "0\n",
      "601.0\n",
      "527.0\n",
      "490.5\n",
      "0\n",
      "0\n",
      "538.0\n",
      "629.0\n",
      "602.5\n",
      "536.5\n",
      "0\n",
      "driver reward  7347.5\n",
      "0\n",
      "61.0\n",
      "69.0\n",
      "33.5\n",
      "34.0\n",
      "0\n",
      "37.0\n",
      "114.0\n",
      "0\n",
      "85.5\n",
      "95.0\n",
      "0\n",
      "0\n",
      "146.0\n",
      "23.0\n",
      "0\n",
      "30.5\n",
      "0\n",
      "-4.5\n",
      "20.0\n",
      "0\n",
      "60.5\n",
      "35.5\n",
      "74.5\n",
      "32.5\n",
      "96.0\n",
      "0\n",
      "32.0\n",
      "4.0\n",
      "84.0\n",
      "-10.5\n",
      "5.5\n",
      "5.0\n",
      "97.5\n",
      "-22.5\n",
      "85.0\n",
      "driver reward  1323.0\n",
      "45.5\n",
      "57.0\n",
      "0\n",
      "0\n",
      "55.5\n",
      "0\n",
      "41.0\n",
      "147.0\n",
      "0\n",
      "0\n",
      "0\n",
      "13.0\n",
      "64.0\n",
      "83.5\n",
      "61.5\n",
      "159.5\n",
      "135.5\n",
      "45.0\n",
      "82.5\n",
      "0\n",
      "12.5\n",
      "0\n",
      "0\n",
      "22.0\n",
      "6.5\n",
      "93.0\n",
      "0\n",
      "101.0\n",
      "0\n",
      "0\n",
      "92.0\n",
      "-1.5\n",
      "55.5\n",
      "25.5\n",
      "0\n",
      "64.0\n",
      "41.0\n",
      "47.0\n",
      "26.0\n",
      "0\n",
      "548.0\n",
      "574.5\n",
      "525.5\n",
      "637.5\n",
      "driver reward  3860.5\n",
      "46.0\n",
      "40.5\n",
      "0\n",
      "46.0\n",
      "146.0\n",
      "0\n",
      "0\n",
      "103.5\n",
      "28.5\n",
      "0\n",
      "74.5\n",
      "0\n",
      "-19.0\n",
      "0\n",
      "41.5\n",
      "2.0\n",
      "-4.0\n",
      "78.0\n",
      "0\n",
      "0\n",
      "0\n",
      "54.5\n",
      "31.5\n",
      "38.5\n",
      "0\n",
      "0\n",
      "-4.5\n",
      "51.5\n",
      "55.5\n",
      "43.0\n",
      "93.0\n",
      "0.0\n",
      "87.0\n",
      "100.5\n",
      "0\n",
      "17.0\n",
      "88.5\n",
      "56.0\n",
      "34.5\n",
      "0\n",
      "550.5\n",
      "599.0\n",
      "484.0\n",
      "0\n",
      "driver reward  2963.5\n",
      "46.5\n",
      "62.0\n",
      "0\n",
      "39.5\n",
      "0\n",
      "-6.0\n",
      "148.0\n",
      "0\n",
      "17.5\n",
      "13.0\n",
      "0\n",
      "95.5\n",
      "80.5\n",
      "0\n",
      "73.5\n",
      "0\n",
      "113.0\n",
      "124.5\n",
      "0\n",
      "21.5\n",
      "0\n",
      "-1.5\n",
      "0\n",
      "-8.0\n",
      "8.0\n",
      "73.0\n",
      "-10.5\n",
      "43.0\n",
      "78.0\n",
      "0\n",
      "27.5\n",
      "43.5\n",
      "11.5\n",
      "-5.5\n",
      "70.0\n",
      "-4.0\n",
      "0\n",
      "111.0\n",
      "driver reward  1265.0\n",
      "49.5\n",
      "0\n",
      "31.0\n",
      "-13.5\n",
      "151.0\n",
      "87.5\n",
      "133.5\n",
      "73.0\n",
      "-12.0\n",
      "24.0\n",
      "11.5\n",
      "0\n",
      "-8.0\n",
      "0\n",
      "9.0\n",
      "165.0\n",
      "144.0\n",
      "45.5\n",
      "0\n",
      "67.5\n",
      "46.0\n",
      "15.5\n",
      "0\n",
      "154.5\n",
      "72.0\n",
      "47.5\n",
      "53.0\n",
      "-16.0\n",
      "16.5\n",
      "0\n",
      "0\n",
      "15.0\n",
      "8.0\n",
      "-6.0\n",
      "23.5\n",
      "-3.5\n",
      "116.5\n",
      "0\n",
      "100.5\n",
      "0\n",
      "driver reward  1601.5\n",
      "total reward  49569.5\n"
     ]
    }
   ],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreward results - \\nrandom policy - around 9.5k\\nlearned policy - 14k\\nalways accept policy - 19.4k\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fa715307c8a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
