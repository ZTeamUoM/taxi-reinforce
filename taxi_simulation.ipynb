{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "from random import sample \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import categorical_q_network\n",
    "\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import policy_step\n",
    "\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation\n",
    "from RideSimulator import reward_parameters as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 10 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action\n",
    "\n",
    "epsilon_greedy = 0.01 #should be low for imitation learning, higher for regular q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter,\n",
    "    epsilon_greedy = epsilon_greedy)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "saver = policy_saver.PolicySaver(eval_policy, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#catagorical dqn agent\n",
    "gamma = 0.99\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=n_step_update+1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation([acceptPol])\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#policy which accepts all positive reward trips (for evaluation purposes)\n",
    "#this policy looks at pickup distance & trip distance and calculates trip reward\n",
    "class AcceptPositiveTripsPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    observations = obs.observation.numpy()[0]\n",
    "    trip_reward = (observations[1] * rp.unit_reward) - ((observations[0] + observations[1]) * rp.per_km_cost)\n",
    "    #print(trip_reward)\n",
    "    if (trip_reward >= 0):\n",
    "        return (tf.constant([1]))\n",
    "    else:\n",
    "        return (tf.constant([0]))\n",
    "\n",
    "accpt_positive_trips_policy = AcceptPositiveTripsPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    hrly_accepted_trips = np.zeros(24)\n",
    "    hrly_trip_counts = np.zeros(24)\n",
    "    hrly_acceptance_rates = []\n",
    "    for state_list in eval_env[0]:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            print(int(np.array([state_list[i][\"observation\"]])[0][1]))\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "                print(np.array([state_list[i][\"observation\"]]))\n",
    "                hrly_accepted_trips[int(np.array([state_list[i][\"observation\"]])[0][1])] +=1\n",
    "            else:\n",
    "                reward = 0\n",
    "            #print (reward)\n",
    "            driver_reward += reward\n",
    "            hrly_trip_counts[int(np.array([state_list[i][\"observation\"]])[0][1])] +=1\n",
    "            \n",
    "            \n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "    \n",
    "    #find average acceptance for each hour\n",
    "    print(\"trips\", hrly_trip_counts )\n",
    "    for j in range(24):\n",
    "        hrly_acceptance_rates.append(hrly_accepted_trips[j]/hrly_trip_counts[j])\n",
    "    print (hrly_acceptance_rates)\n",
    "\n",
    "evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "\n",
    "pickup_distance_brackets = [1, 2, 3, 4, 6, 10]\n",
    "trip_distance_brackets = [5, 10, 15, 25, 35, 50]\n",
    "    \n",
    "#categorize distance\n",
    "def sortDistance(dist, distance_brackets):\n",
    "    if dist > distance_brackets[-1]:\n",
    "        return len(distance_brackets)\n",
    "    for i in range(len(distance_brackets)):\n",
    "        if dist <= distance_brackets[i]:\n",
    "            return i\n",
    "    \n",
    "#calculate acceptance rates based on distances\n",
    "def evaluatePolicyDistances(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    \n",
    "    pickup_accepted_trips = np.zeros(len(pickup_distance_brackets)+1)\n",
    "    pickup_trip_counts = np.zeros(len(pickup_distance_brackets)+1)\n",
    "    pickup_acceptance_rates = []\n",
    "    \n",
    "    for state_list in eval_env[0]:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "                #print(np.array([state_list[i][\"observation\"]]))\n",
    "                pickup_accepted_trips[sortDistance(float(np.array([state_list[i][\"observation\"]])[0][0]), pickup_distance_brackets)] +=1\n",
    "            else:\n",
    "                reward = 0\n",
    "            #print (reward)\n",
    "            driver_reward += reward\n",
    "            pickup_trip_counts[sortDistance(float(np.array([state_list[i][\"observation\"]])[0][0]), pickup_distance_brackets)] +=1\n",
    "            \n",
    "            \n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "    \n",
    "    #find average acceptance for each hour\n",
    "    print(\"trips\", pickup_trip_counts )\n",
    "    for j in range(len(pickup_distance_brackets)+1):\n",
    "        pickup_acceptance_rates.append(float(pickup_accepted_trips[j])/float(pickup_trip_counts[j]))\n",
    "    print (pickup_accepted_trips)\n",
    "    print(pickup_acceptance_rates)\n",
    "evaluatePolicyDistances(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatePolicy(accpt_positive_trips_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average returnstep\n",
    "def compute_avg_return(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation([policy])\n",
    "        episode_reward = 0\n",
    "        for state_list in state_lists[0]:\n",
    "            states = []\n",
    "            driver_reward = 0\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                driver_reward += state_tf.reward\n",
    "            episode_reward += driver_reward\n",
    "        \n",
    "        #take average reward for all drivers in the episode\n",
    "        episode_reward = episode_reward / len(state_lists)\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_return = total_reward / num_episodes\n",
    "    print(avg_return)\n",
    "    return avg_return.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories\n",
    "\n",
    "def collect_data(num_iterations, policy, replay_buffer):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation([policy])\n",
    "        print(\"driver count : \", len(state_lists[0]))\n",
    "        for state_list in state_lists[0]:\n",
    "            states = []\n",
    "            actions = []\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                #create time step\n",
    "                if i == 0:\n",
    "                    #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                    state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                    #print (state_tf)\n",
    "                elif i < (len(state_list) - 1):\n",
    "                    #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                else:\n",
    "                    state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                #create action\n",
    "                \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                    action = tf.constant([1], dtype=tf.int32)\n",
    "                else:\n",
    "                    action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                action = state_list[i][\"action\"]\n",
    "                #print\n",
    "                #print (\"action\", state_list[i][\"action\"])\n",
    "                #print(\"obs\", state_list[i][\"observation\"])\n",
    "                states.append(state_tf)\n",
    "                actions.append(action)\n",
    "\n",
    "            for j in range(len(states)-1):\n",
    "                present_state = states[j]\n",
    "                next_state = states[j+1]\n",
    "                action = actions[j]\n",
    "                traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                #print(action)\n",
    "                # Add trajectory to the replay buffer\n",
    "                replay_buffer.add_batch(traj)\n",
    "                #print(traj)\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imitation learning strategy\n",
    "#parse pickme data into trajectories\n",
    "#pickup distance ignored since it is nto available in dataset\n",
    "#load data\n",
    "import pandas as pd \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "driver_actions =  pd.read_csv(\"data/driver-action.csv\") \n",
    "trip_data =  pd.read_csv(\"data/trip-data.csv\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver_actions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_behaviours = []\n",
    "\n",
    "#trajectory format - trip distance, time of day, drop location_long, drop_location_lat, (trips_till_weekly_reward)\n",
    "\n",
    "#MANUAL and SYSTEM_REJECTED count as rejected, null counts as acccepted\n",
    "\n",
    "bool_accepted_series = pd.notnull(driver_actions[\"accepted_driver_id\"])  \n",
    "accepted_driver_actions = driver_actions[bool_accepted_series]\n",
    "bool_rejected_series = pd.isnull(driver_actions[\"accepted_driver_id\"])\n",
    "rejected_driver_actions = driver_actions[bool_rejected_series]\n",
    "print(\"total\", len(driver_actions.index))\n",
    "print(\"accepted\", len(accepted_driver_actions.index))\n",
    "print(\"rejected\", len(rejected_driver_actions.index))\n",
    "print(\"trips\", len(trip_data.index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO distance scale\n",
    "# for rides without distance, estimate distance based on l2 distance and scaling \n",
    "# factor calculated from ratio between l2 distance and real distance\n",
    "scaling_factor = 1.3\n",
    "\n",
    "#get l2 distance (assume that locations are close to equator)\n",
    "def l2_dist(x1, x2, y1, y2):\n",
    "    return math.sqrt((x2-x1)*(x2-x1) + (y2-y1)*(y2-y1))*111 #mult by 111 to turn cordinates to km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup drop location, trip distance, time from trip_data\n",
    "def get_trip_data(trips, trip_id):\n",
    "    trip = trips.loc[trips['passengerslogid'] == trip_id]\n",
    "    #if exactly one record is found\n",
    "    if (len(trip.index)) ==1:\n",
    "        #trip = trip[\"droplatitude\"]\n",
    "        for tr in trip.iterrows():\n",
    "            lat  = tr[1][\"pickuplatitude\"]\n",
    "            long  = tr[1][\"pickuplongitude\"]\n",
    "            time  = pd.to_datetime(tr[1][\"createddate\"])\n",
    "            \n",
    "            #check if distance is available\n",
    "            if (math.isnan(tr[1][\"distance\"])):\n",
    "                distance = scaling_factor * l2_dist(tr[1][\"pickuplatitude\"],\n",
    "                                                   tr[1][\"droplatitude\"],\n",
    "                                                   tr[1][\"pickuplongitude\"],\n",
    "                                                   tr[1][\"droplongitude\"])                \n",
    "            else:\n",
    "\n",
    "                distance  = tr[1][\"distance\"]\n",
    "            return {\n",
    "                \"drop_lat\": lat,\n",
    "                \"drop_long\": long,\n",
    "                \"time\": time,\n",
    "                \"trip_dist\": distance\n",
    "            }\n",
    "    else:\n",
    "        return None\n",
    "    #print(trip)\n",
    "\n",
    "\n",
    "get_trip_data(trip_data, 212554705)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - lookup trips_till_weekly_reward from achievements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group behaviour by drivers, sorted by time\n",
    "#rejected_driver_actions['rejected_driver_id'].nunique()\n",
    "#create dictionary with entry for each driver. the entry is a list of actions\n",
    "driver_actions ={}\n",
    "count = 0\n",
    "#loop through accepted trips\n",
    "for id, row in tqdm(accepted_driver_actions.iterrows()):\n",
    "    count+=1\n",
    "    if count == 10000: #temporary limit for testing\n",
    "        break\n",
    "        \n",
    "    #print(row[\"trip_id\"])\n",
    "    action = {\"accept\": 1, \"observation\": get_trip_data(trip_data, row[\"trip_id\"])}\n",
    "    key = str(int(row[\"accepted_driver_id\"]))\n",
    "\n",
    "    #add to driver actions table\n",
    "    if key not in driver_actions:\n",
    "        driver_actions[key] = [action]\n",
    "    else:\n",
    "        driver_actions[key].append(action)\n",
    "        \n",
    "#loop through rejected trips\n",
    "count = 0\n",
    "\n",
    "for id, row in tqdm(rejected_driver_actions.iterrows()):\n",
    "    count+=1\n",
    "    if count == 10000:\n",
    "        break\n",
    "        \n",
    "    #print(row[\"trip_id\"])\n",
    "    action = {\"accept\": 0, \"observation\": get_trip_data(trip_data, row[\"trip_id\"])}\n",
    "    if (math.isnan(row[\"rejected_driver_id\"])):\n",
    "        continue\n",
    "    key = str(int(row[\"rejected_driver_id\"]))\n",
    "    if key not in driver_actions:\n",
    "        driver_actions[key] = [action]\n",
    "    else:\n",
    "        driver_actions[key].append(action)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort action in order (based on time)\n",
    "\n",
    "#remove invalid values\n",
    "for driver in driver_actions:\n",
    "    refined_actions = []\n",
    "    for action in driver_actions[driver]:\n",
    "        try:\n",
    "            time = action['observation'][\"time\"].value\n",
    "            refined_actions.append(action)\n",
    "        except:\n",
    "            continue\n",
    "    driver_actions[driver] = refined_actions\n",
    "    \n",
    "for driver in driver_actions:\n",
    "        driver_actions[driver] = sorted(driver_actions[driver], key = lambda i: i['observation'][\"time\"].value)\n",
    "\n",
    "#switch time to hour of day\n",
    "for driver in driver_actions:\n",
    "    for action in driver_actions[driver]:\n",
    "        action[\"observation\"][\"time\"] = action[\"observation\"][\"time\"].hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver_actions[\"66075\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create trajectory collection from pickme trajectory set\n",
    "#reward is set to 0 because it is  not available in the dataset and at this stage the agent does not explore\n",
    "\n",
    "driver_trajectories = []\n",
    "\n",
    "#loop through drivers\n",
    "for key in driver_actions:\n",
    "    #driver must have at least 2 rides to create a trajectory\n",
    "    if len(driver_actions[key]) > 1:\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        #convert states directly to tf timesteps\n",
    "        for i in range(len(driver_actions[key])):\n",
    "            #create time step\n",
    "            obs = driver_actions[key][i][\"observation\"]\n",
    "            obs_list = [obs[\"trip_dist\"], obs[\"drop_lat\"], obs[\"drop_long\"], obs[\"time\"]]\n",
    "            if i == 0:\n",
    "                #initial trajectory\n",
    "                state_tf = ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "            elif i < (len(driver_actions[key]) - 1):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "            else:\n",
    "                #terminating tranjectory\n",
    "                state_tf = ts.TimeStep(tf.constant([2]), tf.constant([0.0]), tf.constant([0.0]), tf.convert_to_tensor(np.array([obs_list], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "            #create action\n",
    "            #action = state_list[i][\"action\"]\n",
    "            action = policy_step.PolicyStep(tf.constant([driver_actions[key][i][\"accept\"]], dtype=tf.int64), ())\n",
    "            #print (action)\n",
    "            states.append(state_tf)\n",
    "            actions.append(action)\n",
    "\n",
    "        for j in range(len(states)-1):\n",
    "            present_state = states[j]\n",
    "            #print(present_state)\n",
    "            next_state = states[j+1]\n",
    "            action = actions[j]\n",
    "\n",
    "            traj = trajectory.from_transition(present_state, action, next_state)\n",
    "            #print(action)\n",
    "            # Add trajectory to the replay buffer\n",
    "            driver_trajectories.append(traj)\n",
    "            #replay_buffer.add_batch(traj)\n",
    "            #print(traj)\n",
    "\n",
    "print(len(driver_trajectories))\n",
    "#cache trajectories as json to disk?\n",
    "\n",
    "#convert trajectories to tf agents format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add subset of trajectorise to replay buffer\n",
    "def collect_data_imitation(num_trajectories, replay_buffer, driver_trajectories):\n",
    "    #sample from trajectories\n",
    "    sample_trajectories = sample(driver_trajectories, num_trajectories)\n",
    "    for traj in sample_trajectories:\n",
    "        replay_buffer.add_batch(traj)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trajectories = 1000\n",
    "#collect_data_imitation(num_trajectories, replay_buffer, driver_trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model training - WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print(' Average Return = {0}'.format( avg_return))\n",
    "#returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        \n",
    "        #imitation learning\n",
    "        collect_data_imitation(num_trajectories, replay_buffer, driver_trajectories)\n",
    "        \n",
    "        #regular q learning\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "        \n",
    "        #evaluation\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "            saver.save('policdriver_trajectoriesy_%d' % step)\n",
    "        \n",
    "        \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# add trips till weekly reward observation\n",
    "# add evaluation\n",
    "# mix with regular q learning\n",
    "# visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non imitation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print(' Average Return = {0}'.format( avg_return))\n",
    "returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, state=(), info=())tf.constant(0, dtype=tf.int32)\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "            saver.save('policy_%d' % step)\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test against data from pickme dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "week_6 = pd.read_csv(\"Eval_data.csv\")\n",
    "tot = 0\n",
    "tot_accept = 0\n",
    "dataset_accept = 0\n",
    "num = 10000\n",
    "\n",
    "pickup_accepted_trips = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_accepted_trips_agent = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_trip_counts = np.zeros(len(pickup_distance_brackets)+1)\n",
    "pickup_acceptance_rates = []\n",
    "pickup_acceptance_rates_agent = []\n",
    "\n",
    "trip_accepted_trips = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_accepted_trips_agent = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_counts = np.zeros(len(trip_distance_brackets)+1)\n",
    "trip_acceptance_rates = []\n",
    "trip_acceptance_rates_agent = []\n",
    "    \n",
    "for i in range(num):\n",
    "    #load relevant fields from dataset\n",
    "    data_point = week_6.iloc[i][['distance_to_pickup','trip_distance','day_time','accepted_trip_count','drop_latitude', 'drop_longitude', 'action']].tolist()\n",
    "    #observation_ts = ts.transition(np.array(data_point[:-1], dtype=np.float32), reward=0.0, discount=1.0)\n",
    "    #print(np.array(data_point[:-1],dtype=np.float32))\n",
    "    \n",
    "    \n",
    "    #group by pickup distances, trip distances\n",
    "    pickup_trip_counts[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "    trip_counts[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "    \n",
    "    #scale drop location\n",
    "    data_point[-3] = (data_point[-3] - 6.8) * 40\n",
    "    data_point[-2] = (data_point[-2] - 79.85) * 40\n",
    "    \n",
    "    observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([data_point[:-1]], dtype=np.float32), dtype=tf.float32))\n",
    "    policy_step = eval_policy.action(observation_ts)\n",
    "    policy_state = policy_step.state\n",
    "    #print(policy_step.action.numpy()[0])\n",
    "    if policy_step.action.numpy()[0] == 1:\n",
    "        tot_accept += 1\n",
    "        pickup_accepted_trips_agent[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "        trip_accepted_trips_agent[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "        \n",
    "    if data_point[-1] == 1:\n",
    "        dataset_accept += 1\n",
    "        pickup_accepted_trips[sortDistance(data_point[0], pickup_distance_brackets)] += 1\n",
    "        trip_accepted_trips[sortDistance(data_point[1], trip_distance_brackets)] += 1\n",
    "        \n",
    "    if policy_step.action.numpy()[0] == data_point[-1]:\n",
    "        tot += 1\n",
    "\n",
    "for j in range(len(pickup_distance_brackets)+1):\n",
    "    pickup_acceptance_rates.append(float(pickup_accepted_trips[j])/float(pickup_trip_counts[j]))\n",
    "    pickup_acceptance_rates_agent.append(float(pickup_accepted_trips_agent[j])/float(pickup_trip_counts[j]))\n",
    "print (\"pickup distribution\", pickup_accepted_trips)\n",
    "print(\"pikcup acceptance rates\", pickup_acceptance_rates)\n",
    "print(\"pikcup acceptance rates_agent\", pickup_acceptance_rates_agent)\n",
    "\n",
    "for k in range(len(trip_distance_brackets)+1):\n",
    "    trip_acceptance_rates.append(float(trip_accepted_trips[k])/float(trip_counts[k]))\n",
    "    trip_acceptance_rates_agent.append(float(trip_accepted_trips_agent[k])/float(trip_counts[k]))\n",
    "print (\"trip distribution\", trip_counts)\n",
    "print(\"trip acceptance rates\", trip_acceptance_rates)\n",
    "print(\"trip acceptance rates_agent\", trip_acceptance_rates_agent)\n",
    "    \n",
    "print(f'Accuracy: {tot/num * 100}%')\n",
    "print(f'accept freq: {tot_accept/num * 100}%')\n",
    "print(f'dataset accept freq: {dataset_accept/num * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw graphs\n",
    "\"\"\"\n",
    "plt.plot([0]+pickup_distance_brackets, trip_acceptance_rates)\n",
    "plt.xlabel('pickup distance (km)')\n",
    "plt.ylabel('acceptance rate')\n",
    "legned()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Make some fake data.\n",
    "a = b = [0]+pickup_distance_brackets\n",
    "c = pickup_acceptance_rates\n",
    "d = pickup_acceptance_rates_agent\n",
    "\n",
    "# Create plots with pre-defined labels.\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(a, c, 'k--', label='Dataset trip acceptance rates')\n",
    "ax.plot(a, d, 'k:', label='Agent trip acceptance rates')\n",
    "#ax.plot(a, c + d, 'k', label='Total message length')\n",
    "plt.xlabel('pickup distance (km)')\n",
    "plt.ylabel('acceptance rate')\n",
    "legend = ax.legend(loc='best', shadow=True, fontsize='medium')\n",
    "\n",
    "# Put a nicer background color on the legend.\n",
    "legend.get_frame().set_facecolor('C0')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0-1', '1-2', '2-3', '3-4', '4-6', '6-10', '10+']\n",
    "men_means = [0.5270572916666666, 0.5762839879154078, 0.6209407665505226, 0.5801063022019742, 0.5761851015801355, 0.5835427135678392, 0.595703125]\n",
    "women_means = [0.51046875, 0.5805740181268882, 0.6181881533101046, 0.5536142748671223, 0.5333182844243793, 0.5501256281407035, 0.578828125]\n",
    "#men_means = pickup_acceptance_rates\n",
    "#women_means = pickup_acceptance_rates_agent\n",
    "\n",
    "labels1 = ['0-5', '5-10', '10-15', '15-25', '25-35', '50-35', '50+']\n",
    "men_means1 = [0.5959821428571429, 0.5641864268192968, 0.6, 0.5790166812993854, 0.5739299610894941, 0.5859598853868195, 0.5761124121779859]\n",
    "women_means1 = [0.37857142857142856, 0.4463614063777596, 0.51423487544483983, 0.546268656716418, 0.5578469520103762, 0.5914040114613181, 0.6959016393442623]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "rects1 = ax[0].bar(x - width/2, men_means, width, label='Dataset acceptance rates')\n",
    "rects2 = ax[0].bar(x + width/2, women_means, width, label='Agent acceptance rates')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax[0].set_ylabel('acceptance rate', fontsize=20)\n",
    "ax[0].set_xlabel('pickup distance (km)', fontsize=20)\n",
    "ax[0].set_title('acceptance rate variation with pickup distance', fontsize=20)\n",
    "ax[0].set_xticks(x)\n",
    "ax[0].set_xticklabels(labels)\n",
    "#ax[0].legend(fontsize='medium')\n",
    "\n",
    "rects1 = ax[1].bar(x - width/2, men_means1, width, label='Dataset acceptance rates')\n",
    "rects2 = ax[1].bar(x + width/2, women_means1, width, label='Agent acceptance rates')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax[1].set_ylabel('acceptance rate', fontsize=20)\n",
    "ax[1].set_xlabel('trip distance (km)', fontsize=20)\n",
    "ax[1].set_title('acceptance rate variation with trip distance', fontsize=20)\n",
    "ax[1].set_xticks(x)\n",
    "ax[1].set_xticklabels(labels1, fontsize=16)\n",
    "handles, labels = ax[1].get_legend_handles_labels()    \n",
    "ax[1].legend(handles, labels, loc=\"center\", bbox_to_anchor=(-0.15,-0.2),prop={'size': 18},ncol=2)\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "#autolabel(rects1)\n",
    "#autolabel(rects2)\n",
    "\n",
    "#fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('file', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize progress\n",
    "iterations = range(0, num_iterations +1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluateSavedPolicy(policy, policy_state, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf, policy_state)\n",
    "\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved policy\n",
    "saved_policy = tf.compat.v2.saved_model.load('pol/policy_10')\n",
    "policy_state = saved_policy.get_initial_state(batch_size=3)\n",
    "\"\"\"time_step = ...\n",
    "while True:\n",
    "  policy_step = saved_policy.action(time_step, policy_state)\n",
    "  policy_state = policy_step.state\n",
    "  time_step = f(policy_step.action)\n",
    "\"\"\"\n",
    "observations = [8, 10, 0, 35]\n",
    "#observation_ts = ts.transition(np.array(observations, dtype=np.float32), reward=0.0, discount=1.0)\n",
    "observation_ts = ts.TimeStep(tf.constant([1]), tf.constant([0.0]), tf.constant([1.0]),\n",
    "                                tf.convert_to_tensor(np.array([observations], dtype=np.float32), dtype=tf.float32))\n",
    "action = saved_policy.action(observation_ts, policy_state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(saved_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy#evaluateSavedPolicy(saved_policy, policy_state, eval_env)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "\n",
    "#img = np.load(\"1600332703880_npimg.npy\", allow_pickle=True)\n",
    "img = np.load(\"1600371385971_npimg.npy\", encoding = 'bytes')\n",
    "img = img[:, 1:]\n",
    "fig = plt.figure()\n",
    "print(img)\n",
    "print(img.shape)\n",
    "\n",
    "print(np.nanmin(img[img != -np.inf]))\n",
    "print(np.nanmax(img[img != np.inf]))\n",
    "\n",
    "zmax = np.nanmin(img[img != -np.inf])\n",
    "zmin = np.nanmax(img[img != np.inf])\n",
    "print(img.max())\n",
    "print(img.min())\n",
    "\"\"\"\n",
    "maxz= 298.6142120361328/2\n",
    "minz= 298.4643096923828/2\n",
    "maxx= 2.2676000595092773\n",
    "minx= -2.5011000633239746\n",
    "maxy= 3.3237826631069183\n",
    "miny= 0.47209998965263367\n",
    "\"\"\"\n",
    "maxx= -18.123884002685546\n",
    "maxy= 58.166160583496094\n",
    "maxz= 5.541902542114258\n",
    "minx= -47.775150299072266\n",
    "miny= 2.8466339111328125\n",
    "minz= 5.391929626464844\n",
    "\n",
    "print(\"conv\", maxz/3.28084, minz/3.28084)\n",
    "ax = fig.add_axes([.57,.05,.9,.8])\n",
    "ax.set_xlabel(\"X position (Feet)\")\n",
    "ax.set_ylabel(\"Y position (Feet)\")\n",
    "\n",
    "img_plotted = ax.imshow(\n",
    "        img,\n",
    "        vmin=minz,\n",
    "        vmax=maxz,\n",
    "        extent=(minx, maxx, miny, maxy),\n",
    "        cmap = 'jet'\n",
    "    )\n",
    "\n",
    "fig.colorbar(img_plotted).set_label(\"Deviation (Inches)\")\n",
    "\n",
    "plt.show()\n",
    "nanRem = img[~np.isnan(img)]\n",
    "np.histogram(nanRem[nanRem != np.inf])\n",
    "\n",
    "print(np.where(img==147.25175))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import EventCollection\n",
    "import numpy as np\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# create random data\n",
    "xdata = np.random.random([2, 10])\n",
    "\n",
    "# split the data into two parts\n",
    "xdata1 = xdata[0, :]\n",
    "xdata2 = xdata[1, :]\n",
    "\n",
    "# sort the data so it makes clean curves\n",
    "xdata1.sort()\n",
    "xdata2.sort()\n",
    "\n",
    "# create some y data points\n",
    "ydata1 = xdata1 ** 2\n",
    "ydata2 = 1 - xdata2 ** 3\n",
    "\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(xdata1, ydata1, color='tab:blue')\n",
    "ax.plot(xdata2, ydata2, color='tab:orange')\n",
    "\n",
    "# create the events marking the x data points\n",
    "xevents1 = EventCollection(xdata1, color='tab:blue', linelength=0.05)\n",
    "xevents2 = EventCollection(xdata2, color='tab:orange', linelength=0.05)\n",
    "\n",
    "# create the events marking the y data points\n",
    "yevents1 = EventCollection(ydata1, color='tab:blue', linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "yevents2 = EventCollection(ydata2, color='tab:orange', linelength=0.05,\n",
    "                           orientation='vertical')\n",
    "\n",
    "# add the events to the axis\n",
    "ax.add_collection(xevents1)\n",
    "ax.add_collection(xevents2)\n",
    "ax.add_collection(yevents1)\n",
    "ax.add_collection(yevents2)\n",
    "\n",
    "# set the limits\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "ax.set_title('line plot with data points')\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
