{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 20 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 5  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "eval_policy_list = [eval_policy]\n",
    "collect_policy_list = [collect_policy]\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n",
    "replay_buffer_list = [replay_buffer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x150555650>\n"
     ]
    }
   ],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset_list = []\n",
    "iterator_list = []\n",
    "\n",
    "for replay_buffer in replay_buffer_list:\n",
    "    dataset = replay_buffer.as_dataset(\n",
    "        num_parallel_calls=3, \n",
    "        sample_batch_size=batch_size, \n",
    "        num_steps=2).prefetch(3)\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "    iterator = iter(dataset)\n",
    "    iterator_list.append(iterator)\n",
    "    \n",
    "    print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npolicy.action(reset)\\n#tf_env.time_step_spec()\\nprint(reset)\\n#print(env.reset())\\n#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\\nprint(\" \")\\nprint(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\\n\\n#print(tensor_spec.to_array_spec(reset))\\n#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\\n#encoder_func(env.reset())\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "Number of trips generated: 2237\n"
     ]
    }
   ],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation([acceptPol])\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "\n",
    "#evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average return\n",
    "def compute_avg_return(policy_list, num_episodes=10):\n",
    "    total_rewards = defaultdict(int)\n",
    "    avg_returns = defaultdict(int)\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        episode_rewards = defaultdict(int)\n",
    "        policy_state_lists_dict = run_simulation(policy_list)\n",
    "        for policy, state_lists in policy_state_lists_dict.items():\n",
    "            for state_list in state_lists:\n",
    "                states = []\n",
    "                driver_reward = 0\n",
    "\n",
    "                #convert states directly to tf timesteps\n",
    "                for i in range(len(state_list)):\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    driver_reward += state_tf.reward\n",
    "                episode_rewards[policy] += driver_reward\n",
    "                \n",
    "            #take average reward for all drivers in the episode\n",
    "            episode_rewards[policy] = episode_rewards[policy] / len(state_lists)\n",
    "            total_rewards[policy] += episode_rewards[policy]\n",
    "\n",
    "    for policy in policy_state_lists_dict:\n",
    "        avg_returns[policy] = total_rewards[policy] / num_episodes\n",
    "        avg_returns[policy] = avg_returns[policy].numpy()\n",
    "    print(avg_returns)\n",
    "    return avg_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories\n",
    "\n",
    "def collect_data(num_iterations, policy_list, replay_buffer_list):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        policy_state_lists_dict = run_simulation(policy_list)\n",
    "        for policy, state_lists in policy_state_lists_dict.items():\n",
    "            print(\"driver count : \", len(state_lists))\n",
    "            for state_list in state_lists:\n",
    "                states = []\n",
    "                actions = []\n",
    "\n",
    "                #convert states directly to tf timesteps\n",
    "                for i in range(len(state_list)):\n",
    "                    #create time step\n",
    "                    if i == 0:\n",
    "                        #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                        state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                        #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                        #print (state_tf)\n",
    "                    elif i < (len(state_list) - 1):\n",
    "                        #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                        state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                        #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                    else:\n",
    "                        state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                    #create action\n",
    "                    \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                        action = tf.constant([1], dtype=tf.int32)\n",
    "                    else:\n",
    "                        action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                    action = state_list[i][\"action\"]\n",
    "\n",
    "                    #print (action)\n",
    "                    states.append(state_tf)\n",
    "                    actions.append(action)\n",
    "                    \n",
    "                for j in range(len(states)-1):\n",
    "                    present_state = states[j]\n",
    "                    next_state = states[j+1]\n",
    "                    action = actions[j]\n",
    "                    traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                    #print(action)\n",
    "                    # Add trajectory to the replay buffer\n",
    "                    replay_buffer_list[policy].add_batch(traj)\n",
    "                    #print(traj)\n",
    "    \n",
    "\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips generated: 2210\n",
      "Number of trips generated: 2315\n",
      "Number of trips generated: 2239\n",
      "Number of trips generated: 2157\n",
      "Number of trips generated: 2161\n",
      "defaultdict(<class 'int'>, {0: 41304.56})\n",
      "Number of trips generated: 2291\n",
      "driver count :  20\n",
      "Number of trips generated: 2208\n",
      "driver count :  20\n",
      "Number of trips generated: 2217\n",
      "driver count :  20\n",
      "Number of trips generated: 2204\n",
      "driver count :  20\n",
      "Number of trips generated: 2212\n",
      "driver count :  20\n",
      "Number of trips generated: 2307\n",
      "Number of trips generated: 2252\n",
      "Number of trips generated: 2247\n",
      "Number of trips generated: 2209\n",
      "Number of trips generated: 2186\n",
      "defaultdict(<class 'int'>, {0: 43339.363})\n",
      "step = 5: Average Return = defaultdict(<class 'int'>, {0: 43339.363})\n",
      "evaluation\n",
      "Number of trips generated: 2145\n",
      "driver count :  20\n",
      "Number of trips generated: 2149\n",
      "driver count :  20\n",
      "Number of trips generated: 2137\n",
      "driver count :  20\n",
      "Number of trips generated: 2203\n",
      "driver count :  20\n",
      "Number of trips generated: 2160\n",
      "driver count :  20\n",
      "step = 10: loss = [<tf.Tensor: shape=(), dtype=float32, numpy=180834.38>]\n",
      "Number of trips generated: 2255\n",
      "Number of trips generated: 2238\n",
      "Number of trips generated: 2253\n",
      "Number of trips generated: 2252\n",
      "Number of trips generated: 2173\n",
      "defaultdict(<class 'int'>, {0: 43145.945})\n",
      "step = 10: Average Return = defaultdict(<class 'int'>, {0: 43145.945})\n",
      "evaluation\n",
      "Number of trips generated: 2276\n",
      "driver count :  20\n",
      "Number of trips generated: 2194\n",
      "driver count :  20\n",
      "Number of trips generated: 2266\n",
      "driver count :  20\n",
      "Number of trips generated: 2106\n",
      "driver count :  20\n",
      "Number of trips generated: 2272\n",
      "driver count :  20\n",
      "Number of trips generated: 2218\n",
      "Number of trips generated: 2196\n",
      "Number of trips generated: 2219\n",
      "Number of trips generated: 2185\n",
      "Number of trips generated: 2265\n",
      "defaultdict(<class 'int'>, {0: 42695.168})\n",
      "step = 15: Average Return = defaultdict(<class 'int'>, {0: 42695.168})\n",
      "evaluation\n",
      "Number of trips generated: 2247\n",
      "driver count :  20\n",
      "Number of trips generated: 2197\n",
      "driver count :  20\n",
      "Number of trips generated: 2271\n",
      "driver count :  20\n",
      "Number of trips generated: 2160\n",
      "driver count :  20\n",
      "Number of trips generated: 2323\n",
      "driver count :  20\n",
      "step = 20: loss = [<tf.Tensor: shape=(), dtype=float32, numpy=189685.06>]\n",
      "Number of trips generated: 2229\n",
      "Number of trips generated: 2210\n",
      "Number of trips generated: 2270\n",
      "Number of trips generated: 2247\n",
      "Number of trips generated: 2264\n",
      "defaultdict(<class 'int'>, {0: 43359.64})\n",
      "step = 20: Average Return = defaultdict(<class 'int'>, {0: 43359.64})\n",
      "evaluation\n"
     ]
    }
   ],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_returns = compute_avg_return(eval_policy_list, num_eval_episodes)\n",
    "returns = [avg_returns]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy_list, replay_buffer_list)\n",
    "        \n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        \n",
    "        train_loss_list = []\n",
    "        for iterator in iterator_list:\n",
    "            experience, unused_info = next(iterator)\n",
    "            train_loss = agent.train(experience).loss\n",
    "            train_loss_list.append(train_loss)\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss_list))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_returns = compute_avg_return(eval_policy_list, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_returns))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize progress\n",
    "for i in range(len(collect_policy_list)):\n",
    "    per_policy_returns = []\n",
    "    for avg_returns_dict in returns:\n",
    "        per_policy_returns.append(avg_returns_dict[i])\n",
    "\n",
    "    iterations = range(0, num_iterations +1, eval_interval)\n",
    "    plt.plot(iterations, per_policy_returns)\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer object at 0x15003be10>]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 55.4\n",
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
