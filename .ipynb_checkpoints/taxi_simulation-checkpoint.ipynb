{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import simpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.specs import tensor_spec\n",
    "#from env.RideSimulator.Grid import Grid\n",
    "import tf_agents\n",
    "\n",
    "\n",
    "import os,sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from RideSimulator.taxi_sim import run_simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register custom env\n",
    "import gym\n",
    "\n",
    "gym.envs.register(\n",
    "     id='taxi-v0',\n",
    "     entry_point='env.taxi:TaxiEnv',\n",
    "     max_episode_steps=1500,\n",
    "     kwargs={'state_dict':None},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "\n",
    "num_iterations = 30 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 10  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 2  # @param {type:\"integer\"}\n",
    "eval_interval = 5  # @param {type:\"integer\"}action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load taxi env\n",
    "env_name = \"taxi-v0\"\n",
    "env = suite_gym.load(env_name)\n",
    "\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "reset = tf_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent and policy\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "\n",
    "\n",
    "#random policy\n",
    "random_policy = random_tf_policy.RandomTFPolicy(tf_env.time_step_spec(),tf_env.action_spec())\n",
    "\n",
    "#agent policy\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "#replay buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f6545c73700>\n"
     ]
    }
   ],
   "source": [
    "#create dataset and iterator\n",
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npolicy.action(reset)\\n#tf_env.time_step_spec()\\nprint(reset)\\n#print(env.reset())\\n#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\\nprint(\" \")\\nprint(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\\n\\n#print(tensor_spec.to_array_spec(reset))\\n#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\\n#encoder_func(env.reset())\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "policy.action(reset)\n",
    "#tf_env.time_step_spec()\n",
    "print(reset)\n",
    "#print(env.reset())\n",
    "#print(ts.restart(tf.convert_to_tensor(np.array([0,0,0,0], dtype=np.int32), dtype=tf.float32)))\n",
    "print(\" \")\n",
    "print(ts.TimeStep(tf.constant([0]), tf.constant([0.0]), tf.constant([1.0]),tf.convert_to_tensor(np.array([[0,0,0,0]], dtype=np.int32), dtype=tf.float32)))\n",
    "\n",
    "#print(tensor_spec.to_array_spec(reset))\n",
    "#encoder_func = tf_agents.utils.example_encoding.get_example_encoder(env.reset())\n",
    "#encoder_func(env.reset())\n",
    "\"\"\"\n",
    "\n",
    "#run_simulation(policy)\n",
    "#ts.termination(np.array([1,2,3,4], dtype=np.int32), reward=0.0)\n",
    "#ts.transition(np.array([1,2,3,4], dtype=np.int32), reward=0.0, discount=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "#create a static environment for evaluation purposes\n",
    "\n",
    "#policy that always accepts\n",
    "class AcceptPolicy:\n",
    "  def __init__(self):\n",
    "    print(\"init\")\n",
    "\n",
    "  def action(self, obs):\n",
    "    return (tf.constant([1]))\n",
    "\n",
    "acceptPol = AcceptPolicy()\n",
    "\n",
    "eval_env = run_simulation(acceptPol)\n",
    "#print(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate a trained policy with respect to a pre-generated static environment\n",
    "def evaluatePolicy(policy, eval_env):\n",
    "    episode_reward = 0\n",
    "    for state_list in eval_env:\n",
    "        states = []\n",
    "        driver_reward = 0\n",
    "        \n",
    "        for i in range(len(state_list)):\n",
    "            state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "            action = policy.action(state_tf)\n",
    "            #action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "            if (action[0].numpy() == 1):\n",
    "                reward = state_list[i][\"reward\"]\n",
    "            else:\n",
    "                reward = 0\n",
    "            print (reward)\n",
    "            driver_reward += reward\n",
    "        episode_reward += driver_reward\n",
    "        print(\"driver reward \", driver_reward)\n",
    "    print(\"total reward \", episode_reward)\n",
    "\n",
    "#evaluatePolicy(acceptPol, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average return\n",
    "def compute_avg_return(policy, num_episodes=10):\n",
    "    total_reward = 0\n",
    "\n",
    "    for i in range (num_episodes):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        episode_reward = 0\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            driver_reward = 0\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                driver_reward += state_tf.reward\n",
    "            episode_reward += driver_reward\n",
    "        \n",
    "        #take average reward for all drivers in the episode\n",
    "        episode_reward = episode_reward / len(state_lists)\n",
    "        total_reward += episode_reward\n",
    "\n",
    "    avg_return = total_reward / num_episodes\n",
    "    print(avg_return)\n",
    "    return avg_return.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect trajectories\n",
    "\n",
    "def collect_data(num_iterations, policy, replay_buffer):\n",
    "    for i in range (num_iterations):\n",
    "        #run one episode of simulation and record states\n",
    "        state_lists = run_simulation(policy)\n",
    "        print(\"driver count : \", len(state_lists))\n",
    "        for state_list in state_lists:\n",
    "            states = []\n",
    "            actions = []\n",
    "\n",
    "            #convert states directly to tf timesteps\n",
    "            for i in range(len(state_list)):\n",
    "                #create time step\n",
    "                if i == 0:\n",
    "                    #state_tf = ts.restart(np.array(state_list[i][\"observation\"], dtype=np.float32))\n",
    "                    state_tf = ts.TimeStep(tf.constant([0]), tf.constant([3.0]), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #print(\"first reward \", state_list[i][\"reward\"])\n",
    "                    #print (state_tf)\n",
    "                elif i < (len(state_list) - 1):\n",
    "                    #reward is taken fro (i-1) because it should be the reward from the already completed action (prev. action)\n",
    "                    state_tf = ts.TimeStep(tf.constant([1]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([1.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "                    #state_tf = ts.termination(np.array(state_list[i][\"observation\"], dtype=np.float32), reward=state_list[i][\"reward\"])\n",
    "                else:\n",
    "                    state_tf = ts.TimeStep(tf.constant([2]), tf.constant(state_list[i-1][\"reward\"], dtype=tf.float32), tf.constant([0.0]), tf.convert_to_tensor(np.array([state_list[i][\"observation\"]], dtype=np.float32), dtype=tf.float32))\n",
    "\n",
    "                #create action\n",
    "                \"\"\"if state_list[i][\"action\"] == 1:\n",
    "                    action = tf.constant([1], dtype=tf.int32)\n",
    "                else:\n",
    "                    action = tf.constant([0], dtype=tf.int32)\"\"\"\n",
    "                action = state_list[i][\"action\"]\n",
    "\n",
    "                #print (action)\n",
    "                states.append(state_tf)\n",
    "                actions.append(action)\n",
    "\n",
    "            for j in range(len(states)-1):\n",
    "                present_state = states[j]\n",
    "                next_state = states[j+1]\n",
    "                action = actions[j]\n",
    "                traj = trajectory.from_transition(present_state, action, next_state)\n",
    "                #print(action)\n",
    "                # Add trajectory to the replay buffer\n",
    "                replay_buffer.add_batch(traj)\n",
    "                #print(traj)\n",
    "\n",
    "        \"\"\"\n",
    "        #re-register environemnt with new states\n",
    "        env_name = 'taxi-v'+str(i)\n",
    "        gym.envs.register(\n",
    "             id=env_name,\n",
    "             entry_point='env.taxi:TaxiEnv',\n",
    "             max_episode_steps=1500,\n",
    "             kwargs={'state_dict':state_list},\n",
    "        )\n",
    "\n",
    "        #reload new env\n",
    "        env = suite_gym.load(env_name)\n",
    "        tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        #reset tf env\n",
    "        time_step = tf_env.reset()\n",
    "\n",
    "        #loop through recorded steps\n",
    "        for step in state_dict:\n",
    "            present_state = tf_env.current_time_step()\n",
    "            action = step.action\n",
    "            new_state = tf_env.step(action)\n",
    "            traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "            replay_buffer.add_batch(traj)\n",
    "        \"\"\"\n",
    "        #print(replay_buffer)\n",
    "#collect_data(num_iterations, policy, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1527.625, shape=(), dtype=float32)\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1466.7251, shape=(), dtype=float32)\n",
      "step = 5: Average Return = 1466.72509765625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 10: loss = 17653.826171875\n",
      "tf.Tensor(1619.4, shape=(), dtype=float32)\n",
      "step = 10: Average Return = 1619.4000244140625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(1407.175, shape=(), dtype=float32)\n",
      "step = 15: Average Return = 1407.175048828125\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 20: loss = 4124.14306640625\n",
      "tf.Tensor(742.275, shape=(), dtype=float32)\n",
      "step = 20: Average Return = 742.2750244140625\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "tf.Tensor(704.05005, shape=(), dtype=float32)\n",
      "step = 25: Average Return = 704.050048828125\n",
      "evaluation\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "driver count :  20\n",
      "step = 30: loss = 6186.63330078125\n",
      "tf.Tensor(715.6, shape=(), dtype=float32)\n",
      "step = 30: Average Return = 715.5999755859375\n",
      "evaluation\n"
     ]
    }
   ],
   "source": [
    "#train agents\n",
    "\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "returns = [avg_return]\n",
    "lost_iterations = 0\n",
    "for _ in range(num_iterations):\n",
    "    try:\n",
    "        # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "        collect_data(collect_steps_per_iteration, collect_policy, replay_buffer)\n",
    "\n",
    "        # Sample a batch of data from the buffer and update the agent's network.\n",
    "        experience, unused_info = next(iterator)\n",
    "        train_loss = agent.train(experience).loss\n",
    "\n",
    "        step = agent.train_step_counter.numpy()\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            avg_return = compute_avg_return(eval_policy, num_eval_episodes)\n",
    "            print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "            returns.append(avg_return)\n",
    "            print(\"evaluation\")\n",
    "    \n",
    "    except IndexError:\n",
    "        lost_iterations += 1\n",
    "        print(\"skipping iteration due to driver error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize progress\n",
    "iterations = range(0, num_iterations +1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "75\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "80\n",
      "0\n",
      "60\n",
      "0\n",
      "0\n",
      "80\n",
      "65\n",
      "30\n",
      "0\n",
      "0\n",
      "109\n",
      "101\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "16\n",
      "0\n",
      "-39\n",
      "57\n",
      "116\n",
      "21\n",
      "30\n",
      "-4\n",
      "80\n",
      "25\n",
      "7\n",
      "490\n",
      "0\n",
      "519\n",
      "545\n",
      "592\n",
      "driver reward  3055\n",
      "0\n",
      "81\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "83\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "82\n",
      "0\n",
      "75\n",
      "0\n",
      "0\n",
      "83\n",
      "71\n",
      "36\n",
      "116\n",
      "0\n",
      "63\n",
      "87\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "51\n",
      "154\n",
      "55\n",
      "0\n",
      "41\n",
      "7\n",
      "5\n",
      "driver reward  1090\n",
      "0\n",
      "104\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "84\n",
      "70\n",
      "0\n",
      "0\n",
      "45\n",
      "0\n",
      "0\n",
      "98\n",
      "72\n",
      "0\n",
      "0\n",
      "0\n",
      "66\n",
      "63\n",
      "74\n",
      "0\n",
      "74\n",
      "139\n",
      "64\n",
      "-9\n",
      "9\n",
      "48\n",
      "73\n",
      "0\n",
      "0\n",
      "-10\n",
      "-2\n",
      "69\n",
      "66\n",
      "0\n",
      "132\n",
      "driver reward  1329\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "78\n",
      "96\n",
      "40\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "-8\n",
      "0\n",
      "50\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "26\n",
      "0\n",
      "0\n",
      "driver reward  282\n",
      "0\n",
      "0\n",
      "0\n",
      "149\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "26\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "26\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "18\n",
      "38\n",
      "83\n",
      "29\n",
      "99\n",
      "0\n",
      "1\n",
      "86\n",
      "-15\n",
      "0\n",
      "11\n",
      "42\n",
      "24\n",
      "0\n",
      "-3\n",
      "-30\n",
      "0\n",
      "577\n",
      "driver reward  1161\n",
      "110\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "97\n",
      "0\n",
      "142\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "103\n",
      "154\n",
      "0\n",
      "0\n",
      "53\n",
      "0\n",
      "55\n",
      "-6\n",
      "1\n",
      "87\n",
      "38\n",
      "40\n",
      "0\n",
      "0\n",
      "0\n",
      "71\n",
      "0\n",
      "13\n",
      "19\n",
      "51\n",
      "75\n",
      "29\n",
      "-40\n",
      "537\n",
      "485\n",
      "0\n",
      "547\n",
      "459\n",
      "driver reward  3120\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "88\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "63\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "68\n",
      "32\n",
      "38\n",
      "44\n",
      "0\n",
      "96\n",
      "67\n",
      "44\n",
      "0\n",
      "0\n",
      "-6\n",
      "0\n",
      "57\n",
      "0\n",
      "0\n",
      "0\n",
      "-12\n",
      "0\n",
      "55\n",
      "-50\n",
      "0\n",
      "419\n",
      "driver reward  1003\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "81\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "88\n",
      "107\n",
      "0\n",
      "80\n",
      "108\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "66\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "99\n",
      "0\n",
      "58\n",
      "0\n",
      "-14\n",
      "37\n",
      "20\n",
      "0\n",
      "0\n",
      "driver reward  730\n",
      "0\n",
      "0\n",
      "0\n",
      "71\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "72\n",
      "119\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "70\n",
      "0\n",
      "0\n",
      "0\n",
      "34\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "37\n",
      "0\n",
      "8\n",
      "0\n",
      "0\n",
      "0\n",
      "21\n",
      "33\n",
      "73\n",
      "-17\n",
      "20\n",
      "70\n",
      "29\n",
      "15\n",
      "0\n",
      "577\n",
      "523\n",
      "514\n",
      "driver reward  2269\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "52\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "74\n",
      "108\n",
      "107\n",
      "0\n",
      "128\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "54\n",
      "34\n",
      "25\n",
      "0\n",
      "57\n",
      "0\n",
      "0\n",
      "107\n",
      "38\n",
      "0\n",
      "0\n",
      "driver reward  784\n",
      "0\n",
      "0\n",
      "0\n",
      "108\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "114\n",
      "75\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "75\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "25\n",
      "72\n",
      "56\n",
      "84\n",
      "0\n",
      "44\n",
      "-31\n",
      "-6\n",
      "-55\n",
      "driver reward  561\n",
      "75\n",
      "0\n",
      "0\n",
      "0\n",
      "69\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "59\n",
      "0\n",
      "62\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "115\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "106\n",
      "48\n",
      "50\n",
      "-2\n",
      "0\n",
      "0\n",
      "70\n",
      "-1\n",
      "-1\n",
      "38\n",
      "driver reward  688\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "87\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "38\n",
      "47\n",
      "32\n",
      "34\n",
      "79\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "74\n",
      "159\n",
      "18\n",
      "29\n",
      "73\n",
      "123\n",
      "94\n",
      "0\n",
      "73\n",
      "0\n",
      "-52\n",
      "0\n",
      "driver reward  908\n",
      "95\n",
      "0\n",
      "82\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "58\n",
      "0\n",
      "0\n",
      "88\n",
      "78\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "75\n",
      "47\n",
      "0\n",
      "10\n",
      "19\n",
      "0\n",
      "149\n",
      "33\n",
      "12\n",
      "-17\n",
      "28\n",
      "10\n",
      "128\n",
      "70\n",
      "50\n",
      "-39\n",
      "0\n",
      "0\n",
      "0\n",
      "457\n",
      "537\n",
      "0\n",
      "499\n",
      "0\n",
      "453\n",
      "528\n",
      "479\n",
      "485\n",
      "494\n",
      "470\n",
      "447\n",
      "513\n",
      "560\n",
      "460\n",
      "520\n",
      "0\n",
      "513\n",
      "driver reward  8391\n",
      "0\n",
      "104\n",
      "0\n",
      "0\n",
      "0\n",
      "90\n",
      "0\n",
      "0\n",
      "106\n",
      "0\n",
      "0\n",
      "0\n",
      "53\n",
      "0\n",
      "63\n",
      "0\n",
      "0\n",
      "48\n",
      "115\n",
      "128\n",
      "100\n",
      "0\n",
      "114\n",
      "10\n",
      "0\n",
      "0\n",
      "15\n",
      "134\n",
      "111\n",
      "101\n",
      "0\n",
      "0\n",
      "-1\n",
      "0\n",
      "99\n",
      "0\n",
      "-23\n",
      "driver reward  1367\n",
      "0\n",
      "0\n",
      "108\n",
      "105\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "111\n",
      "83\n",
      "0\n",
      "57\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "105\n",
      "51\n",
      "0\n",
      "0\n",
      "93\n",
      "-13\n",
      "0\n",
      "13\n",
      "106\n",
      "114\n",
      "97\n",
      "4\n",
      "-34\n",
      "0\n",
      "63\n",
      "524\n",
      "488\n",
      "driver reward  2075\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "93\n",
      "0\n",
      "0\n",
      "0\n",
      "89\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "74\n",
      "0\n",
      "0\n",
      "61\n",
      "32\n",
      "42\n",
      "0\n",
      "0\n",
      "34\n",
      "0\n",
      "-2\n",
      "0\n",
      "driver reward  423\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "152\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "82\n",
      "0\n",
      "63\n",
      "110\n",
      "0\n",
      "23\n",
      "72\n",
      "7\n",
      "-13\n",
      "22\n",
      "0\n",
      "127\n",
      "0\n",
      "0\n",
      "0\n",
      "128\n",
      "6\n",
      "8\n",
      "71\n",
      "-3\n",
      "0\n",
      "472\n",
      "521\n",
      "571\n",
      "472\n",
      "driver reward  2891\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "124\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "52\n",
      "117\n",
      "0\n",
      "0\n",
      "0\n",
      "28\n",
      "43\n",
      "0\n",
      "0\n",
      "0\n",
      "107\n",
      "108\n",
      "0\n",
      "60\n",
      "0\n",
      "111\n",
      "0\n",
      "86\n",
      "108\n",
      "83\n",
      "-12\n",
      "-48\n",
      "-13\n",
      "78\n",
      "driver reward  1032\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "35\n",
      "85\n",
      "66\n",
      "0\n",
      "84\n",
      "0\n",
      "0\n",
      "40\n",
      "0\n",
      "15\n",
      "39\n",
      "150\n",
      "80\n",
      "70\n",
      "35\n",
      "86\n",
      "-12\n",
      "0\n",
      "51\n",
      "0\n",
      "31\n",
      "41\n",
      "-67\n",
      "488\n",
      "435\n",
      "606\n",
      "467\n",
      "592\n",
      "0\n",
      "519\n",
      "489\n",
      "484\n",
      "447\n",
      "551\n",
      "436\n",
      "586\n",
      "465\n",
      "496\n",
      "534\n",
      "540\n",
      "driver reward  8964\n",
      "total reward  42123\n"
     ]
    }
   ],
   "source": [
    "#run_simulation(eval_policy)\n",
    "evaluatePolicy(eval_policy, eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-68-ff0ae3fb21d8>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-ff0ae3fb21d8>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "reward results - \n",
    "random policy - around 9.5k\n",
    "learned policy - 14k\n",
    "always accept policy - 19.4k\n",
    "\"\"\"\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# startup simulation\n",
    "\n",
    "def simpy_episode(rewards, steps, time_step, tf_env, policy):\n",
    "\n",
    "    TIME_MULTIPLIER = 50\n",
    "    DRIVER_COUNT = 1\n",
    "    TRIP_COUNT = 8000\n",
    "    RUN_TIME = 10000\n",
    "    INTERVAL = 20\n",
    "    # GRID_WIDTH = 3809\n",
    "    # GRID_HEIGHT = 2622\n",
    "    GRID_WIDTH = 60\n",
    "    GRID_HEIGHT = 40\n",
    "    HEX_AREA = 2.6\n",
    "\n",
    "    Env = simpy.Environment()\n",
    "    map_grid = Grid(env=Env, width=GRID_WIDTH, height=GRID_HEIGHT, interval=INTERVAL, num_drivers=DRIVER_COUNT,\n",
    "                    hex_area=HEX_AREA)\n",
    "\n",
    "    taxi_spots = map_grid.taxi_spots\n",
    "    driver_list = create_drivers(Env, DRIVER_COUNT, map_grid)\n",
    "    driver_pools = map_grid.driver_pools\n",
    "\n",
    "    run_simulation(TRIP_COUNT, RUN_TIME, DRIVER_COUNT, TIME_MULTIPLIER, map_grid, taxi_spots, driver_list, driver_pools, Env, rewards, steps, time_step, tf_env, policy)\n",
    "    t_count = 0\n",
    "    for dr in driver_list:\n",
    "        d_t_count = dr.total_trip_count\n",
    "        t_count += d_t_count\n",
    "        print(f\"{dr.id} completed {d_t_count}\")\n",
    "\n",
    "    print(f\"Total trip count: {t_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "var[0] = 2\n",
    "print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 1\n",
    "\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run - atttempt 2\n",
    "\n",
    "#time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    time_step = tf_env.reset()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    simpy_episode(rewards, step, time_step, tf_env, policy)\n",
    "\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple episode run template\n",
    "\"\"\"\n",
    "time_step = tf_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "  episode_reward = 0\n",
    "  episode_steps = 0\n",
    "  while not time_step.is_last():\n",
    "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
    "    time_step = tf_env.step(action)\n",
    "    episode_steps += 1\n",
    "    episode_reward += time_step.reward.numpy()\n",
    "  rewards.append(episode_reward)\n",
    "  steps.append(episode_steps)\n",
    "  time_step = tf_env.reset()\n",
    "\n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)\n",
    "\n",
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
